<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[后端开发面经]]></title>
    <url>%2F2019%2F09%2F29%2F%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[乐信集团 protected关键字修饰的范围？ 答： private:只能被自身访问 default（默认）：同包可以访问 protected：同包和不同包的子类可以访问 public：可以被所有类访问 final关键字有什么作用？abstract final修饰类的情形是否存在？ 答： final类不能被继承，没有子类，final类中的方法默认是final的。 final方法不能被子类的方法覆盖，但可以被继承。 final成员变量表示常量，只能被赋值一次，赋值后值不再改变。 final不能用于修饰构造方法。 final类型运用于数据： 基本数据类型（int、double、char…）运用final时，使数值恒定不变； 对象引用运用final时，final使得引用恒定不变，引用内部的数据若不是final型，可以进行修改。 数组类型运用final时，final使得数组引用恒定不变，数组内部的数据若不是final型，可以进行修改。 final与static： final指明数据为一个常量，恒定无法修改； static指明数据只占用一份存储区域； 不存在，因为被final修饰的方法意味着该方法不可以被子类重写，这跟abstract的语义相互矛盾。 i++在两个线程分别执行100次，最大值和最小值分别多少？为什么？如何使得i的值为200？ 答： i++不是原子操作，也就是说，它不是单独一条指令，而是3条指令： 1、从内存中把i的值取出来放到CPU的寄存器中 2、CPU寄存器的值+1 3、把CPU寄存器的值写回内存 注： 注：对于多线程，线程共用一个内存，如果线程A在寄存器执行操作后而没有写入内存，则会切换到另一个线程。 i的值的变化范围为2到200： 结果为2的情况是： A线程执行第一次i++后，寄存器的值为1，但是没有刷新到主内存，这时候B线程开始执行第一次i++，寄存器的值也为1，也没有刷新到主内存。后面A线程开始执行完第99次i++操作后，寄存器为99，并且每次都刷新到主内存中去，此时内存的值为99。然后B线程接着执行完第一次i++操作，把1刷新到主内存中去，把99覆盖掉。这时，线程A开始执行自己的第100次i++操作，此时线程A读到内存中的值为1，存到寄存器然后++，此时寄存器的值为2，不刷新到内存里去，内存的值依旧为1。然后，线程B执行完剩余的i++操作，并且每次都刷新值到主内存中去，这时内存的值为100。然后线程A执行完第100次操作的剩余部分，将2刷新到主内存，这时候内存值为2，结束。 结果为200的情况： 线程A和线程B每次执行完i++操作都写入内存，并且每次操作交替进行。 如何使得i的值为200？ 锁机制。给方法添加synchronized关键字或者使用ReentrantLock都可以解决这个问题。这里可以拓展说下synchronized关键字和ReentrantLock的优劣。我认为一般使用synchronized更好，因为JVM团队一直以来都在优先改进这个机制，可以尽早获得更好的性能，并且synchronized对大多数开发人员来说更加熟悉，方便代码的阅读。 API层面：对于Synchronized来说，它是java语言的关键字，是原生语法层面的互斥，需要jvm实现。而ReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。 1234567891011private ReentrantLock lock = new ReentrantLock();public void run() &#123; lock.lock(); try&#123; for(int i=0;i&lt;5;i++)&#123; System.out.println(Thread.currentThread().getName()+":"+i); &#125; &#125;finally&#123; lock.unlock(); &#125;&#125; 等待可中断：当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。可等待特性对处理执行时间非常长的同步快很有帮助。 可实现公平锁。公平锁是指多个线程在等待同一个锁时，必须按照申请的时间顺序来依次获得锁；而非公平锁则不能保证这一点。非公平锁在锁被释放时，任何一个等待锁的线程都有机会获得锁。synchronized的锁是非公平锁，ReentrantLock默认情况下也是非公平锁，但可以通过带布尔值的构造函数要求使用公平锁。 123456789/** * Creates an instance of &#123;@code ReentrantLock&#125; with the * given fairness policy. * * @param fair &#123;@code true&#125; if this lock should use a fair ordering policy */public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; ReentrantLock可以绑定多个condition对象，和多个条件关联。synchronized中，锁对象的wait()和notify()或notifyAll()方法可以实现一个隐含的条件。但如果要和多于一个的条件关联的时候，就不得不额外添加一个锁。 使用AtomicInteger原子类。为什么AtomicInteger使用CAS完成？因为传统的锁机制需要陷入内核态，造成上下文切换，但是一般持有锁的时间很短，频繁的陷入内核开销太大，所以随着机器硬件支持CAS后，JAVA推出基于compare and set机制的AtomicInteger，实际上就是一个CPU循环忙等待。因为持有锁时间一般较短，内核开销较大，所以大部分情况CAS比锁性能更优。 注意：volatile不能解决这个线程安全问题。因为volatile只能保证可见性，不能保证原子性。 拓展： i++和++i的线程安全分为两种情况： 如果i是局部变量（在方法里定义的），那么是线程安全的。因为局部变量是线程私有的，别的线程访问不到，其实也可以说没有线程安不安全之说，因为别的线程对他造不成影响。 如果i是全局变量（类的成员变量），那么是线程不安全的。因为如果是全局变量的话，同一进程中的不同线程都有可能访问到。 如果有大量线程同时执行i++操作，i变量的副本拷贝到每个线程的线程栈，当同时有两个线程栈以上的线程读取线程变量，假如此时是1的话，那么同时执行i++操作，再写入到全局变量，最后两个线程执行完，i会等于3而不会是2，所以，出现不安全性。 假设有一张数据库表tableA，字段分别为(name,age,city)，使用SQl语言求每个城市的平均年龄。 答： 1select city,avg(age) as '平均年龄' from tableA group by city 假设有一张数据库表tableA，字段分别为(name,age,city)，使用SQL语言求每个城市的第二大年龄是多少。(数据库保证每个城市至少有两条记录以上) 答： 1 假设有一张数据库表tableA，字段分别为(name,age,city)，使用SQl语言求纽约中年龄最大的人的姓名。 答： 123select name,age from tableA where city = '纽约' and age = (select max(age) from tableA where city = '纽约') 有表Student(Sno,Sname,Sage,Ssex)学生表，表Course(Cno,Cname,Tno)课程表，表SC(Sno,Cno,score)成绩表，表Teacher(Tno,Tname)教师表 统计列印各科成绩，各分数段人数：课程ID，课程名 123456select c.Cno,c.Cname, count(case when s.score between 85 and 100 then 1 end) as '[100-85分]', count(case when s.score between 70 and 85 then 1 end) as '[85-70分]', count(case when s.score between 60 and 70 then 1 end) as '[70-60分]', count(case when s.score&lt;60 then 1 end) as '[60分以下]' from Course c, SC s where c.Cno = s.Cnogroup by c.Cno,c.Cname 查询两门以上不及格课程的同学的学号及其平均成绩、 123456select Sno,avg(score) from SC where Sno in ( select Sno from SC where score &lt; 60 group by Sno having count(*) &gt;= 2) group by Sno Java中的子类是否要实现父类所有的抽象方法？ 答：子类如果是非抽象类的话，那么一定要实现父类中所有的抽象方法，但是，如果子类也是抽象类，那么可以不实现父类中所有的抽象方法，可以实现一部分抽象方法。 Java的抽象类中可以定义final方法吗？Java的抽象类可以被private修饰吗？ 答： 可以。但是final是不能修饰abstract所修饰的方法的。 Java抽象类是内部类时，可以被private修饰。 Java抽象类不是内部类时，不可以被private修饰。 中信银行 Spring Boot和Spring MVC的区别？ 如果想引入一个jar包，如何在Spring MVC中配置xml文件？ 在数据库表中建立一个字段，只有一个字节的长度，用varchar还是char好？为什么？ 4399 什么是守护线程，实现守护线程的基本流程和原理是什么？ HashMap线程不安全的体现？ HashSet底层实现原理 redis数据存储在内存还是磁盘？ Nosql用过哪些 ConcurrentHashmap加锁体现在哪里？ 数据库分区分表如何实现？ Java Bean和普通的类有什么区别 如何读取Spring Boot配置文件中的属性？(有哪些注解) Synchronized关键字底层语义原理 答： 之所以这个synchronized关键字会起带同步的作用，就是因为monitor。monitor只是一个简称，通常称为intrinsic lock或者monitor lock。 每个对象都有一个monitor lock与之相关联，如果一个线程需要排他和一致访问对象的某个域，它就必须先获得对象的monitor lock，当它完成任务之后再释放这个锁，让其它线程可以访问到。只要一个线程获得了一个monitor lock，那么其它线程就没办法获取到这个锁。也就是说，当其它线程尝试获得锁的时候就会阻塞。 Synchronized进行编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1。相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 在方法执行期间，执行线程持有了monitor，其他任何线程都无法再获得同一个monitor。如果一个同步方法执行期间抛出了异常，并且方法内部无法处理此异常，那这个同步方法所持有的monitor将在异常抛到同步方法之外时自动释放。 无论方法是正常结束还是异常结束，方法中调用过的每条monitorenter指令都有执行其对应monitorexit指令。 注意：Synchronized是一种可重入锁。 synchronized的可重入性 从互斥锁的设计上来说，当一个线程试图操作一个由其他线程持有的对象锁的临界资源时，将会处于阻塞状态，但当一个线程再次请求自己持有对象锁的临界资源时，这种情况属于重入锁，请求将会成功，在java中synchronized是基于原子性的内部锁机制，是可重入的，因此在一个线程调用synchronized方法的同时在其方法体内部调用该对象另一个synchronized方法，也就是说一个线程得到一个对象锁后再次请求该对象锁，是允许的，这就是synchronized的可重入性。如下： 1234567891011121314151617181920212223242526272829public class AccountingSync implements Runnable&#123; static AccountingSync instance=new AccountingSync(); static int i=0; static int j=0; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; //this,当前实例对象锁 synchronized(this)&#123; i++; increase();//synchronized的可重入性 &#125; &#125; &#125; public synchronized void increase()&#123; j++; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread t1=new Thread(instance); Thread t2=new Thread(instance); t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); &#125;&#125; 正如代码所演示的，在获取当前实例对象锁后进入synchronized代码块执行同步代码，并在代码块中调用了当前实例对象的另外一个synchronized方法，再次请求当前实例锁时，将被允许，进而执行方法体代码，这就是重入锁最直接的体现，需要特别注意另外一种情况，当子类继承父类时，子类也是可以通过可重入锁调用父类的同步方法。 注意：由于synchronized是基于monitor实现的，因此每次重入，monitor中的计数器仍会加1。 Java虚拟机对synchronized的优化锁的状态总共有四种，无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级。 自旋锁 锁粗化： 偏向锁 轻量级锁 重量级锁 锁消除：消除锁是虚拟机另外一种锁的优化，这种优化更彻底，Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间。 ReentrantLock和synchronized使用分析？ 答： ReentrantLock是Lock的实现类，是一个互斥的同步器，在多线程高竞争条件下，ReentrantLock比synchronized有更加优异的性能表现。 1、用法比较 Lock使用起来比较灵活，但是必须有释放锁的配合动作 Lock必须手动获取与释放锁，而synchronized不需要手动释放和开启锁 Lock只适用于代码块锁，而synchronized可用于修饰方法、代码块等 2、特性比较 ReentrantLock的优势体现在： 具备尝试非阻塞地获取锁的特性：当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁 能被中断地获取锁的特性：与synchronized不同，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放 超时获取锁的特性：在指定的时间范围内获取锁；如果截止时间到了仍然无法获取锁，则返回 3、注意事项 在使用ReentrantLock类的时，一定要注意三点： 在finally中释放锁，目的是保证在获取锁之后，最终能够被释放 不要将获取锁的过程写在try块内，因为如果在获取锁时发生了异常，异常抛出的同时，也会导致锁无故被释放。 ReentrantLock提供了一个newCondition的方法，以便用户在同一锁的情况下可以根据不同的情况执行等待或唤醒的动作。 什么是覆盖索引？覆盖索引的优点？覆盖索引的使用场景？ 答： 通常大家都会根据查询的where条件来创建合适的索引，不过这只是索引优化的一个方面。索引确实是一种查找数据的高效方式，但是mysql也可以使用索引来直接获取列的数据，这样就不再需要读取数据行。如果索引的叶子节点中已经包含要查询的数据，那么还有什么必要再回表查询呢？ 如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为“覆盖索引”。 覆盖索引的优点 索引项通常远小于记录，所以如果只需读取索引，那mysql就会极大的减少数据访问量。这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝上。覆盖索引对于I/O密集型的应用也很有帮助，因为索引比数据更小，更容易全部放入内存中。 因为索引是按照列值顺序存储的，所以对于I/O密集型的范围查询会比随机从磁盘读取每一行数据的I/O要少的多。对于某些存储引擎，例如MyISAM，甚至可以通过optimize命令使得索引完全顺序排列，这让简单的范围查询能使用完全顺序的索引访问。 一些存储引擎如MyISAM在内存中只缓存索引，数据则依赖于操作系统来缓存，因此要访问数据需要一次系统调用。这可能会导致严重的性能问题，尤其是那些系统调用占用了数据访问中的最大开销的场景。 由于InnoDB的聚簇索引，覆盖索引对InnoDB特别有用。InnoDB的二级索引在叶子节点中保存了数据行的主键值，所以如果二级索引能够覆盖查询，则可以避免对主键索引的二次查询。 判断标准使用explain，可以通过输出的extra列来判断，对于一个索引覆盖查询，显示为using index,MySQL查询优化器在执行查询前会决定是否有索引覆盖查询]]></content>
      <categories>
        <category>java面试准备</category>
        <category>笔经面经</category>
      </categories>
      <tags>
        <tag>笔经面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节跳动后端开发面经]]></title>
    <url>%2F2019%2F07%2F10%2F%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[提前批No.1 Java多态的原理？ 答：参考： ​ https://blog.csdn.net/SEU_Calvin/article/details/52191321 ​ https://zhuanlan.zhihu.com/p/27912079 Java 中接口和抽象类的区别？ 答： 抽象类中可以有普通的成员变量，而接口中没有。 抽象类中可以包含静态方法，而接口不可以。 如果抽象类实现接口，则可以把接口中方法映射到抽象类中作为抽象方法而不必实现，而在抽象类的子类中实现接口中方法 抽象类只能单继承，而接口可以被多个类实现。 抽象类中可以有构造方法，而接口中不可以。 抽象类可以但不是必须有抽象属性和抽象方法，但是一旦有了抽象方法，就一定要把这个类声明为抽象类。 在传统版本上，接口中的所有方法必须是非静态的，且是abstract的，且是public的。普通方法可以不写修饰符，也会默认为public和abstract。但在java版本1.8中，你可以为方法添加默认方法，这时候实现类不继承该方法也是可以编译通过的。 Java中四种引用的关系？ 答： 强引用：强引用是最普遍的引用，如果一个对象具有强引用，垃圾回收器不会回收该对象，当内存空间不足时，JVM 宁愿抛出 OutOfMemoryError异常；只有当这个对象没有被引用时，才有可能会被回收。 软引用：如果一个对象只具有软引用，则 当内存空间足够，垃圾回收器就不会回收它。 当内存空间不足了，就会回收该对象； JVM会优先回收长时间闲置不用的软引用的对象，对那些刚刚构建的或刚刚使用过的“新”软引用对象会尽可能保留； 如果回收完还没有足够的内存，才会抛出内存溢出异常。只要垃圾回收器没有回收它，该对象就可以被程序使用。 软引用可以和一个引用队列(ReferenceQueue)联合使用。如果软引用所引用对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。 123456789101112ReferenceQueue&lt;String&gt; referenceQueue = new ReferenceQueue&lt;&gt;();String str = new String("abc");SoftReference&lt;String&gt; softReference = new SoftReference&lt;&gt;(str, referenceQueue); str = null;// Notify GCSystem.gc(); System.out.println(softReference.get()); // abc Reference&lt;? extends String&gt; reference = referenceQueue.poll();System.out.println(reference); //null 注意：软引用对象是在jvm内存不够的时候才会被回收，我们调用System.gc()方法只是起通知作用，JVM什么时候扫描回收对象是JVM自己的状态决定的。就算扫描到软引用对象也不一定会回收它，只有内存不够的时候才会回收。 应用场景： 浏览器的后退按钮。按后退时，这个后退时显示的网页内容是重新进行请求还是从缓存中取出呢？这就要看具体的实现策略了。 如果一个网页在浏览结束时就进行内容的回收，则按后退查看前面浏览过的页面时，需要重新构建； 如果将浏览过的网页存储到内存中会造成内存的大量浪费，甚至会造成内存溢出。 这时候就可以使用软引用，很好的解决了实际的问题： 1234567891011121314151617// 获取浏览器对象进行浏览Browser browser = new Browser();// 从后台程序加载浏览页面BrowserPage page = browser.getPage();// 将浏览完毕的页面置为软引用SoftReference softReference = new SoftReference(page); // 回退或者再次浏览此页面时if(softReference.get() != null) &#123; // 内存充足，还没有被回收器回收，直接获取缓存 page = softReference.get();&#125; else &#123; // 内存不足，软引用的对象已经回收 page = browser.getPage(); // 重新构建软引用 softReference = new SoftReference(page);&#125; 弱引用：弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期，它只能生存到下一次垃圾收集发生之前。当垃圾回收器扫描到只具有弱引用的对象时，无论当前内存空间是否足够，都会回收它。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。 注意：如果一个对象是偶尔(很少)的使用，并且希望在使用时随时就能获取到，但又不想影响此对象的垃圾收集，那么你应该用Weak Reference来记住此对象。 虚引用：虚引用顾名思义，就是形同虚设。与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。 应用场景： 虚引用主要用来跟踪对象被垃圾回收器回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列(ReferenceQueue)联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 1234String str = new String("abc");ReferenceQueue queue = new ReferenceQueue();// 创建虚引用，要求必须与一个引用队列关联PhantomReference pr = new PhantomReference(str, queue); 程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要进行垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 下面通过一张表格来说明它们的回收时间、用途： 参考：https://juejin.im/post/5b82c02df265da436152f5ad#heading-3 Java多线程实现的几种方式？Runnable接口有哪些优势？ 答： 通过继承Thread类来实现 通过实现Runnable接口 通过内部类实现(有些情况我们的线程就想执行一次，以后就用不到了) 基于线程池的方式实现 带有返回值的线程实现方式，步骤如下： 创建一个类实现Callable接口，实现call方法。这个接口类似于Runnable接口，但比Runnable接口更加强大，增加了异常和返回值。 创建一个FutureTask，指定Callable对象，做为线程任务。 创建线程，指定线程任务 启动线程 Runnable接口的实现优势有： 使用接口的方式可以让我们的程序降低耦合度，Runnable就是一个线程任务，线程任务和线程的控制分离，那么一个线程任务可以提交给多个线程来执行。比如车站的售票窗口，每个窗口可以看做是一个线程，他们每个窗口做的事情都是一样的，也就是售票。这样我们程序在模拟现实的时候就可以定义一个售票任务，让多个窗口同时执行这一个任务。那么如果要改动任务执行计划，只要修改线程任务类，所有的线程就都会按照修改后的来执行。相比较继承Thread类的方式来创建线程的方式，实现Runnable接口是更为常用的。 java不允许多继承，因此实现了Runnable接口的类可以再继承其他类。 参考：https://blog.csdn.net/king_kgh/article/details/78213576 Java中堆栈的区别？堆栈的增长方向有哪些不同？ 答： 堆中主要存放new出来的实例变量和数组，栈描述的是Java方法执行的内存模型，而栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。局部变量表主要存放了编译器可知的各种数据类型(boolean、byte、char、short、int、float、long、double)、对象引用(reference类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)。 对比堆和栈，只要把握一个重要的信息，new的对象是存储在堆中的，但为了存取方便，会在栈中声明一个引用变量指向堆中的对象，这样存取方便而且速度快。另一方面，栈中的对象超过作用域即被释放，Java会立即释放掉内存空间另作他用；而堆中即使超出变量的作用范围也不会变成垃圾，只有在没有引用变量指向它时会变成垃圾，但又不会立刻被回收，只有在一个不确定的时间才会被垃圾回收器回收掉，这也就是为什么Java会比较占用内存的原因了。再一点还要注意，就是被static修饰的变量，它是在程序启动之初就会被分配内存，它的生命周期会一直到程序结束，所以使用static一定要慎重。 栈的增长方向是从高地址到地址，堆的增长方向是从低地址到高地址(简化的Linux/x86模型)。 输入一个url，发生了什么？ 答： 浏览器查询DNS，获取域名对应的IP地址，即是服务器的IP地址。其中查询DNS的过程先对浏览器自身DNS缓存的查询，浏览器自身没有缓存对应的DNS，就去查询操作系统中的DNS缓存，操作系统中没有，就去读取本地的Host文件。如果上述都找不到对应的DNS缓存，就去本地域名服务器查询对应的DNS缓存。 浏览器获取到域名对应的IP地址后，就向服务器发起三次握手，建立连接。 TCP/IP连接建立后，浏览器向服务器发送HTTP请求。 服务器收到请求后，根据路径参数映射到特定的请求处理器进行处理，并将处理结果及响应的视图返回给浏览器。 浏览器解析视图，如果遇上js、css等文件的引用，则继续向服务器发送资源请求。 浏览器根据资源、数据进行渲染，呈现一个完整的页面。 最终，浏览器向服务器发起四次挥手，关闭连接。 这个简单的过程涉及到的协议有：DNS协议、TCP协议、IP协议、OSPF路由选择协议、ARP协议、HTTP协议。 ping的原理是什么？ 答： PING过程的两种情况，一种是在同一网段内，一种是在不同网段内： 在同一网段内(主机Aping主机B)： 如果主机A当中缓存有主机B的MAC地址(即主机A当中的MAC地址表有B的MAC地址)，就会直接将这个MAC地址封装到ICMP报文中向主机B发送，当主机B收到了这个报文后，发现是主机A的ICPM回显请求，就按同样的格式，将ICMP的回显报文发送回去给主机A，这样就完成了同一网段内的ping过程。 如果主机A的MAC地址表没有B的MAC地址，则会向外发送一个ARP广播报文。交换机会收到这个报文后，交换机有学习MAC地址的功能，所以他会检索自己有没有保存主机B的MAC地址，如果有，就返回给主机A，如果没有，就会向所有端口发送ARP广播，其它主机收到后，发现不是在找自己，就纷纷丢弃了该报文，不去理会。直到主机B收到了报文后，就立即响应，我的MAC地址是多少，同时学到主机A的MAC地址,并按同样的ARP报文格式返回给主机A。这时候主机A学习到了主机B的MAC地址，就把这个MAC地址封装到ICMP协议的二层报文中向主机B发送。当主机B收到了这个报文后，发现是主机A的ICPM回显请求，就按同样的格式，将ICMP的回显报文发送回去给主机A，这样就完成了同一网段内的ping过程。 下面是ICMP报文和ICMP回显报文： 在不同网段内(主机Aping主机C)，如果主机A要ping主机C,那么主机A发现主机C的IP和自己不是同一网段,他就去找网关转发： 如果主机A的MAC地址表中有网关的MAC地址，则直接将这个MAC地址封装成ICMP报文发送给网关路由器。当路由器收到主机A发过来的ICMP报文，发现目的地址是其本身的MAC地址，根据目的IP2.1.1.1，查路由表，发现2.1.1.1/24的路由表项，得到一个出口指针，这个出口指针意指主机C的MAC地址。然后去掉原来的MAC头部，加上自己的MAC地址向主机C转发(如果网关也没有主机C的MAC地址，还是要像前面一个步骤一样，ARP广播一下即可相互学习。路由器2端口能学到主机C的MAC地址，主机C也能学到路由器2端口的MAC)。最后，在主机C已学到路由器2端口MAC，路由器2端口转发给路由器1端口，路由1端口学到主机A的MAC的情况下，他们就不需要再做ARP解析，就将ICMP的回显报文发送回去。 如果主机A的MAC地址表没有网关的MAC地址，会先发送一个ARP广播，学到网关的MAC，再发封装ICMP报文给网关路由器。后面的过程跟上述一样。 下面是ICMP报文和ICMP回显报文： 参考： https://blog.csdn.net/f2006116/article/details/51159895 https://baike.baidu.com/item/ICMP https://zh.wikipedia.org/wiki/%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%8D%8F%E8%AE%AE http1.0和http1.1之间有什么区别？ 答： HTTP1.1支持长连接和请求的流水线处理，并且默认使用长连接。而HTTP1.0默认使用短连接，需要在request中增加“Connection：keep-alive”，header才能支持长连接。 流水线的方式处理请求：客户端每遇到一个对象引用就立即发出一个请求，而不必等到收到前一个响应之后才能发出下一个请求。 分块传输数据：发送方将消息实体分割成为任意大小的组块，并单独地发送。在每个组块之前，加上该组块的长度，使接收方可以确保自己能够完整地接收到这个组块。并且，在组块最末尾的地方，发送方生成了长度为零的组块，接收方可据此判断整条消息是否已安全地传输完毕。 HTTP1.1新增了一个状态码 100 Continue，用于客户端在发送POST数据给服务器前，征询服务器的情况，看服务器是否处理POST的数据。 HTTP的请求头里都包含了些什么？HTTP如何发起请求？ 答： HTTP报文由3部分组成(请求行+请求头+请求体) 请求行：包括请求方法、请求URI、HTTP请求的协议及版本。 方法-URI-协议/版本：如GET /index.jsp HTTP/1.1 GET就是请求方法，根据HTTP标准，HTTP协议请求可以使用多种请求方法。HTTP 1.1支持七种请求方法：GET、POST、HEAD、OPTIONS、PUT、DELETE和TRACE等。常用的为请求方法是GET和POST。 /index.jsp表示URI，URI指定了要访问的网络资源。 HTTP/1.1是协议和协议的版本。 请求头：即HTTP请求的报文头。报文头包含若干个属性，格式为“属性名:属性值”(键值对)，服务端据此获取客户端的信息。 请求体：即HTTP请求的报文体。它将一个页面表单中的组件值通过param1=value1&amp;param2=value2的键值对形式编码成一个格式化串，它承载多个请求参数的数据。不但报文体可以传递请求参数，请求URL也可以通过类似于“/chapter15/user.html? param1=value1&amp;param2=value2”的方式传递请求参数。 下面列举常见的HTTP请求报文头属性 Accept：请求报文可通过一个“Accept”报文头属性告诉服务端 客户端接受什么类型的响应。 如下报文头相当于告诉服务端，俺客户端能够接受的响应类型仅为纯文本数据啊，你丫别发其它什么图片啊，视频啊过来，那样我会歇菜的~~~： 1Accept:text/plain Cookie：客户端的Cookie就是通过这个报文头属性传给服务端的哦！如下所示： 1Cookie: $Version=1; Skin=new;jsessionid=5F4771183629C9834F8382E23BE13C4C 服务端是怎么知道客户端的多个请求是隶属于一个Session呢？ 注意到后台的那个jsessionid=5F4771183629C9834F8382E23BE13C4C木有？原来就是通过HTTP请求报文头的Cookie属性的jsessionid的值关联起来的！(当然也可以通过重写URL的方式将会话ID附带在每个URL的后面哦) Referer：表示这个请求是从哪个URL过来的。 假如你通过google搜索出一个商家的广告页面，你对这个广告页面感兴趣，鼠标一点发送一个请求报文到商家的网站，这个请求报文的Referer报文头属性值就是http://www.google.com。 唐僧到了西天.如来问：侬是不是从东土大唐来啊？唐僧：厉害！你咋知道的！如来：哈哈，我偷看了你的Refere。 Cache-Control：对缓存进行控制，如一个请求希望响应返回的内容在客户端要被缓存一年，或不希望被缓存就可以通过这个报文头达到目的。 如以下设置，相当于让服务端将对应请求返回的响应内容不要在客户端缓存： 1Cache-Control: no-cache 其他请求报文头属性： 参见：http://en.wikipedia.org/wiki/List_of_HTTP_header_fields 如何访问请求报文头 由于请求报文头是客户端发过来的，服务端当然只能读取了。 以下是HttpServletRequest一些用于读取请求报文头的API： 123451. //获取请求报文中的属性名称 2. java.util.Enumeration&lt;java.lang.String&gt; getHeaderNames(); 3. 4. //获取指定名称的报文头属性的值 5. java.lang.String getHeader(java.lang.String name) 由于一些请求报文头属性“太著名”了，因此HttpServletRequest为它们提供了VIP的API： 123456789101. //获取报文头中的Cookie(读取Cookie的报文头属性） 2. Cookie[] getCookies() ; 3. 4. //获取客户端本地化信息（读取 Accept-Language 的报文头属性） 5. java.util.Locale getLocale() 6. 7. //获取请求报文体的长度（读取Content-Length的报文头属性） 8. int getContentLength(); HttpServletRequest可以通过 1HttpSession getSession() 获取请求所关联的HttpSession,其内部的机理是通过读取请求报文头中Cookie属性的JSESSIONID的值，在服务端的一个会话Map中，根据这个JSESSIONID获取对应的HttpSession的对象。 HTTP是如何发起请求的： DNS域名解析、三次握手建立连接、发起HTTP请求。 参考： https://blog.csdn.net/u010256388/article/details/68491509 https://blog.csdn.net/yezitoo/article/details/78193794 HashMap的实现原理？ 答： JDK1.7：HashMap底层数据结构为数组+链表，使用一个Entry数组存储数据，用key的hash值取模来决定key会被存放数组中的哪个位置，而hash值由key的hashcode通过某些运算得来。如果key的hash值取模后的结果相同，那些这些key会被放到数组中的同一个位置，这时候就会发生冲突。因此，相同的元素在同一个数组位置会形成链表。在hashcode特别差的情况下，冲突会频繁发生，这时候形成的链表就会很长，那么put/get操作可能会遍历整个链表，时间复杂度会退化到O(n)。 JDK1.8：HashMap底层数据结构为数组+链表+红黑树，使用Node数组存储数据，但这个Node结点可能是链表结构也可能是红黑树结构；与JDK1.7不同的是，如果插入同一位置的元素超过8个，就会调用treeifyBin函数，将链表转换为红黑树。那么即使hashcode完全相同，由于红黑树的特点，查找某个特定原色，也只需要O(logn)的开销，也就是说put/get操作的时间复杂度最差只有O(logn)。 进程的五种状态以及如何进行切换的？ 答： 就绪 -&gt; 运行：对就绪状态的进程，当进程调度程序按一种选定的策略从中选中一个就绪进程，为之分配了CPU(也叫处理机)后，该进程便由就绪状态变为运行状态； 运行 -&gt; 阻塞：正在执行的进程因发生某等待事件而无法执行，则进程由执行状态变为阻塞状态。如下： 进程提出输入/输出请求(IO请求)而变成等待外部设备传输信息的状态； 进程申请资源（主存空间或外部设备）得不到满足时变成等待资源状态； 进程运行中出现了故障（程序出错或主存储器读写错等）变成等待干预状态等等。 阻塞 -&gt; 就绪：处于阻塞状态的进程，在其等待的事件已经发生，如输入/输出操作完成，资源得到满足或错误处理完毕时，处于等待状态的进程并不马上转入运行状态，而是先转入就绪状态，然后再由系统进程调度程序在适当的时候将该进程转为运行状态； 运行 -&gt; 就绪：正在执行的进程，因时间片用完而被暂停执行，或在采用抢先式优先级调度算法的系统中,当有更高优先级的进程要运行而被迫让出处理机时，该进程便由执行状态转变为就绪状态。 进程之间的通信方式？ 答： 管道（pipe）：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有血缘关系的进程间使用。进程的血缘关系通常指父子进程关系。 有名管道（named pipe）：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间通信。 信号量（semophore）：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它通常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列（message queue）：消息队列是由消息组成的链表，存放在内核中，并由消息队列标识符标识。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号（signal）：信号是一种比较复杂的通信方式，用于通知接收进程某一事件已经发生。 共享内存（shared memory）：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的IPC方式，它是针对其他进程间的通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。 套接字（socket）：套接口也是一种进程间的通信机制，与其他通信机制不同的是它可以用于不同及其间的进程通信。 电梯调度算法？ 答： 先来先服务(FIFO) 最短寻道时间优先(SSTF) 扫描算法： SCAN算法：也就是很形象的电梯调度算法。比如磁盘扫描，先按照一个方向，从当前磁道向内磁道方向访问(磁道减少的方向)，当访问到最内层的磁道号以后，就会反向，向着磁道增加的方向继续访问未访问过的磁道号。这就好比坐电梯，中间一直往下面接人，下面没人的时候反向，向上去接人。 CSCAN算法：循环扫描算法，同SCAN算法不同的是，这个算法扫描到最里面的磁道后，会立即跳到最外层的磁道，然后按照原来访问的方向去扫描。故也称单向扫描调度算法。 String、StringBuffer、StringBuilder的区别？ 答： 首先明确一点，String是不可变的对象，而StringBuffer和StringBuilder是可变的字符序列，他们两个都是类似于String的字符串缓冲区。两者不同的是StringBuffer是线程安全的，而StringBuilder是非线程安全的。 每次对String类型的对象进行改变的时候其实都等于新生成了一个新的String对象，然后将栈里的指针(引用)指向了新的String对象。而如果是使用StringBuffer或者StringBuilder的话，每次改变都是对原有对象本身进行操作。 初始化的方式不同，StringBuffer和StringBuilder只能用构造函数的形式进行初始化，而String对象除了可以用构造函数进行初始化以外，还可以直接赋值。 new一个String对象会产生几个对象？ 答： 这种情况首先应该明白堆栈存储的是字符串对象和字符串的对象引用，而字符串常量池是在方法区中。 下面我们看一段代码： 12345String str1 = “abc”;String str2 = “abc”;String str3 = “abc”;String str4 = new String(“abc”);String str5 = new String(“abc”); 分析：我们在new str4这个对象的时候，会先去常量池中查找是否有“abc”这个字面量。如果有，则不做任何事情，没有则创建对应的常量对象。然后会在堆中new一个String对象，通过new操作符创建的字符串对象不指向字符串池中的任何对象，但是可以通过使用字符串的intern()方法来指向其中的某一个。最后，将堆中对象的地址赋值给str4，创建一个引用。（上图中堆和常量池中的指向只是一种对应关系） 所以，常量池中没有“abc”字面量则创建两个对象，否则创建一个对象，以及创建一个引用。 拓展： 根据字面量，往往会提出这样的变式题： String str1 = new String(“A”+”B”) ; 会创建多少个对象?String str2 = new String(“ABC”) + “ABC” ; 会创建多少个对象? str1：字符串常量池：”A”,”B”,”AB” : 3个堆：new String(“AB”) ：1个引用： str1 ：1个总共 ： 4个 str2 ：字符串常量池：”ABC” : 1个堆：new String(“ABC”) ：1个引用： str2 ：1个总共 ： 2个 参考：https://segmentfault.com/a/1190000009888357 JDK动态代理和CGLIB动态代理的区别？ 答： JDK动态代理的前提是目标类有要实现的接口，而CGLIB动态代理的前提是目标类不能被final修饰，因为被final修饰的类不能被继承。由于CGLIB动态代理的实质是动态生成的子类继承了目标类，在运行期动态地在内存中创建一个子类。 JDK动态代理的实质是动态地生成一个类去实现目标类所实现的接口，而CGLIB动态代理的实质是动态生成的子类继承了目标类，在运行期动态地在内存中创建一个子类。 JDK动态代理是不需要第三方库支持的，只需要JDK环境就可以代理，创建JDK代理工厂的条件： 代理工厂类实现InvocationHandler接口; 使用java.lang.reflect包中的Proxy类提供的newProxyInstance方法来生成代理对象; 1Proxy.newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 被代理类（目标类）一定要实现接口。 CGLIB的实现依赖于cglib类库，创建CGLIB代理工厂的条件： 通过cglib类库提供的Enhancer类的create静态方法来创建代理类; 1Enhancer.create(Class type, Callback callback) type是原对象的Class对象 callback是回调方法接口 cglib类库中的callback方法通过实现它的MethodInterceptor接口的intercept方法来进行回调，因此代理工厂类需要实现MethodInterceptor接口： 1public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args, MethodProxy proxy) throws Throwable; obj是被代理的对象 method是执行的方法 args是执行方法的参数数组 proxy用来执行未被拦截的原方法 被代理类（目标类）不能被final所修饰。 参考：https://blog.csdn.net/yhl_jxy/article/details/80635012 Spring AOP的实现原理是什么？ 答： 其实真要说AOP的实现原理，很难说的上来，但是我知道动态代理类的生成是依据JVM反射等机制动态生成的，并且代理类和委托类的关系是在运行时才确定的。而如果是从动态代理的两种方式的来讲，无非是看JDK和CGLIB动态代理是怎么实现的。（Spring AOP使用的是动态代理）接下来就要去看两种方式的实现区别和实现原理，并且清除的阐述代理模式是怎么运作的。 为什么重写equals时必须重写hashCode方法？ 答： 简单来说，就是假如我们要比较HashMap中的key是否相等，我们首先会调用key的hashcode()方法，而没有重写过的hashcode()方法是这个对象在内存中的地址。重写hashcode()之后，若两个key的hashcode值一样。此时并不能说明这两个key是相等的(相同含义)，这时候要调用equals()方法比较两个key对象的内容是否一样。如果我们没有重写equals()方法，则比较的是两个对象的内存地址是否相等(即是否指向同一个地方)，就达不到比较两个key对象的意义，因为不同key对象equals(没有重写)的话一定会返回false。 算法题： 判断一颗二叉树是否对称 松鼠捡豆（动态规划） 123456789101112131415161718//松鼠捡豆 //max数组表示当前元素能捡到的豆的最大数量 public static int getMostBean(int row, int col, int[][] bean) &#123; int[][] max = new int[row][col]; max[0][0] = bean[0][0]; for(int i = 0 ; i &lt; row ; i++) &#123; for(int j = 0 ; j &lt; col ; j++) &#123; if(i == 0 &amp;&amp; j &gt; 0) max[i][j] = max[i][j-1] + bean[i][j]; else if(i &gt; 0 &amp;&amp; j == 0) max[i][j] = max[i-1][j] + bean[i][j]; else if(i &gt; 0 &amp;&amp; j &gt; 0) max[i][j] = bean[i][j] + Math.max(max[i-1][j], max[i][j-1]); &#125; &#125; return max[row-1][col-1]; &#125; No.2 讲一讲你对volatile关键字的理解 答： 保证被修饰的变量对所有线程的可见性，即当一条线程修改了这个变量时，其他线程也是可以立即得知的，而普通变量的修改是需要通过工作内存和主内存之间的通信来完成。 禁止指令重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 volatile关键字保证的是一个线程对于它的会立即刷新到主内存中去，并置其他线程的副本为无效，但是并不保证对volatile变量的操作都具有原子性。 讲一讲HashMap与HashTable的区别？多个线程同时使用一个HashMap时你觉得会发生什么？ 答： 区别： HashMap是线程不安全的，而HashTable是线程安全的。 HashMap最多只允许一条记录的键值为Null，允许多条记录的值为Null；HashTable不允许记录的键或值为Null。 HashTable是基于Dictionary类，而HashMap是基于AbstractMap。 多个线程同时使用一个HashMap，会出现链表闭环的情况(多个线程同时put数据)。这时候如果进行get数据，就会进入死循环。 多个线程同时使用一个HashMap，会出现数据覆盖的问题。举个栗子，HashMap进行put操作时是先计算hashCode找到桶，然后遍历桶内的链表找到插入位置插入。如果2个线程t1、t2分别put一个hashCode相同的元素e1、e2，就可能导致找到相同的插入位置(a)，t1里a.next=e1，t2里a.next=e2，就只有一个数据保留了下来，丢了一个。 讲讲你对ThreadLocal的理解？（在多线程环境下，如何防止自己的变量被其它线程篡改？） 答： ThreadLocal顾名思义可以理解为线程本地变量，用来维护线程中的变量不受其他线程的干扰。也就是说如果定义了一个ThreadLocal，每个线程往这个ThreadLocal中的读写是隔离的，互相之间不会影响。 它大致的实现思路是怎样的？ Thread类有一个类型为ThreadLocal.ThreadLocalMap的实例变量threadLocals，也就是说每个线程有一个自己的ThreadLocalMap。ThreadLocalMap有自己的独立实现，可以简单地将它的key视作ThreadLocal，value为代码中放入的值（实际上key并不是ThreadLocal本身，而是它的一个弱引用）。每个线程在往某个ThreadLocal里塞值的时候，都会往自己的ThreadLocalMap里存，读也是以某个ThreadLocal作为引用，在自己的map里找对应的key，从而实现了线程隔离。 1234567891011121314151617181920212223242526public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 说明：可以发现，每个线程中都有一个ThreadLocalMap数据结构，当执行set方法时，其值是保存在当前线程的threadLocals变量中，当执行set方法时，是从当前线程的threadLocals变量获取。 所以在线程1中set的值，对线程2来说是摸不到的，而且在线程2中重新set的话，也不会影响到线程1中的值，保证了线程之间不会相互干扰。 一个ThreadLocal只能保存一个键值对，但是一个线程可以创建多个ThreadLocal对象，并且各个线程之间的数据互不干扰。 在ThreadLoalMap中，也是初始化一个大小16的Entry数组，Entry对象用来保存每一个key-value键值对，只不过这里的key永远都是ThreadLocal对象，是不是很神奇，通过ThreadLocal对象的set方法，结果把ThreadLocal对象自己当做key，放进了ThreadLoalMap中。 12345678910111213141516171819202122232425262728293031323334353637383940/** * 初始容量，必须为2的幂 */private static final int INITIAL_CAPACITY = 16;/** * Entry表，大小必须为2的幂 */private Entry[] table;/** * 表里entry的个数 */private int size = 0;/** * 重新分配表大小的阈值，默认为0 */private int threshold; /** * 设置resize阈值以维持最坏2/3的装载因子 */private void setThreshold(int len) &#123; threshold = len * 2 / 3;&#125;/** * 环形意义的下一个索引 */private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0);&#125;/** * 环形意义的上一个索引 */private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1);&#125; 这里需要注意的是，ThreadLoalMap的Entry是继承WeakReference，和HashMap很大的区别是，Entry中没有next字段，所以就不存在链表的情况了。 123456789static class Entry extends WeakReference&lt;java.lang.ThreadLocal&lt;?&gt;&gt; &#123; // 往ThreadLocal里实际塞入的值 Object value; Entry(java.lang.ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 为什么要弱引用？ 因为如果这里使用普通的key-value形式来定义存储结构，实质上就会造成节点的生命周期与线程强绑定，只要线程没有销毁，那么节点在GC分析中一直处于可达状态，没办法被回收，而程序本身也无法判断是否可以清理节点。弱引用是Java中四档引用的第三档，比软引用更加弱一些，如果一个对象没有强引用链可达，那么一般活不过下一次GC。当某个ThreadLocal已经没有强引用可达，则随着它被垃圾回收，在ThreadLocalMap里对应的Entry的键值会失效，这为ThreadLocalMap本身的垃圾清理提供了便利。 内存泄漏 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 通过之前的分析已经知道，当使用ThreadLocal保存一个value时，会在ThreadLocalMap中的数组插入一个Entry对象，按理说key-value都应该以强引用保存在Entry对象中，但在ThreadLocalMap的实现中，key被保存到了WeakReference对象中。 这就导致了一个问题，ThreadLocal在没有外部强引用时，发生GC时会被回收，如果创建ThreadLocal的线程一直持续运行，那么这个Entry对象中的value就有可能一直得不到回收，发生内存泄露。 解释：如果一个ThreadLocal对象被回收了，我们往里面放的value对于【当前线程-&gt;当前线程的threadLocals(ThreadLocal.ThreadLocalMap对象）-&gt;Entry数组-&gt;某个entry.value】这样一条强引用链是可达的，因此value不会被回收。 如何解决内存泄漏？ 当我们仔细读过ThreadLocalMap的源码，我们可以推断，如果在使用的ThreadLocal的过程中，显式地进行remove是个很好的编码习惯，这样是不会引起内存泄漏。那么如果没有显式地进行remove呢？只能说如果对应线程之后调用ThreadLocal的get和set方法都有很高的概率会顺便清理掉无效对象，断开value强引用，从而大对象被收集器回收。 在调用ThreadLocal的get()、set()可能会清除ThreadLocalMap中key为null的Entry对象，这样对应的value就没有GC Roots可达了，下次GC的时候就可以被回收，当然如果调用remove方法，肯定会删除对应的Entry对象。 使用场景 直接定位到 ThreadLocal 的源码，可以看到源码注释中有很清楚的解释：它是线程的局部变量，这些变量只能在这个线程内被读写，在其他线程内是无法访问的。 ThreadLocal 定义的通常是与线程关联的私有静态字段（例如，用户ID或事务ID）。 变量有局部的还有全局的，局部变量没什么好说的，一涉及到全局，那自然就会出现多线程的安全问题，要保证多线程安全访问，不出现脏读脏写，那就要涉及到线程同步了。而 ThreadLocal 相当于提供了介于局部变量与全局变量中间的这样一种线程内部的全局变量。 总结了半天，发现使用场景说到底就概括成一个：就是当我们只想在本身的线程内使用的变量，可以用 ThreadLocal 来实现，并且这些变量是和线程的生命周期密切相关的，线程结束，变量也就销毁了。 所以说 ThreadLocal 不是为了解决线程间的共享变量问题的，如果是多线程都需要访问的数据，那需要用全局变量加同步机制。 举几个例子说明一下： 1、比如线程中处理一个非常复杂的业务，可能方法有很多，那么，使用 ThreadLocal 可以代替一些参数的显式传递； 2、比如用来存储用户 Session。Session 的特性很适合 ThreadLocal ，因为 Session 之前当前会话周期内有效，会话结束便销毁。我们先笼统但不正确的分析一次 web 请求的过程： 用户在浏览器中访问 web 页面； 浏览器向服务器发起请求； 服务器上的服务处理程序（例如tomcat）接收请求，并开启一个线程处理请求，期间会使用到 Session ； 最后服务器将请求结果返回给客户端浏览器。 从这个简单的访问过程我们看到正好这个 Session 是在处理一个用户会话过程中产生并使用的，如果单纯的理解一个用户的一次会话对应服务端一个独立的处理线程，那用 ThreadLocal 在存储 Session ,简直是再合适不过了。但是例如 tomcat 这类的服务器软件都是采用了线程池技术的，并不是严格意义上的一个会话对应一个线程。并不是说这种情况就不适合 ThreadLocal 了，而是要在每次请求进来时先清理掉之前的 Session ，一般可以用拦截器、过滤器来实现。 3、在一些多线程的情况下，如果用线程同步的方式，当并发比较高的时候会影响性能，可以改为 ThreadLocal 的方式，例如高性能序列化框架 Kyro 就要用 ThreadLocal 来保证高性能和线程安全； 4、还有像线程内上线文管理器、数据库连接等可以用到 ThreadLocal;]]></content>
      <categories>
        <category>java面试准备</category>
        <category>笔经面经</category>
      </categories>
      <tags>
        <tag>笔经面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的finalize()方法总结]]></title>
    <url>%2F2019%2F05%2F30%2Fjava%E4%B8%AD%E7%9A%84finalize-%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[注：本文的目的并不是鼓励使用finalize方法，而是大致理清其作用、问题以及GC执行finalize的过程。 finalize()的作用 finalize()是Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 finalize()与C++中的析构函数不是对应的。C++中的析构函数调用的时机是确定的（对象离开作用域或delete掉），但Java中的finalize的调用具有不确定性 不建议用finalize方法完成“非内存资源”的清理工作，但建议用于： ① 清理本地对象(通过JNI创建的对象)； ② 作为确保某些非内存资源(如Socket、文件等)释放的一个补充：在finalize方法中显式调用其他资源释放方法。其原因可见下文[finalize的问题] finalize()的问题 一些与finalize相关的方法，由于一些致命的缺陷，已经被废弃了，如System.runFinalizersOnExit()方法、Runtime.runFinalizersOnExit()方法 System.gc()与System.runFinalization()方法增加了finalize方法执行的机会，但不可盲目依赖它们 Java语言规范并不保证finalize方法会被及时地执行、而且根本不会保证它们会被执行 finalize方法可能会带来性能问题。因为JVM通常在单独的低优先级线程中完成finalize的执行 对象再生问题：finalize方法中，可将待回收对象赋值给GC Roots可达的对象引用，从而达到对象再生的目的 finalize方法至多由GC执行一次(用户当然可以手动调用对象的finalize方法，但并不影响GC对finalize的行为) finalize()的执行过程(生命周期)(1) 首先，大致描述一下finalize流程：当对象变成(GC Roots)不可达时，GC会判断该对象是否覆盖了finalize方法，若未覆盖，则直接将其回收。否则，若对象未执行过finalize方法，将其放入F-Queue队列，由一低优先级线程执行该队列中对象的finalize方法。执行finalize方法完毕后，GC会再次判断该对象是否可达，若不可达，则进行回收，否则，对象 “复活” 。 (2) 具体的finalize流程： 对象可由两种状态，涉及到两类状态空间： 一是终结状态空间 F = {unfinalized, finalizable, finalized}； 二是可达状态空间 R = {reachable, finalizer-reachable, unreachable}。 各状态含义如下： unfinalized: 新建对象会先进入此状态，GC并未准备执行其finalize方法，因为该对象是可达的 finalizable: 表示GC可对该对象执行finalize方法，GC已检测到该对象不可达。正如前面所述，GC通过F-Queue队列和一专用线程完成finalize的执行 finalized: 表示GC已经对该对象执行过finalize方法 reachable: 表示GC Roots引用可达 finalizer-reachable(f-reachable)：表示不是reachable，但可通过某个finalizable对象可达 unreachable：对象不可通过上面两种途径可达 状态变迁图： 变迁说明： 新建对象首先处于[reachable, unfinalized]状态(A) 随着程序的运行，一些引用关系会消失，导致状态变迁，从reachable状态变迁到f-reachable(B, C, D)或unreachable(E, F)状态 若JVM检测到处于unfinalized状态的对象变成f-reachable或unreachable，JVM会将其标记为finalizable状态(G,H)。若对象原处于[unreachable, unfinalized]状态，则同时将其标记为f-reachable(H)。 在某个时刻，JVM取出某个finalizable对象，将其标记为finalized并在某个线程中执行其finalize方法。由于是在活动线程中引用了该对象，该对象将变迁到(reachable, finalized)状态(K或J)。该动作将影响某些其他对象从f-reachable状态重新回到reachable状态(L, M, N) 处于finalizable状态的对象不能同时是unreahable的，由第4点可知，将对象finalizable对象标记为finalized时会由某个线程执行该对象的finalize方法，致使其变成reachable。这也是图中只有八个状态点的原因 程序员手动调用finalize方法并不会影响到上述内部标记的变化，因此JVM只会至多调用finalize一次，即使该对象“复活”也是如此。程序员手动调用多少次不影响JVM的行为 若JVM检测到finalized状态的对象变成unreachable，回收其内存(I) 若对象并未覆盖finalize方法，JVM会进行优化，直接回收对象（O） 注：System.runFinalizersOnExit()等方法可以使对象即使处于reachable状态，JVM仍对其执行finalize方法 一些代码示例对象复活12345678910111213141516171819202122232425262728293031public class GC &#123; public static GC SAVE_HOOK = null; public static void main(String[] args) throws InterruptedException &#123; SAVE_HOOK = new GC(); SAVE_HOOK = null; System.gc(); Thread.sleep(500); if (null != SAVE_HOOK) &#123; //此时对象应该处于(reachable, finalized)状态 System.out.println("Yes , I am still alive"); &#125; else &#123; System.out.println("No , I am dead"); &#125; SAVE_HOOK = null; System.gc(); Thread.sleep(500); if (null != SAVE_HOOK) &#123; System.out.println("Yes , I am still alive"); &#125; else &#123; System.out.println("No , I am dead"); &#125; &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println("execute method finalize()"); SAVE_HOOK = this; &#125; &#125; 执行结果如下： 123execute method finalize()Yes , I am still aliveNo , I am dead 参考： https://www.cnblogs.com/Smina/p/7189427.html]]></content>
      <categories>
        <category>java面试准备</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK提供的并发容器总结]]></title>
    <url>%2F2019%2F05%2F01%2FJDK%E6%8F%90%E4%BE%9B%E7%9A%84%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[JDK提供的并发容器总结JDK提供的这些容器大部分在 java.util.concurrent 包中： ConcurrentHashMap: 线程安全的HashMap。 CopyOnWriteArrayList: 线程安全的List，在读多写少的场合性能非常好，远远好于Vector。 ConcurrentLinkedQueue: 高效的并发队列，使用链表实现。可以看做一个线程安全的 LinkedList，这是一个非阻塞队列。 BlockingQueue: 这是一个接口，JDK内部通过链表、数组等方式实现了这个接口。表示阻塞队列，非常适合用于作为数据共享的通道。 ConcurrentSkipListMap: 跳表的实现。这是一个Map，使用跳表的数据结构进行快速查找。 ConcurrentHashMap我们知道 HashMap 不是线程安全的，在并发场景下如果要保证一种可行的方式是使用 Collections.synchronizedMap() 方法来包装我们的 HashMap。但这是通过使用一个全局的锁来同步不同线程间的并发访问，因此会带来不可忽视的性能问题。 因此就有了 HashMap 的线程安全版本——ConcurrentHashMap 的诞生。在ConcurrentHashMap中，无论是读操作还是写操作都能保证很高的性能：在进行读操作时(几乎)不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。 关于 ConcurrentHashMap 相关问题，我在 这几道Java集合框架面试题几乎必问 这篇文章中已经提到过。 下面梳理一下关于 ConcurrentHashMap 比较重要的问题： ConcurrentHashMap 和 Hashtable 的区别ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。 底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) ：使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 两者的对比图： 图片来源：http://www.cnblogs.com/chengxiao/p/6842045.html HashTable： JDK1.7的ConcurrentHashMap： JDK1.8的ConcurrentHashMap（TreeBin: 红黑二叉树节点，Node: 链表节点）： ConcurrentHashMap线程安全的具体实现方式/底层具体实现JDK1.7（上面有示意图）首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。 Segment 实现了 ReentrantLock，所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。 123static class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; &#125; 一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。 JDK1.8 （上面有示意图）ConcurrentHashMap 取消了 Segment 分段锁，采用 CAS 和 synchronized 来保证并发安全。ConcurrentHashMap 底层数据结构跟 HashMap1.8 的结构类似，数组+链表/红黑二叉树。Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))） synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。源码示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null; &#125; CopyOnWriteArrayListCopyOnWriteArrayList 简介123public class CopyOnWriteArrayList&lt;E&gt;extends Objectimplements List&lt;E&gt;, RandomAccess, Cloneable, Serializable 在很多应用场景中，读操作可能会远远大于写操作。由于读操作根本不会修改原有的数据，因此对于每次读取都进行加锁其实是一种资源浪费。我们应该允许多个线程同时访问List的内部数据，毕竟读取操作是安全的。 这和我们之前在多线程章节讲过 ReentrantReadWriteLock 读写锁的思想非常类似，也就是读读共享、写写互斥、读写互斥、写读互斥。JDK中提供了 CopyOnWriteArrayList 类比相比于在读写锁的思想又更进一步。为了将读取的性能发挥到极致，CopyOnWriteArrayList 读取是完全不用加锁的，并且更厉害的是：写入也不会阻塞读取操作，只有写入和写入之间需要进行同步等待。这样一来，读操作的性能就会大幅度提升。那它是怎么做的呢？ CopyOnWriteArrayList 是如何做到的？CopyOnWriteArrayList 类的所有可变操作（add，set等等）都是通过创建底层数组的新副本来实现的。当 List 需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。 从 CopyOnWriteArrayList 的名字就能看出CopyOnWriteArrayList 是满足CopyOnWrite 的ArrayList，所谓CopyOnWrite 也就是说：在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。 CopyOnWriteArrayList 读取和写入源码简单分析CopyOnWriteArrayList 读取操作的实现读取操作没有任何同步控制和锁操作，理由就是：内部数组 array 不会发生修改，只会被另外一个 array 替换，因此可以保证数据安全。 123456789101112/** The array, accessed only via getArray/setArray. */ private transient volatile Object[] array; public E get(int index) &#123; return get(getArray(), index); &#125; @SuppressWarnings("unchecked") private E get(Object[] a, int index) &#123; return (E) a[index]; &#125; final Object[] getArray() &#123; return array; &#125; CopyOnWriteArrayList 写入操作的实现CopyOnWriteArrayList 写入操作 add() 方法在添加集合的时候加了锁，保证了同步，避免了多线程写的时候会 copy 出多个副本出来。 1234567891011121314151617181920/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */ public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock();//加锁 try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1);//拷贝新数组 newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock();//释放锁 &#125; &#125; ConcurrentLinkedQueueJava提供的线程安全的 Queue 可以分为阻塞队列和非阻塞队列，其中阻塞队列的典型例子是 BlockingQueue，非阻塞队列的典型例子是ConcurrentLinkedQueue，在实际应用中要根据实际需要选用阻塞队列或者非阻塞队列。 阻塞队列可以通过加锁来实现，非阻塞队列可以通过 CAS 操作实现。 从名字可以看出，ConcurrentLinkedQueue这个队列使用链表作为其数据结构．ConcurrentLinkedQueue 应该算是在高并发环境中性能最好的队列了。它之所有能有很好的性能，是因为其内部复杂的实现。 ConcurrentLinkedQueue 内部代码我们就不分析了，大家知道 ConcurrentLinkedQueue 主要使用 CAS 非阻塞算法来实现线程安全就好了。 ConcurrentLinkedQueue 适合在对性能要求相对较高，同时对队列的读写存在多个线程同时进行的场景，即如果对队列加锁的成本较高则适合使用无锁的 ConcurrentLinkedQueue 来替代。 BlockingQueueBlockingQueue 简单介绍上面我们己经提到了 ConcurrentLinkedQueue 作为高性能的非阻塞队列。下面我们要讲到的是阻塞队列——BlockingQueue。阻塞队列（BlockingQueue）被广泛使用在“生产者-消费者”问题中，其原因是BlockingQueue提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。 BlockingQueue 是一个接口，继承自 Queue，所以其实现类也可以作为 Queue 的实现来使用，而 Queue 又继承自 Collection 接口。 下面是 BlockingQueue 的相关实现类： 下面主要介绍一下：ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue，这三个 BlockingQueue 的实现类。 ArrayBlockingQueueArrayBlockingQueue 是 BlockingQueue 接口的有界队列实现类，底层采用数组来实现。ArrayBlockingQueue一旦创建，容量不能改变。其并发控制采用可重入锁来控制，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。 ArrayBlockingQueue 默认情况下不能保证线程访问队列的公平性，所谓公平性是指严格按照线程等待的绝对时间顺序，即最先等待的线程能够最先访问到 ArrayBlockingQueue。而非公平性则是指访问 ArrayBlockingQueue 的顺序不是遵守严格的时间顺序，有可能存在，当 ArrayBlockingQueue 可以被访问时，长时间阻塞的线程依然无法访问到 ArrayBlockingQueue。 如果保证公平性，通常会降低吞吐量。如果需要获得公平性的 ArrayBlockingQueue，可采用如下代码： 1private static ArrayBlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;Integer&gt;(10,true); LinkedBlockingQueueLinkedBlockingQueue 底层基于单向链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用，同样满足FIFO的特性，与ArrayBlockingQueue 相比起来具有更高的吞吐量。为了防止 LinkedBlockingQueue 容量迅速增，损耗大量内存，通常在创建LinkedBlockingQueue 对象时，会指定其大小，如果未指定，容量等于Integer.MAX_VALUE。 相关构造方法： 12345678910111213141516171819202122/** *某种意义上的无界队列 * Creates a &#123;@code LinkedBlockingQueue&#125; with a capacity of * &#123;@link Integer#MAX_VALUE&#125;. */ public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE); &#125; /** *有界队列 * Creates a &#123;@code LinkedBlockingQueue&#125; with the given (fixed) capacity. * * @param capacity the capacity of this queue * @throws IllegalArgumentException if &#123;@code capacity&#125; is not greater * than zero */ public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null); &#125; PriorityBlockingQueuePriorityBlockingQueue 是一个支持优先级的无界阻塞队列。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现 compareTo() 方法来指定元素排序规则，或者初始化时通过构造器参数 Comparator 来指定排序规则。 PriorityBlockingQueue 并发控制采用的是 ReentrantLock，队列为无界队列（ArrayBlockingQueue 是有界队列，LinkedBlockingQueue 也可以通过在构造函数中传入 capacity 指定队列最大的容量，但是 PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容）。 简单地说，它就是 PriorityQueue 的线程安全版本。 PriorityBlockingQueue 不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。 它的插入操作 put 方法不会 block，因为它是无界队列（ take 方法在队列为空的时候会阻塞）。 推荐文章： 《解读 Java 并发队列 BlockingQueue 》 https://javadoop.com/post/java-concurrent-queue ConcurrentSkipListMap为了引出ConcurrentSkipListMap，先带着大家简单理解一下跳表。 对于一个单链表，即使链表是有序的，如果我们想要在其中查找某个数据，也只能从头到尾遍历链表，这样效率自然就会很低，跳表就不一样了。跳表是一种可以用来快速查找的数据结构，有点类似于平衡树。它们都可以对元素进行快速的查找。但一个重要的区别是：对平衡树的插入和删除往往很可能导致平衡树进行一次全局的调整。而对跳表的插入和删除只需要对整个数据结构的局部进行操作即可。这样带来的好处是：在高并发的情况下，你会需要一个全局锁来保证整个平衡树的线程安全。而对于跳表，你只需要部分锁即可。这样，在高并发环境下，你就可以拥有更好的性能。而就查询的性能而言，跳表的时间复杂度也是 O(logn) 。所以在并发数据结构中，JDK 使用跳表来实现一个 Map。 跳表的本质是同时维护了多个链表，并且链表是分层的， 最低层的链表维护了跳表内所有的元素，每上面一层链表都是下面一层的子集。 跳表内的所有链表的元素都是排序的。查找时，可以从顶级链表开始找，一旦发现被查找的元素大于当前链表中的取值，就会转入下一层链表继续找。这也就是说在查找过程中，搜索是跳跃式的。 比如在跳表中查找元素18，查找过程如下图所示： 查找18 的时候原来需要遍历 18 次，现在只需要 7 次即可。针对链表长度比较大的时候，构建索引查找效率的提升就会非常明显。 从上面很容易看出，跳表是一种利用空间换时间的算法。 使用跳表实现 Map 和使用哈希算法实现 Map 的另外一个不同之处是：哈希并不会保存元素的顺序，而跳表内所有的元素都是排序的。因此在对跳表进行遍历时，你会得到一个有序的结果。所以，如果你的应用需要有序性，那么跳表就是你不二的选择。 JDK 中实现这一数据结构的类是ConcurrentSkipListMap。 参考： 《实战Java高并发程序设计》 https://javadoop.com/post/java-concurrent-queue https://juejin.im/post/5aeebd02518825672f19c546 并发容器总结 数据结构与算法之美]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AQS原理以及AQS同步组件总结]]></title>
    <url>%2F2019%2F04%2F28%2FAQS%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8AAQS%E5%90%8C%E6%AD%A5%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[常见问题： AQS原理了解吗？ CountDownLatch和CyclicBarrier了解吗，两者的区别是什么？ 用过Semaphore吗？ 本节思维导图： AQS 简单介绍AQS的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面： AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 AQS 原理 在面试中被问到并发知识的时候，大多都会被问到“请你说一下自己对于AQS原理的理解”。下面给大家一个示例供大家参加，面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来。 下面大部分内容其实在AQS类注释上已经给出了，不过是英语看着比较吃力一点，感兴趣的话可以看看源码。 AQS 原理概览AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。 看个AQS(AbstractQueuedSynchronizer)原理图： AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值的修改。 1private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过protected类型的getState，setState，compareAndSetState进行操作： 123456789101112//返回同步状态的当前值protected final int getState() &#123; return state;&#125; // 设置同步状态的值protected final void setState(int newState) &#123; state = newState;&#125;//原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值）protected final boolean compareAndSetState(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, stateOffset, expect, update);&#125; AQS 对资源的共享方式AQS定义两种资源共享方式 Exclusive(独占)：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share(共享)：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在上层已经帮我们实现好了。 AQS底层使用了模板方法模式同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）： 使用者继承AbstractQueuedSynchronizer并重写指定的方法。（这些重写方法很简单，无非是对于共享资源state的获取和释放） 将AQS组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。 这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用，下面简单的给大家介绍一下模板方法模式，模板方法模式是一个很容易理解的设计模式之一。 模板方法模式是基于”继承“的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码。举个很简单的例子假如我们要去一个地方的步骤是：购票buyTicket()-&gt;安检securityCheck()-&gt;乘坐某某工具回家ride()-&gt;到达目的地arrive()。我们可能乘坐不同的交通工具回家比如飞机或者火车，所以除了ride()方法，其他方法的实现几乎相同。我们可以定义一个包含了这些方法的抽象类，然后用户根据自己的需要继承该抽象类然后修改 ride()方法。 AQS使用了模板方法模式，自定义同步器时需要重写下面几个AQS提供的模板方法： 12345isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS类中的其他方法都是final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 下面举两个例子： 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS(Compare and Swap)减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 推荐两篇 AQS 原理和相关源码分析的文章： http://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html Semaphore(信号量)-允许多个线程同时访问synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。示例代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940/** * * @author Snailclimb * @date 2018年9月30日 * @Description: 需要一次性拿一个许可的情况 */public class SemaphoreExample1 &#123; // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException &#123; // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); // 一次只能允许执行的线程数量。 final Semaphore semaphore = new Semaphore(20); for (int i = 0; i &lt; threadCount; i++) &#123; final int threadnum = i; threadPool.execute(() -&gt; &#123;// Lambda 表达式的运用 try &#123; semaphore.acquire();// 获取一个许可，所以可运行线程数量为20/1=20 test(threadnum); semaphore.release();// 释放一个许可 &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); System.out.println("finish"); &#125; public static void test(int threadnum) throws InterruptedException &#123; Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println("threadnum:" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 &#125;&#125; 执行 acquire 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 release 方法增加一个许可证，这可能会释放一个阻塞的acquire方法。然而，其实并没有实际的许可证这个对象，Semaphore只是维持了一个可获得许可证的数量。 Semaphore经常用于限制获取某种资源的线程数量。 当然一次也可以一次拿取和释放多个许可，不过一般没有必要这样做： 123semaphore.acquire(5);// 获取5个许可，所以可运行线程数量为20/5=4test(threadnum);semaphore.release(5);// 释放5个许可 除了 acquire方法之外，另一个比较常用的与之对应的方法是tryAcquire方法，该方法如果获取不到许可就立即返回false。 Semaphore 有两种模式，公平模式和非公平模式。 公平模式： 调用acquire的顺序就是获取许可证的顺序，遵循FIFO； 非公平模式： 抢占式的。 Semaphore 对应的两个构造方法如下： 1234567public Semaphore(int permits) &#123; sync = new NonfairSync(permits);&#125;public Semaphore(int permits, boolean fair) &#123; sync = fair ? new FairSync(permits) : new NonfairSync(permits);&#125; 这两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。 由于篇幅问题，如果对 Semaphore 源码感兴趣的朋友可以看下面这篇文章： https://blog.csdn.net/qq_19431333/article/details/70212663 CountDownLatch （倒计时器）CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完后再执行。在Java并发中，countdownlatch的概念是一个常见的面试题，所以一定要确保你很好的理解了它。 CountDownLatch 的三种典型用法①某一线程在开始运行前等待n个线程执行完毕。将 CountDownLatch 的计数器初始化为n ：new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减1 countdownlatch.countDown()，当计数器的值变为0时，在CountDownLatch上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。(一个线程等待多个线程) ②实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 ：new CountDownLatch(1)，多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为0，多个线程同时被唤醒。(多个线程等待一个线程) ③死锁检测：一个非常方便的使用场景是，你可以使用n个线程访问共享资源，在每次测试阶段的线程数目是不同的，并尝试产生死锁。 CountDownLatch 的使用示例123456789101112131415161718192021222324252627282930313233343536373839/** * * @author SnailClimb * @date 2018年10月1日 * @Description: CountDownLatch 使用方法示例 */public class CountDownLatchExample1 &#123; // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException &#123; // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i &lt; threadCount; i++) &#123; final int threadnum = i; threadPool.execute(() -&gt; &#123;// Lambda 表达式的运用 try &#123; test(threadnum); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; countDownLatch.countDown();// 表示一个请求已经被完成 &#125; &#125;); &#125; countDownLatch.await(); threadPool.shutdown(); System.out.println("finish"); &#125; public static void test(int threadnum) throws InterruptedException &#123; Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println("threadnum:" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 &#125;&#125; 上面的代码中，我们定义了请求的数量为550，当这550个请求被处理完成之后，才会执行System.out.println(&quot;finish&quot;);。 与CountDownLatch的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用CountDownLatch.await()方法，这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他N个线程必须引用闭锁对象，因为他们需要通知CountDownLatch对象，他们已经完成了各自的任务。这种通知机制是通过 CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的count值就减1。所以当N个线程都调用了这个方法，count的值等于0，然后主线程就能通过await()方法，恢复执行自己的任务。 CountDownLatch 的不足CountDownLatch是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当CountDownLatch使用完毕后，它不能再次被使用。 CountDownLatch相常见面试题解释一下CountDownLatch概念？ CountDownLatch 和CyclicBarrier的不同之处？ 给出一些CountDownLatch使用的例子？ CountDownLatch 类中主要的方法？ CyclicBarrier(循环栅栏)CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。 CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 CyclicBarrier 的应用场景CyclicBarrier 可以用于多线程计算数据，最后合并计算结果的应用场景。比如我们用一个Excel保存了用户所有银行流水，每个Sheet保存一个帐户近一年的每笔银行流水，现在需要统计用户的日均银行流水，先用多线程处理每个sheet里的银行流水，都执行完之后，得到每个sheet的日均银行流水，最后，再用barrierAction用这些线程的计算结果，计算出整个Excel的日均银行流水。 CyclicBarrier 的使用示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * * @author Snailclimb * @date 2018年10月1日 * @Description: 测试 CyclicBarrier 类中带参数的 await() 方法 */public class CyclicBarrierExample2 &#123; // 请求的数量 private static final int threadCount = 550; // 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5); public static void main(String[] args) throws InterruptedException &#123; // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; threadCount; i++) &#123; final int threadNum = i; Thread.sleep(1000); threadPool.execute(() -&gt; &#123; try &#123; test(threadNum); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); &#125; public static void test(int threadnum) throws InterruptedException, BrokenBarrierException &#123; System.out.println("threadnum:" + threadnum + "is ready"); try &#123; /**等待60秒，保证子线程完全执行结束*/ cyclicBarrier.await(60, TimeUnit.SECONDS); &#125; catch (Exception e) &#123; System.out.println("-----CyclicBarrierException------"); &#125; System.out.println("threadnum:" + threadnum + "is finish"); &#125;&#125; 运行结果，如下： 123456789101112131415161718192021threadnum:0is readythreadnum:1is readythreadnum:2is readythreadnum:3is readythreadnum:4is readythreadnum:4is finishthreadnum:0is finishthreadnum:1is finishthreadnum:2is finishthreadnum:3is finishthreadnum:5is readythreadnum:6is readythreadnum:7is readythreadnum:8is readythreadnum:9is readythreadnum:9is finishthreadnum:5is finishthreadnum:8is finishthreadnum:7is finishthreadnum:6is finish...... 可以看到当线程数量也就是请求数量达到我们定义的 5 个的时候， await方法之后的方法才被执行。 另外，CyclicBarrier还提供一个更高级的构造函数CyclicBarrier(int parties, Runnable barrierAction)，用于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景。 示例代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * * @author SnailClimb * @date 2018年10月1日 * @Description: 新建 CyclicBarrier 的时候指定一个 Runnable */public class CyclicBarrierExample3 &#123; // 请求的数量 private static final int threadCount = 550; // 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5, () -&gt; &#123; System.out.println("------当线程数达到之后，优先执行------"); &#125;); public static void main(String[] args) throws InterruptedException &#123; // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; threadCount; i++) &#123; final int threadNum = i; Thread.sleep(1000); threadPool.execute(() -&gt; &#123; try &#123; test(threadNum); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); &#125; public static void test(int threadnum) throws InterruptedException, BrokenBarrierException &#123; System.out.println("threadnum:" + threadnum + "is ready"); cyclicBarrier.await(); System.out.println("threadnum:" + threadnum + "is finish"); &#125;&#125; 运行结果，如下： 1234567891011121314151617181920212223threadnum:0is readythreadnum:1is readythreadnum:2is readythreadnum:3is readythreadnum:4is ready------当线程数达到之后，优先执行------threadnum:4is finishthreadnum:0is finishthreadnum:2is finishthreadnum:1is finishthreadnum:3is finishthreadnum:5is readythreadnum:6is readythreadnum:7is readythreadnum:8is readythreadnum:9is ready------当线程数达到之后，优先执行------threadnum:9is finishthreadnum:5is finishthreadnum:6is finishthreadnum:8is finishthreadnum:7is finish...... CyclicBarrier和CountDownLatch的区别CountDownLatch是计数器，只能使用一次，而CyclicBarrier的计数器提供reset功能，可以多次使用。但是我不那么认为它们之间的区别仅仅就是这么简单的一点。我们来从jdk作者设计的目的来看，javadoc是这么描述它们的： CountDownLatch: A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.(CountDownLatch: 一个或者多个线程，等待其他多个线程完成某件事情之后才能执行；) CyclicBarrier : A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.(CyclicBarrier : 多个线程互相等待，直到到达同一个同步点，再继续一起执行。) 对于CountDownLatch来说，重点是“一个线程（多个线程）等待”，而其他的N个线程在完成“某件事情”之后，可以终止，也可以等待。而对于CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。 CountDownLatch是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而CyclicBarrier更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。 CyclicBarrier和CountDownLatch的区别这部分内容参考了如下两篇文章： https://blog.csdn.net/u010185262/article/details/54692886 https://blog.csdn.net/tolcf/article/details/50925145?utm_source=blogxgwz0 ReentrantLock 和 ReentrantReadWriteLockReentrantLock 和 synchronized 的区别在上面已经讲过了这里就不多做讲解。另外，需要注意的是：读写锁 ReentrantReadWriteLock 可以保证多个线程可以同时读，所以在读操作远大于写操作的时候，读写锁就非常有用了。 参考： https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/Multithread/AQS.md]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC中的Atomic原子类]]></title>
    <url>%2F2019%2F04%2F28%2FJUC%E4%B8%AD%E7%9A%84Atomic%E5%8E%9F%E5%AD%90%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[个人觉得这一节掌握基本的使用即可！ 本节思维导图: Atomic 原子类介绍Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。 并发包 java.util.concurrent 的原子类都存放在java.util.concurrent.atomic下，如下图所示： 分类根据操作的数据类型，可以将JUC包中的原子类分为4类： 基本类型 使用原子的方式更新基本类型 AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 数组类型 使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整型数组原子类 AtomicLongArray：长整型数组原子类 AtomicReferenceArray ：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicStampedRerence：原子更新引用类型里的字段原子类 AtomicMarkableReference ：原子更新带有标记位的引用类型 对象的属性修改类型 AtomicIntegerFieldUpdater:原子更新整型字段的更新器 AtomicLongFieldUpdater：原子更新长整型字段的更新器 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 下面我们来详细介绍一下这些原子类。 基本类型原子类基本类型原子类介绍使用原子的方式更新基本类型 AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 上面三个类提供的方法几乎相同，所以我们这里以 AtomicInteger 为例子来介绍。 AtomicInteger 类常用方法： 1234567public final int get() //获取当前的值public final int getAndSet(int newValue)//获取当前的值，并设置新的值public final int getAndIncrement()//获取当前的值，并自增public final int getAndDecrement() //获取当前的值，并自减public final int getAndAdd(int delta) //获取当前的值，并加上预期的值boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update）public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 AtomicInteger 常见方法使用1234567891011121314151617import java.util.concurrent.atomic.AtomicInteger;public class AtomicIntegerTest &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub int temvalue = 0; AtomicInteger i = new AtomicInteger(0); temvalue = i.getAndSet(3); System.out.println("temvalue:" + temvalue + "; i:" + i);//temvalue:0; i:3 temvalue = i.getAndIncrement(); System.out.println("temvalue:" + temvalue + "; i:" + i);//temvalue:3; i:4 temvalue = i.getAndAdd(5); System.out.println("temvalue:" + temvalue + "; i:" + i);//temvalue:4; i:9 &#125;&#125; 结果如下： 基本数据类型原子类的优势通过一个简单例子带大家看一下基本数据类型原子类的优势： ①多线程环境不使用原子类保证线程安全（基本数据类型） 1234567891011class Test &#123; private volatile int count = 0; //若要线程安全执行执行count++，需要加锁 public synchronized void increment() &#123; count++; &#125; public int getCount() &#123; return count; &#125;&#125; ②多线程环境使用原子类保证线程安全（基本数据类型） 1234567891011class Test2 &#123; private AtomicInteger count = new AtomicInteger(); public void increment() &#123; count.incrementAndGet(); &#125; //使用AtomicInteger之后，不需要加锁，也可以实现线程安全。 public int getCount() &#123; return count.get(); &#125;&#125; AtomicInteger 线程安全原理简单分析AtomicInteger 类的部分源码： 12345678910111213// setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用）private static final Unsafe unsafe = Unsafe.getUnsafe();private static final long valueOffset;static &#123; try &#123; // 拿到"value"的内存地址 valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125;private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 数组类型原子类数组类型原子类介绍使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray ：引用类型数组原子类 上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerArray 为例子来介绍： AtomicIntegerArray 类常用方法： 1234567public final int get(int i) //获取 index=i 位置元素的值public final int getAndSet(int i, int newValue)//返回 index=i 位置的当前的值，并将其设置为新值：newValuepublic final int getAndIncrement(int i)//获取 index=i 位置元素的值，并让该位置的元素自增public final int getAndDecrement(int i) //获取 index=i 位置元素的值，并让该位置的元素自减public final int getAndAdd(int delta) //获取 index=i 位置元素的值，并加上预期的值boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将 index=i 位置的元素值设置为输入值（update）public final void lazySet(int i, int newValue)//最终 将index=i 位置的元素设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 AtomicIntegerArray 常见方法使用123456789101112131415161718192021import java.util.concurrent.atomic.AtomicIntegerArray;public class AtomicIntegerArrayTest &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub int temvalue = 0; int[] nums = &#123; 1, 2, 3, 4, 5, 6 &#125;; AtomicIntegerArray i = new AtomicIntegerArray(nums); for (int j = 0; j &lt; nums.length; j++) &#123; System.out.println(i.get(j)); &#125; temvalue = i.getAndSet(0, 2); System.out.println("temvalue:" + temvalue + "; i:" + i); temvalue = i.getAndIncrement(0); System.out.println("temvalue:" + temvalue + "; i:" + i); temvalue = i.getAndAdd(0, 5); System.out.println("temvalue:" + temvalue + "; i:" + i); &#125;&#125; 结果如下： 引用类型原子类引用类型原子类介绍基本类型原子类只能更新一个变量，如果需要原子更新多个变量，需要使用 引用类型原子类。 AtomicReference：引用类型原子类 AtomicStampedRerence：原子更新引用类型里的字段原子类 AtomicMarkableReference ：原子更新带有标记位的引用类型 上面三个类提供的方法几乎相同，所以我们这里以 AtomicReference 为例子来介绍： AtomicReference 类使用示例12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.concurrent.atomic.AtomicReference;public class AtomicReferenceTest &#123; public static void main(String[] args) &#123; AtomicReference&lt;Person&gt; ar = new AtomicReference&lt;Person&gt;(); Person person = new Person("SnailClimb", 22); ar.set(person); Person updatePerson = new Person("Daisy", 20); ar.compareAndSet(person, updatePerson); System.out.println(ar.get().getName()); System.out.println(ar.get().getAge()); &#125;&#125;class Person &#123; private String name; private int age; public Person(String name, int age) &#123; super(); this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 上述代码首先创建了一个 Person 对象，然后把 Person 对象设置进 AtomicReference 对象中，然后调用 compareAndSet 方法，该方法就是通过通过 CAS 操作设置 ar。如果 ar 的值为 person 的话，则将其设置为 updatePerson。实现原理与 AtomicInteger 类中的 compareAndSet 方法相同。 运行上面的代码后的输出结果如下： 对象的属性修改类型原子类对象的属性修改类型原子类介绍如果需要原子更新某个类里的某个字段时，需要用到对象的属性修改类型原子类。 AtomicIntegerFieldUpdater：原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 要想原子地更新对象的属性需要两步： 第一步、因为对象的属性修改类型原子类都是抽象类，所以每次使用都必须使用静态方法 newUpdater() 创建一个更新器，并且需要设置想要更新的类和属性。 第二步、更新的对象属性必须使用 public volatile 修饰符。 上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerFieldUpdater为例子来介绍： AtomicIntegerFieldUpdater 类使用示例123456789101112131415161718192021222324252627282930313233343536373839import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;public class AtomicIntegerFieldUpdaterTest &#123; public static void main(String[] args) &#123; AtomicIntegerFieldUpdater&lt;User&gt; a = AtomicIntegerFieldUpdater.newUpdater(User.class, "age"); User user = new User("Java", 22); System.out.println(a.getAndIncrement(user));// 22 System.out.println(a.get(user));// 23 &#125;&#125;class User &#123; private String name; public volatile int age; public User(String name, int age) &#123; super(); this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 结果如下： 参考： https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/Multithread/Atomic.md#31-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%E5%8E%9F%E5%AD%90%E7%B1%BB%E4%BB%8B%E7%BB%8D]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试必备之乐观锁与悲观锁]]></title>
    <url>%2F2019%2F04%2F27%2F%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87%E4%B9%8B%E4%B9%90%E8%A7%82%E9%94%81%E4%B8%8E%E6%82%B2%E8%A7%82%E9%94%81%2F</url>
    <content type="text"><![CDATA[何谓悲观锁与乐观锁 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 两种锁的使用场景从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1. 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2. CAS算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 关于自旋锁，大家可以看一下这篇文章，非常不错：《 面试必备之深入理解自旋锁》 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 1. ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2. 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用： 第一、它可以延迟流水线执行指令（de-pipeline），使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。 第二、它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3. 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作。所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 参考： https://github.com/Snailclimb/JavaGuide/blob/master/docs/essential-content-for-interview/%E9%9D%A2%E8%AF%95%E5%BF%85%E5%A4%87%E4%B9%8B%E4%B9%90%E8%A7%82%E9%94%81%E4%B8%8E%E6%82%B2%E8%A7%82%E9%94%81.md]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[synchronized关键字]]></title>
    <url>%2F2019%2F04%2F26%2Fsynchronized%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[synchronized关键字最主要的三种使用方式的总结 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法，作用于当前类对象，进入同步代码前要获得当前类对象的锁 ，也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份，所以对该类的所有对象都加了锁）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 和 synchronized 方法一样，synchronized(this)代码块也是锁定当前对象的。synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。这里再提一下：synchronized关键字加到非 static 静态方法上是给对象实例上锁。 另外需要注意的是：尽量不要使用 synchronized(String a) ，因为JVM中，字符串常量池具有缓冲功能！ 下面我已一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全）1234567891011121314151617181920public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance = new Singleton()， 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 synchronized 关键字底层原理总结synchronized 关键字底层原理属于 JVM 层面 synchronized 同步语句块的情况1234567public class SynchronizedDemo &#123; public void method() &#123; synchronized (this) &#123; System.out.println("synchronized 代码块"); &#125; &#125;&#125; 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor (monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 synchronized 修饰方法的的情况12345public class SynchronizedDemo2 &#123; public synchronized void method() &#123; System.out.println("synchronized 方法"); &#125;&#125; synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取而代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 在 Java 早期版本中，synchronized 属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对 synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。 JDK1.6 之后的底层优化JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。 锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 偏向锁引入偏向锁的目的和引入轻量级锁的目的很像，他们都是为了没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉。 偏向锁的“偏”就是偏心的偏，它的意思是会偏向于第一个获得它的线程，如果在接下来的执行中，该锁没有被其他线程获取，那么持有偏向锁的线程就不需要进行同步！关于偏向锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。 但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。 轻量级锁倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)。轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 关于轻量级锁的加锁和解锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。（互斥量的意思就是本质上的信号量机制问题） 轻量级锁能够提升程序同步性能的依据是“对于绝大部分锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用 CAS 操作避免了使用互斥操作的开销。但如果存在锁竞争，除了互斥量开销外，还会额外发生CAS操作，因此在有锁竞争的情况下，轻量级锁比传统的重量级锁更慢！如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！ 自旋锁和自适应自旋轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。 互斥同步对性能最大的影响就是阻塞的实现，因为挂起线程/恢复线程的操作都需要转入内核态中完成（用户态转换到内核态会耗费时间）。 一般线程持有锁的时间都不是太长，所以仅仅为了这一点时间去挂起线程/恢复线程是得不偿失的。 所以，虚拟机的开发团队就这样去考虑：“我们能不能让后面来的请求获取锁的线程等待一会而不被挂起呢？看看持有锁的线程是否很快就会释放锁”。为了让一个线程等待，我们只需要让线程执行一个忙循环（自旋），这项技术就叫做自旋。 百度百科对自旋锁的解释： 何谓自旋锁？它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 自旋锁在 JDK1.6 之前其实就已经引入了，不过是默认关闭的，需要通过--XX:+UseSpinning参数来开启。JDK1.6及1.6之后，就改为默认开启的了。需要注意的是：自旋等待不能完全替代阻塞，因为它还是要占用处理器时间。如果锁被占用的时间短，那么效果当然就很好了！反之，相反！自旋等待的时间必须要有限度。如果自旋超过了限定次数任然没有获得锁，就应该挂起线程。自旋次数的默认值是10次，用户可以修改–XX:PreBlockSpin来更改。 另外,在 JDK1.6 中引入了自适应的自旋锁。自适应的自旋锁带来的改进就是：自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定，虚拟机变得越来越“聪明”了。 锁消除锁消除理解起来很简单，它指的就是虚拟机即使编译器在运行时，如果检测到那些共享数据不可能存在竞争，那么就执行锁消除。锁消除可以节省毫无意义的请求锁的时间。 锁粗化原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，直在共享数据的实际作用域才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待线程也能尽快拿到锁。 大部分情况下，上面的原则都是没有问题的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，那么会带来很多不必要的性能消耗。 Synchronized 和 ReenTrantLock 的对比两者都是可重入锁两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 APIsynchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。 ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ReenTrantLock 比 synchronized 增加了一些高级功能相比synchronized，ReenTrantLock增加了一些高级功能。 主要来说主要有三点：①等待可中断；②可实现公平锁；③可实现选择性通知（锁可以绑定多个条件） ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法只会唤醒注册在该Condition实例中的所有等待线程。 性能已不是选择标准在JDK1.6之前，synchronized 的性能是比 ReenTrantLock 差很多。 具体表示为：synchronized 关键字吞吐量随线程数的增加，下降得非常严重。而ReenTrantLock 基本保持一个比较稳定的水平。我觉得这也侧面反映了， synchronized 关键字还有非常大的优化余地。后续的技术发展也证明了这一点，我们上面也讲了在 JDK1.6 之后 JVM 团队对 synchronized 关键字做了很多优化。JDK1.6 之后，synchronized 和 ReenTrantLock 的性能基本是持平了。所以网上那些说因为性能才选择 ReenTrantLock 的文章都是错的！JDK1.6之后，性能已经不是选择synchronized和ReenTrantLock的影响因素了！而且虚拟机在未来的性能改进中会更偏向于原生的synchronized，所以还是提倡在synchronized能满足你的需求的情况下，优先考虑使用synchronized关键字来进行同步！优化后的synchronized和ReenTrantLock一样，在很多地方都是用到了CAS操作。 参考 https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/synchronized.md]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的理解Spring AOP吗]]></title>
    <url>%2F2019%2F04%2F25%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E7%90%86%E8%A7%A3Spring-AOP%E5%90%97%2F</url>
    <content type="text"><![CDATA[概述为什么会有面向切面编程（AOP）？ 我们知道Java是一个面向对象(OOP)的语言，但它有一些弊端,比如当我们需要为多个不具有继承关系的对象引入一个公共行为，例如日志,权限验证,事务等功能时，只能在在每个对象里引用公共行为，这样做不便于维护，而且有大量重复代码。AOP的出现弥补了OOP的这点不足。 为了阐述清楚Spring AOP，我们从将以下方面进行讨论： 代理模式 静态代理原理及实践 动态代理原理及实践 Spring AOP原理及实战 代理模式代理模式：为其他对象提供一种代理以控制对这个对象的访问。这段话比较官方，但我更倾向于用自己的语言理解：比如A对象要做一件事情，在没有代理前，自己来做；在对 A 代理后，由 A 的代理类 B 来做。 代理其实是在原实例前后加了一层处理，这也是 AOP 的初级轮廓。 静态代理原理及实践静态代理模式：静态代理说白了，就是在程序运行前就已经存在代理类的字节码文件，代理类和原始类的关系在运行前就已经确定。 源码实例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243package test.staticProxy; // 接口public interface IUserDao &#123; void save(); void find();&#125; //目标对象class UserDao implements IUserDao&#123; @Override public void save() &#123; System.out.println("模拟：保存用户！"); &#125; @Override public void find() &#123; System.out.println("模拟：查询用户"); &#125;&#125; /** * 静态代理 * 特点： * 1. 目标对象必须要实现接口 * 2. 代理对象，要实现与目标对象一样的接口 */class UserDaoProxy implements IUserDao&#123; // 代理对象，需要维护一个目标对象 private IUserDao target = new UserDao(); @Override public void save() &#123; System.out.println("代理操作： 开启事务..."); target.save(); // 执行目标对象的方法 System.out.println("代理操作：提交事务..."); &#125; @Override public void find() &#123; target.find(); &#125;&#125; 测试结果： 静态代理虽然保证了业务类只需关注逻辑本身，代理对象的一个接口只服务于一种类型的对象。如果要代理的方法很多，势必要为每一种方法都进行代理。再者，如果增加一个方法，除了实现类需要实现这个方法外，所有的代理类也要实现此方法。增加了代码的维护成本。那么要如何解决呢？答案是使用动态代理。 动态代理原理及实践动态代理模式：动态代理类的源码是在程序运行期间，通过 JVM 反射等机制动态生成。代理类和委托类的关系是运行时才确定的。 源码实例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package test.dynamicProxy; import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy; // 接口public interface IUserDao &#123; void save(); void find();&#125; //目标对象class UserDao implements IUserDao&#123; @Override public void save() &#123; System.out.println("模拟： 保存用户！"); &#125; @Override public void find() &#123; System.out.println("查询"); &#125;&#125; /** * 动态代理： * 代理工厂，给多个目标对象生成代理对象！ * */class ProxyFactory &#123; // 接收一个目标对象 private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; // 返回对目标对象(target)代理后的对象(proxy) public Object getProxyInstance() &#123; Object proxy = Proxy.newProxyInstance( target.getClass().getClassLoader(), // 目标对象使用的类加载器 target.getClass().getInterfaces(), // 目标对象实现的所有接口 new InvocationHandler() &#123; // 执行代理对象方法时候触发 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; // 获取当前执行的方法的方法名 String methodName = method.getName(); // 方法返回值 Object result = null; if ("find".equals(methodName)) &#123; // 直接调用目标对象方法 result = method.invoke(target, args); &#125; else &#123; System.out.println("开启事务..."); // 执行目标对象方法 result = method.invoke(target, args); System.out.println("提交事务..."); &#125; return result; &#125; &#125; ); return proxy; &#125;&#125; 测试结果如下： 1IUserDao proxy = (IUserDao)new ProxyFactory(target).getProxyInstance(); 其实是 JDK 动态生成了一个类去实现接口，隐藏了这个过程： 1class $jdkProxy implements IUserDao&#123;&#125; 使用 JDK 生成的动态代理的前提是目标类必须有实现的接口。 这里又引入一个问题，如果某个类没有实现接口，就不能使用 JDK 动态代理。所以 CGLIB 代理就是解决这个问题的。 CGLIB 是以动态生成的子类继承目标类的方式实现，在运行期动态的在内存中构建一个子类，如下： 1234public class UserDao&#123;&#125; // CGLIB是以动态生成的子类继承目标类的方式实现,程序执行时,隐藏了下面的过程。public class $Cglib_Proxy_class extends UserDao&#123;&#125; CGLIB 使用的前提是目标类不能为 final 修饰，因为 final 修饰的类不能被继承。 现在，我们可以看看 AOP 的定义：面向切面编程，核心原理是使用动态代理模式在方法执行前后或出现异常时加入相关逻辑。 通过定义和前面代码我们可以发现3点： AOP 是基于动态代理模式。 AOP 是方法级别的。 AOP 可以分离业务代码和关注点代码（重复代码），在执行业务代码时，动态的注入关注点代码。切面就是关注点代码形成的类。 Spring AOP前文提到 JDK 代理和 CGLIB 代理两种动态代理。优秀的 Spring 框架把两种方式在底层都集成了进去，我们无需担心自己去实现动态生成代理。那么，Spring是如何生成代理对象的？ 创建容器对象的时候，根据切入点表达式拦截的类，生成代理对象。 如果目标对象有实现接口，使用 JDK 代理。如果目标对象没有实现接口，则使用 CGLIB 代理。然后从容器获取代理后的对象，在运行期植入“切面”类的方法。 通过查看 Spring 源码，我们在 DefaultAopProxyFactory 类中，找到这样一段话： 简单的从字面意思看出：如果有接口，则使用 JDK 代理，反之使用 CGLIB ，这刚好印证了前文所阐述的内容。 Spring AOP 综合两种代理方式的使用前提有会如下结论：如果目标类没有实现接口，且 class 为 final 修饰的，则不能进行 Spring AOP 编程！ 现在我们将自己手动实现 Spring 的 AOP： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package test.spring_aop_anno; import org.aspectj.lang.ProceedingJoinPoint; public interface IUserDao &#123; void save();&#125; // 用于测试 CGLIB 动态代理class OrderDao &#123; public void save() &#123; //int i =1/0; 用于测试异常通知 System.out.println("保存订单..."); &#125;&#125; //用于测试 JDK 动态代理class UserDao implements IUserDao &#123; public void save() &#123; //int i =1/0; 用于测试异常通知 System.out.println("保存用户..."); &#125;&#125; //切面类class TransactionAop &#123; public void beginTransaction() &#123; System.out.println("[前置通知] 开启事务.."); &#125; public void commit() &#123; System.out.println("[后置通知] 提交事务.."); &#125; public void afterReturing() &#123; System.out.println("[返回后通知]"); &#125; public void afterThrowing() &#123; System.out.println("[异常通知]"); &#125; public void arroud(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println("[环绕前：]"); pjp.proceed(); // 执行目标方法 System.out.println("[环绕后：]"); &#125;&#125;package test.spring_aop_anno; import org.aspectj.lang.ProceedingJoinPoint; public interface IUserDao &#123; void save();&#125; // 用于测试 CGLIB 动态代理class OrderDao &#123; public void save() &#123; //int i =1/0; 用于测试异常通知 System.out.println("保存订单..."); &#125;&#125; //用于测试 JDK 动态代理class UserDao implements IUserDao &#123; public void save() &#123; //int i =1/0; 用于测试异常通知 System.out.println("保存用户..."); &#125;&#125; //切面类class TransactionAop &#123; public void beginTransaction() &#123; System.out.println("[前置通知] 开启事务.."); &#125; public void commit() &#123; System.out.println("[后置通知] 提交事务.."); &#125; public void afterReturing() &#123; System.out.println("[返回后通知]"); &#125; public void afterThrowing() &#123; System.out.println("[异常通知]"); &#125; public void arroud(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println("[环绕前：]"); pjp.proceed(); // 执行目标方法 System.out.println("[环绕后：]"); &#125;&#125; Spring 的 XML 配置文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd"&gt; &lt;!-- dao实例加入容器 --&gt; &lt;bean id="userDao" class="test.spring_aop_anno.UserDao"&gt;&lt;/bean&gt; &lt;!-- dao实例加入容器 --&gt; &lt;bean id="orderDao" class="test.spring_aop_anno.OrderDao"&gt;&lt;/bean&gt; &lt;!-- 实例化切面类 --&gt; &lt;bean id="transactionAop" class="test.spring_aop_anno.TransactionAop"&gt;&lt;/bean&gt; &lt;!-- Aop相关配置 --&gt; &lt;aop:config&gt; &lt;!-- 切入点表达式定义 --&gt; &lt;aop:pointcut expression="execution(* test.spring_aop_anno.*Dao.*(..))" id="transactionPointcut"/&gt; &lt;!-- 切面配置 --&gt; &lt;aop:aspect ref="transactionAop"&gt; &lt;!-- 【环绕通知】 --&gt; &lt;aop:around method="arroud" pointcut-ref="transactionPointcut"/&gt; &lt;!-- 【前置通知】 在目标方法之前执行 --&gt; &lt;aop:before method="beginTransaction" pointcut-ref="transactionPointcut" /&gt; &lt;!-- 【后置通知】 --&gt; &lt;aop:after method="commit" pointcut-ref="transactionPointcut"/&gt; &lt;!-- 【返回后通知】 --&gt; &lt;aop:after-returning method="afterReturing" pointcut-ref="transactionPointcut"/&gt; &lt;!-- 异常通知 --&gt; &lt;aop:after-throwing method="afterThrowing" pointcut-ref="transactionPointcut"/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 代码的测试结果如下： 最后，我们来聊聊Spring AOP用来做什么？ Spring声明式事务管理配置：请参考博主的另一篇文章：分布式系统架构实战 demo：SSM+Dubbo Controller层的参数校验：参考 Spring AOP拦截Controller做参数校验 使用 Spring AOP 实现 MySQL 数据库读写分离案例分析 在执行方法前，判断是否具有权限 对部分函数的调用进行日志记录：监控部分重要函数，若抛出指定的异常，可以以短信或邮件方式通知相关人员。 信息过滤，页面转发等等功能 参考： http://www.importnew.com/31318.html#comment-765192]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的会写单例模式吗]]></title>
    <url>%2F2019%2F04%2F25%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BC%9A%E5%86%99%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E5%90%97%2F</url>
    <content type="text"><![CDATA[概述 “你知道茴香豆的‘茴’字有几种写法吗？” 纠结单例模式有几种写法有用吗？有点用，面试中经常选择其中一种或几种写法作为话头，考查设计模式和coding style的同时，还很容易扩展到其他问题。这里讲解几种猴子常用的写法，但切忌生搬硬套，去记“茴香豆的写法”。编程最大的乐趣在于“know everything, control everything”。 面试中单例模式有几种写法？ 大体可分为4类，下面分别介绍他们的基本形式、变种及特点。 饱汉模式基础的饱汉(单线程写法)饱汉，即已经吃饱，不着急再吃，饿的时候再吃。所以他就先不初始化单例，等第一次使用的时候再初始化，即“懒加载”。 写法：由私有构造器和一个公有静态工厂方法构成，在工厂方法中对singleton进行null判断，如果是null就new一个出来，最后返回singleton对象。 注意：这种方法可以实现延时加载，但是有一个致命弱点：线程不安全。如果有两条线程同时调用getInstance()方法，就有很大可能导致重复创建对象。 123456789101112131415// 饱汉// UnThreadSafepublic class Singleton1 &#123; private static Singleton1 singleton = null; private Singleton1() &#123;&#125; public static Singleton1 getInstance() &#123; if (singleton == null) &#123; singleton = new Singleton1(); &#125; return singleton; &#125;&#125; 饱汉模式的核心就是懒加载。好处是启动速度快、节省资源，一直到实例被第一次访问，才需要初始化单例；小坏处是写起来麻烦，大坏处是线程不安全，if语句存在竞态条件。 这种写法写起来麻烦不是大问题，但是可读性好啊。因此，单线程环境下，基础饱汉是我们最喜欢的写法。但多线程环境下，基础饱汉就彻底不可用了。下面的几种变种都在试图解决基础饱汉线程不安全的问题。 饱汉 - 变种 1最粗暴的犯法是用synchronized关键字修饰getInstance()方法，这样能达到绝对的线程安全。 1234567891011121314151617181920212223242526272829303132// 饱汉// ThreadSafepublic class Singleton1_1 &#123; private static Singleton1_1 singleton = null; private Singleton1_1() &#123;&#125; public synchronized static Singleton1_1 getInstance() &#123; if (singleton == null) &#123; singleton = new Singleton1_1(); &#125; return singleton; &#125;&#125;或public class Singleton &#123; private static volatile Singleton singleton = null; private Singleton() &#123;&#125; public static Singleton getSingleton()&#123; synchronized (Singleton.class)&#123; if(singleton == null)&#123; singleton = new Singleton(); &#125; &#125; return singleton; &#125; &#125; 变种1的好处是写起来简单，且绝对线程安全；坏处是并发性能极差，事实上完全退化到了串行。单例只需要初始化一次，但就算初始化以后，synchronized的锁也无法避开，从而getInstance()完全变成了串行操作。性能不敏感的场景建议使用。 饱汉 - 变种 2变种2是“臭名昭著”的DCL 1.0。 针对变种1中单例初始化后锁仍然无法避开的问题，变种2在变种1的外层又套了一层check，加上synchronized内层的check，即所谓“双重检查锁”（Double Check Lock，简称DCL）。 12345678910111213141516171819202122// 饱汉// UnThreadSafepublic class Singleton1_2 &#123; private static Singleton1_2 singleton = null; public int f1 = 1;// 触发部分初始化问题 public int f2 = 2; private Singleton1_2() &#123;&#125; public static Singleton1_2 getInstance() &#123; // may get half object if (singleton == null) &#123; synchronized (Singleton1_2.class) &#123; if (singleton == null) &#123; singleton = new Singleton1_2(); &#125; &#125; &#125; return singleton; &#125;&#125; 变种2的核心是DCL，看起来变种2似乎已经达到了理想的效果：懒加载+线程安全。可惜的是，正如注释中所说，DCL仍然是线程不安全的，由于指令重排序，你可能会得到“半个对象”，即”部分初始化“问题。 饱汉 - 变种 3(兼顾线程安全和效率的写法)变种3专门针对变种2，可谓DCL 2.0。 针对变种3的“半个对象”问题，变种3在实例上增加了volatile关键字。 123456789101112131415161718192021// 饱汉// ThreadSafepublic class Singleton1_3 &#123; private static volatile Singleton1_3 singleton = null; public int f1 = 1; // 触发部分初始化问题 public int f2 = 2; private Singleton1_3() &#123;&#125; public static Singleton1_3 getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton1_3.class) &#123; // must be a complete instance if (singleton == null) &#123; singleton = new Singleton1_3(); &#125; &#125; &#125; return singleton; &#125;&#125; 这种写法被称为”双重检查锁”，顾名思义，就是在getInstance()方法中，进行两次null检查。 看似多此一举，但实际上却极大提升了并发度，进而提升了性能。 为什么可以提高并发度呢？就像上文说的，在单例中new的情况非常少，绝大多数都是可以并行的读操作。因此在加锁前多进行一次null检查就可以减少绝大多数的加锁操作，执行效率提高的目的也就达到了。 多线程环境下，变种3更适用于性能敏感的场景。但后面我们将了解到，就算是线程安全的，还有一些办法能破坏单例。 注意：那么，这种写法是不是绝对安全呢？前面说了，从语义角度来看，并没有什么问题。但是其实还是有坑，说这个坑之前我们要先来看看volatile这个关键字。其实这个关键字有两层语义。第一层语义相信大家都比较熟悉，就是可见性。可见性指的是在一个线程中对该变量的修改会马上由工作内存（Work Memory）写回主内存（Main Memory），所以会马上反应在其它线程的读取操作中。顺便一提，工作内存和主内存可以近似理解为实际电脑中的高速缓存和主存，工作内存是线程独享的，主存是线程共享的。volatile的第二层语义是禁止指令重排序优化。大家知道我们写的代码（尤其是多线程代码），由于编译器优化，在实际执行的时候可能与我们编写的顺序不同。编译器只保证程序执行结果与源代码相同，却不保证实际指令的顺序与源代码相同。这在单线程看起来没什么问题，然而一旦引入多线程，这种乱序就可能导致严重问题。 volatile关键字就可以从语义上解决这个问题。虽然从语义上讲是没有问题的，但是很不幸，禁止指令重排优化这条语义直到jdk1.5以后才能正确工作。此前的JDK中即使将变量声明为volatile也无法完全避免重排序所导致的问题。所以，在jdk1.5版本前，双重检查锁形式的单例模式是无法保证线程安全的。 饿汉模式顾名思义，饿汉法就是在第一次引用该类的时候就创建对象实例，而不管实际是否需要创建。 与饱汉相对，饿汉很饿，只想着尽早吃到。所以他就在最早的时机，即类加载时初始化单例，以后访问时直接返回即可。 1234567891011// 饿汉// ThreadSafepublic class Singleton2 &#123; private static final Singleton2 singleton = new Singleton2(); private Singleton2() &#123;&#125; public static Singleton2 getInstance() &#123; return singleton; &#125;&#125; 饿汉的好处是天生的线程安全（得益于类加载机制），写起来超级简单，使用时没有延迟；坏处是有可能造成资源浪费（如果类加载后就一直不使用单例的话）。 值得注意的时，单线程环境下，饿汉与饱汉在性能上没什么差别；但多线程环境下，由于饱汉需要加锁，饿汉的性能反而更优。 Holder模式(静态内部类法)我们既希望利用饿汉模式中静态变量的方便和线程安全，又希望通过懒加载规避资源浪费。 Holder模式满足了这两点要求： 核心仍然是静态变量，足够方便和线程安全；通过静态的Holder类持有真正实例，间接实现了懒加载。 123456789101112131415// Holder模式// ThreadSafepublic class Singleton3 &#123; private static class SingletonHolder &#123; private static final Singleton3 singleton = new Singleton3(); private SingletonHolder() &#123;&#125; &#125; private Singleton3() &#123;&#125; public static Singleton3 getInstance() &#123; return SingletonHolder.singleton; &#125;&#125; 相对于饿汉模式，Holder模式仅增加了一个静态内部类的成本，与饱汉的变种3效果相当（略优），都是比较受欢迎的实现方式，同样建议考虑。 注意上面提到的所有实现方式都有两个共同的缺点： 都需要额外的工作(Serializable、transient、readResolve())来实现序列化，否则每次反序列化一个序列化的对象实例时都会创建一个新的实例。 可能会有人使用反射强行调用我们的私有构造器（如果要避免这种情况，可以修改构造器，让它在创建第二个实例的时候抛异常）。 枚举模式还有一种更加优雅的方法来实现单例模式，那就是枚举写法。 通过反射或序列化，我们仍然能够访问到私有构造器，创建新的实例破坏单例模式。此时，只有枚举模式能天然防范这一问题。 用枚举实现单例模式，相当好用，但可读性是不存在的。 基础的枚举将枚举的静态成员变量作为单例的实例： 1234567891011121314// 枚举// ThreadSafepublic enum Singleton4 &#123; SINGLETON; private String name; public String getName()&#123; return name; &#125; public void setName(String name)&#123; this.name = name; &#125;&#125; 代码量比饿汉模式更少，但用户只能直接访问实例Singleton4.SINGLETON——事实上，这样的访问方式作为单例使用也是恰当的，只是牺牲了静态工厂方法的优点，如无法实现懒加载。 使用枚举除了线程安全和防止反射强行调用构造器之外，还提供了自动序列化机制，防止反序列化的时候创建新的对象。 丑陋但好用的语法糖Java的枚举是一个“丑陋但好用的语法糖”。 通过反编译打开语法糖，就看到了枚举类型的本质，简化如下： 1234567// 枚举// ThreadSafepublic class Singleton4 extends Enum&lt;Singleton4&gt; &#123; ... public static final Singleton4 SINGLETON = new Singleton4(); ...&#125; 本质上和饿汉模式相同，区别仅在于公有的静态成员变量。 用枚举实现一些trick 这一部分与单例没什么关系，可以跳过。如果选择阅读也请认清这样的事实：虽然枚举相当灵活，但如何恰当的使用枚举有一定难度。一个足够简单的典型例子是TimeUnit类，建议有时间耐心阅读。 上面已经看到，枚举型单例的本质仍然是一个普通的类。实际上，我们可以在枚举型型单例上增加任何普通类可以完成的功能。要点在于枚举实例的初始化，可以理解为实例化了一个匿名内部类。为了更明显，我们在Singleton4_1中定义一个普通的私有成员变量，一个普通的公有成员方法，和一个公有的抽象成员方法，如下： 123456789101112131415161718192021222324252627// 枚举// ThreadSafepublic enum Singleton4_1 &#123; SINGLETON("enum is the easiest singleton pattern, but not the most readable") &#123; public void testAbsMethod() &#123; print(); System.out.println("enum is ugly, but so flexible to make lots of trick"); &#125; &#125;; private String comment = null; Singleton4_1(String comment) &#123; this.comment = comment; &#125; public void print() &#123; System.out.println("comment=" + comment); &#125; abstract public void testAbsMethod(); public static Singleton4_1 getInstance() &#123; return SINGLETON; &#125;&#125; 这样，枚举类Singleton4_1中的每一个枚举实例不仅继承了父类Singleton4_1的成员方法print()，还必须实现父类Singleton4_1的抽象成员方法testAbsMethod()。 总结 实现方式 关键点 资源浪费 线程安全 多线程环境的性能足够优化 基础饱汉 懒加载 否 否 - 饱汉变种1 懒加载、同步 否 是 否 饱汉变种2 懒加载、DCL 否 否 - 饱汉变种3 懒加载、DCL、volatile 否 是 是 饿汉 静态变量初始化 是 是 是 Holder 静态变量初始化、holder 否 是 是 枚举 枚举本质、静态变量初始化 否 是 是 最后，不管采取何种方案，请时刻牢记单例的三大要点： 线程安全 延迟加载 序列化与反序列化安全 参考： http://www.importnew.com/27056.html http://www.tekbroaden.com/singleton-java.html]]></content>
      <categories>
        <category>java面试准备</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解JAVA存储模型的Happens-Before规则]]></title>
    <url>%2F2019%2F04%2F21%2F%E7%90%86%E8%A7%A3JAVA%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9E%8B%E7%9A%84Happens-Before%E8%A7%84%E5%88%99%2F</url>
    <content type="text"><![CDATA[背景知识Java内存模型JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程的共享变量的副本。主要目标是定义程序中各个变量的访问规则**。** 如果线程 A 和线程 B 要通信的话，要如下两个步骤： 1、线程A需要将本地内存A中的共享变量副本刷新到主内存去； 2、线程B去主内存读取线程A之前已更新过的共享变量。 JMM的特性1）原子性：原子性指的是一个操作是不可中断的。JVM自身提供的对基本数据类型读写操作的原子性外，对于方法级别或者代码块级别的原子性操作，可以使用synchronized关键字或者重入锁(ReentrantLock)保证程序执行的原子性。 2）可见性：可见性指的是当一个线程修改了某个共享变量的值，其他线程能够马上得知这个修改的值。 3）有序性：有序性是指对于单线程的执行代码，可以认为代码的执行是按顺序依次执行的，但对于多线程环境，则可能出现乱序现象。 Happens-Before规则除了靠sychronized和volatile关键字来保证原子性、可见性以及有序性外，JMM内部还定义一套happens-before原则来保证多线程环境下两个操作间的原子性、可见性以及有序性，happens-before 原则也是判断数据是否存在竞争、线程是否安全的依据。 happens-before 原则：指的是前一个动作的结果对后一个动作是可见的。 “Happens-before”规则都有哪些（摘自《Java并发编程实践》）： ① 程序次序法则：线程中的每个动作A都happens-before于该线程中的每一个动作B，其中，在程序中，所有的动作B都能出现在A之后。 ② 监视器锁法则：对一个监视器锁的解锁 happens-before 于每一个后续对同一监视器锁的加锁。 ③ volatile变量法则：对volatile域的写入操作happens-before于每一个后续对同一个域的读写操作。④ 线程启动法则：在一个线程里，对Thread.start的调用会happens-before于每个启动线程的动作。⑤ 线程终结法则：线程中的任何动作都happens-before于其他线程检测到这个线程已经终结、或者从Thread.join调用中成功返回，或Thread.isAlive返回false。⑥ 中断法则：一个线程调用另一个线程的interrupt happens-before于被中断的线程发现中断。⑦ 终结法则：一个对象的构造函数的结束happens-before于这个对象finalizer的开始。⑧ 传递性：如果A happens-before于B，且B happens-before于C，则A happens-before于C 解释与分析在解释该规则之前，我们先看一段多线程访问数据的代码例子： 123456789101112131415public class Test1 &#123; private int a=1, b=2; public void foo()&#123; // 线程1 a=3; b=4; &#125; public int getA()&#123; // 线程2 return a; &#125; public int getB()&#123; // 线程2 return b; &#125;&#125; 上面的代码，当线程1执行foo方法的时候，线程2访问getA和getB会得到什么样的结果？ 结果如下： 1234A：a=1, b=2 // 都未改变B：a=3, b=4 // 都改变了C：a=3, b=2 // a改变了，b未改变D：a=1, b=4 // b改变了，a未改变 上面的A,B,C都好理解，但是D可能会出乎一些人的预料。一些不了解JMM的同学可能会问怎么可能 b=4语句会先于 a=3 执行？ 这是一个多线程之间内存可见性（Visibility）顺序不一致的问题，有两种可能会造成上面的D选项： 1) Java编译器的重排序(Reording)操作有可能导致执行顺序和代码顺序不一致 关于Reordering： Java语言规范规定了JVM要维护内部线程类似顺序化语义(within-thread as-is-serial semantics)：只要程序的最终结果等同于它在严格的顺序化环境中执行的结果，那么上述所有的行为都是允许的。 简单的说：假设代码有两条语句，代码顺序是语句1先于语句2执行；那么只要语句2不依赖于语句1的结果，打乱它们的顺序对最终的结果没有影响的话，那么真正交给CPU去执行时，他们的顺序可以是没有限制的，即可以允许语句2先于语句1被CPU执行，和代码中的顺序不一致。 重排序（Reordering）是JVM针对现代CPU的一种优化，Reordering后的指令会在性能上有很大提升。(不知道这种优化对于多核CPU是否更加明显，也或许和单核多核没有关系。) 因为我们例子中的两条赋值语句，并没有依赖关系，无论谁先谁后结果都是一样的，所以就可能有Reordering的情况，这种情况下，对于其他线程来说就可能造成了可见性顺序不一致的问题。 2) 从线程工作内存写回主存时顺序无法保证 下图描述了JVM中主存和线程工作内存之间的交互： JLS中对线程和主存互操作定义了6个行为，分别为load、save、read、write、assign、use，这些操作行为具有原子性，且相互依赖，有明确的调用先后顺序。这个细节也比较繁琐，我们暂不深入追究，下面再解释。先简单认为线程在修改一个变量时，先拷贝入线程工作内存中，在线程工作内存修改后再写回主存(Main Memery)中。 下面介绍一下内存交互操作： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用。 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 假设例子中Reording后顺序仍与代码中的顺序一致，那么接下来呢？有意思的事情就发生在线程把Working Copy Memery中的变量写回Main Memery的时刻，线程1把变量写回Main Memery的过程对线程2的可见性顺序也是无法保证的。上面的例子中，a=3; b=4； 这两个语句在 Working Copy Memery中执行后，写回主存的过程对于线程2来说同样可能出现先b=4，后a=3，这样的相反顺序。 JMM为所有程序内部动作定义了一个偏序关系，叫做happens-before。要想保证执行动作B的线程看到动作A的结果（无论A和B是否发生在同一个线程中），A和B之间就必须满足happens-before关系。 上面的happens-before规则中我们重点关注的是②，③，这两条也是我们通常编程中常用的。 例如在ConcurrenHashMap中使用到锁(ReentrantLock)、Volatile、final等手段来保证happens-before规则的。 使用锁方式实现“Happens-before”是最简单，容易理解的： 早期Java中的锁只有最基本的synchronized，它是一种互斥的实现方式。在Java5之后，增加了一些其它锁，比如ReentrantLock，它基本作用和synchronized相似，但提供了更多的操作方式，比如在获取锁时不必像synchronized那样只是傻等，可以设置定时，轮询，或者中断，这些方法使得它在获取多个锁的情况可以避免死锁操作。 而我们需要了解的是ReentrantLock的性能相对synchronized来说有很大的提高。在ConcurrentHashMap中，每个hash区间使用的锁正是ReentrantLock。 volatile关键字volatile内存语义1、保证内存可见性 2、防止指令重排 3、volatile并不保证操作的原子性 volatile保证可见性的原理是在每次访问变量时都会进行一次刷新，因此每次访问都是主内存中最新的版本。所以volatile关键字的作用之一就是保证变量修改的实时可见性。volatile关键字通过提供“内存屏障”的方式来防止指令被重排序，为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 为何要有内存屏障？每个CPU都会有自己的缓存，缓存的目的就是为了提高性能，避免每次都要向内存取。但是这样的弊端也很明显：不能实时的和内存发生信息交换，分在不同CPU执行的不同线程对同一个变量的缓存值不同。 内存屏障是什么？硬件层的内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。 内存屏障有两个作用： A.阻止屏障两侧的指令重排序； B.强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效。 在指令前插入Load Barrier，可以让高速缓存中的数据失效，强制从主内存加载数据；在指令后插入Store Barrier，能让写入缓存中的最新数据更新写入主内存，让其他线程可见。 java的内存屏障有四种，即LoadLoad,StoreStore,LoadStore,StoreLoad。 LoadLoad屏障：对于Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕； StoreStore屏障：对于Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见； LoadStore屏障：对于Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕； StoreLoad屏障：对于Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能。 volatile语义中的内存屏障？1）在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障； ​ 在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障； 2）由于内存屏障的作用，避免了volatile变量和其它指令重排序、线程之间实现了通信，使得volatile表现出了锁的特性。 补充volatile可以看做一种轻量级的锁，但又和锁有些不同。a) 它对于多线程，不是一种互斥（mutex）关系。b) 用volatile修饰的变量，不能保证该变量状态的改变对于其他线程来说是一种“原子化操作”。 在Java5之前，JMM对Volatile的定义是：保证读写volatile都直接发生在main memory中，线程的working memory不进行缓存。它只承诺了读和写过程的可见性，并没有对Reording做限制，所以旧的Volatile并不太可靠。在Java5之后，JMM对volatile的语义进行了增强，就是我们看到的③volatile变量法则。 那对于“原子化操作”怎么理解呢？看下面例子： 12345private static volatile int nextSerialNum = 0; public static int generateSerialNumber()&#123; return nextSerialNum++;&#125; 上面代码中对nextSerialNum使用了volatile来修饰，根据前面“Happens-Before”法则的第三条Volatile变量法则，看似不同线程都会得到一个新的serialNumber 问题出在了 nextSerialNum++ 这条语句上，它不是一个原子化的，实际上是read-modify-write三项操作，这就有可能使得在线程1在write之前，线程2也访问到了nextSerialNum，造成了线程1和线程2得到一样的serialNumber。 所以，在使用Volatile时，需要注意：a) 需不需要互斥；b) 对象状态的改变是不是原子化的。 final关键字不变模式（immutable）是多线程安全里最简单的一种保障方式。因为你拿他没有办法，想改变它也没有机会。不变模式主要通过final关键字来限定的。 在JMM中final关键字还有特殊的语义。Final域使得确保初始化安全性（initialization safety）成为可能，初始化安全性让不可变形对象不需要同步就能自由地被访问和共享。 final域的内存语义 写final域的重排序规则：JMM禁止编译器在构造函数之外进行final域的写重排。编译器会在final域的写之后，构造函数的return之前，插入一个StoreStore屏障。 写final域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了。 读final域的重排序规则：在一个线程中，JMM禁止处理器重排序初次读对象引用与初次读该对象包含的final域。编译器会在读final域操作的前面插入一个LoadLoad屏障。 参考： http://www.importnew.com/21781.html]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap]]></title>
    <url>%2F2019%2F04%2F21%2FConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[概述ConcurrentHashMap是HashMap的线程安全版本的实现版本。 ConcurrentHashMap是conccurrent家族中的一个类，由于它可以高效地支持并发操作，以及被广泛使用，经典的开源框架Spring的底层数据结构就是使用ConcurrentHashMap实现的。与同是线程安全的老大哥HashTable相比，它已经更胜一筹，因为它的锁更加细化，而不是像HashTable一样为几乎每个方法都添加了synchronized锁，这样的锁无疑会影响到性能。 在JDK1.6中，ConcurrentHashMap将数据分成一段一段存储，给每一段数据配一把锁，当一个线程获得锁互斥访问一个段数据时，其他段的数据也可被其他线程访问；每个Segment拥有一把可重入锁，因此ConcurrentHashMap的分段锁数目即为Segment数组长度。 ConcurrentHashMap结构：每一个segment都是一个HashEntry&lt;K,V&gt;[] table， table中的每一个元素本质上都是一个HashEntry的单向队列（单向链表实现）。每一个segment都是一个HashEntry&lt;K,V&gt;[] table， table中的每一个元素本质上都是一个HashEntry的单向队列。 本文的分析的源码是JDK8的版本，与JDK6的版本有很大的差异。实现线程安全的思想也已经完全变了，它摒弃了Segment（锁段）的概念，而是启用了一种全新的方式实现,利用CAS算法。它沿用了与它同时期的HashMap版本的思想，底层依然由“数组”+链表+红黑树的方式思想，但是为了做到并发，又增加了很多辅助的类，例如TreeBin，Traverser等对象内部类。 源码分析重要属性首先来看几个重要的属性，与HashMap相同的就不再介绍了，这里重点解释一下sizeCtl这个属性。可以说它是ConcurrentHashMap中出镜率很高的一个属性，因为它是一个控制标识符，在不同的地方有不同用途，而且它的取值不同，也代表不同的含义。 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化 表示初始化或下一次进行扩容的大小，这一点类似于扩容阈值的概念。 它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor相对应。 相关属性的源码如下： 123456789101112131415161718192021222324252627282930313233343536373839 /** * 盛装Node元素的数组 它的大小是2的整数次幂 * Size is always a power of two. Accessed directly by iterators. */ transient volatile Node&lt;K,V&gt;[] table; /** * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. * hash表初始化或扩容时的一个控制位标识量。 * 负数代表正在进行初始化或扩容操作 * -1代表正在初始化 * -N 表示有N-1个线程正在进行扩容操作 * 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小 */ private transient volatile int sizeCtl; // 以下两个是用来控制扩容的时候 单线程进入的变量 /** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */ private static int RESIZE_STAMP_BITS = 16;/** * The bit shift for recording size stamp in sizeCtl. */ private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS; /* * Encodings for Node hash fields. See above for explanation. */ static final int MOVED = -1; // hash值是-1，表示这是一个forwardNode节点 static final int TREEBIN = -2; // hash值是-2 表示这时一个TreeBin节点 重要内部类NodeNode是最核心的内部类，它包装了key-value键值对，所有插入 ConcurrentHashMap 的数据都包装在这里面。它与 HashMap 中的定义很相似，但是有一些差别。 它对 value 和 next 属性设置了volatile同步锁，它不允许调用setValue方法直接改变Node的value域，它增加了find方法辅助map.get()方法。 相关源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val;//带有同步锁的value volatile Node&lt;K,V&gt; next;//带有同步锁的next指针 Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + "=" + val; &#125; //不允许直接改变value的值 public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125; &#125; TreeNode树节点类，另外一个核心的数据结构。当链表长度过长的时候，会转换为TreeNode。但是与HashMap不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成TreeNode放在TreeBin对象中，由TreeBin完成对红黑树的包装。而且TreeNode在ConcurrentHashMap继承自Node类，而并非HashMap中的继承自LinkedHashMap.Entry&lt;K,V&gt;类，也就是说TreeNode带有next指针，这样做的目的是方便基于TreeBin的访问。 TreeBin这个类并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。它代替了TreeNode的根节点，也就是说在实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象，这是与HashMap的区别。另外这个类还带有了读写锁。 可以看到在构造TreeBin节点时，仅仅指定了它的hash值为TREEBIN常量，这也就是个标识为。同时也看到我们熟悉的红黑树构造方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock //省略大部分代码/** * Creates bin with initial set of nodes headed by b. */ TreeBin(TreeNode&lt;K,V&gt; b) &#123; super(TREEBIN, null, null, null); this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = r;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root); &#125; ForwardingNode一个用于连接两个table的节点类。 它包含一个 nextTable 指针，用于指向下一张表。而且这个节点的key、value、next指针全部为null，它的hash值为 -1 ，这里面定义的find的方法是从nextTable里进行查询节点，而不是以自身为头节点进行查找。 123456789101112131415161718192021222324252627282930313233343536/** * A node inserted at head of bins during transfer operations. */ static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; if (eh &lt; 0) &#123; if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125; &#125; Unsafe与CAS在ConcurrentHashMap中，随处可以看到U, 大量使用了U.compareAndSwapXXX的方法，这个方法是利用一个CAS算法实现无锁化的修改值的操作，他可以大大降低锁代理的性能消耗。 这个算法的基本思想就是不断地去比较当前内存中的变量值与你指定的一个变量值是否相等，如果相等，则接受你指定的修改的值，否则拒绝你的操作。因为当前线程中的值已经不是最新的值，你的修改很可能会覆盖掉其他线程修改的结果。这一点与乐观锁，SVN的思想是比较类似的。 unsafe静态块unsafe代码块控制了一些属性的修改工作，比如最常用的SIZECTL 。 在这一版本的concurrentHashMap中，大量应用来的CAS方法进行变量、属性的修改工作。利用CAS进行无锁操作，可以大大提高性能。 1234567891011121314151617181920212223242526272829303132333435// Unsafe mechanics private static final sun.misc.Unsafe U; private static final long SIZECTL; private static final long TRANSFERINDEX; private static final long BASECOUNT; private static final long CELLSBUSY; private static final long CELLVALUE; private static final long ABASE; private static final int ASHIFT; static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField("sizeCtl")); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField("transferIndex")); BASECOUNT = U.objectFieldOffset (k.getDeclaredField("baseCount")); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField("cellsBusy")); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField("value")); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); int scale = U.arrayIndexScale(ak); if ((scale &amp; (scale - 1)) != 0) throw new Error("data type scale not a power of two"); ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; 三个核心方法ConcurrentHashMap定义了三个原子操作，用于对指定位置的节点进行操作。正是这些原子操作保证了ConcurrentHashMap的线程安全。 123456789101112131415161718 @SuppressWarnings("unchecked") //获得在i位置上的Node节点 static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE); &#125;//利用CAS算法设置i位置上的Node节点。之所以能实现并发是因为他指定了原来这个节点的值是多少//在CAS算法中，会比较内存中的值与你指定的这个值是否相等，如果相等才接受你的修改，否则拒绝你的修改//因此当前线程中的值并不是最新的值，这种修改可能会覆盖掉其他线程的修改结果 有点类似于SVN static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125;//利用volatile方法设置节点位置的值 static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v); &#125; 初始化方法initTable对于ConcurrentHashMap来说，调用它的构造方法仅仅是设置了一些参数而已。而整个table的初始化是在向ConcurrentHashMap中插入元素的时候发生的。如调用put、computeIfAbsent、compute、merge等方法的时候，初始化方法的调用时机是检查table==null。 初始化方法主要应用了关键属性sizeCtl 如果这个值小于0，表示其他线程正在进行初始化，就放弃这个操作。在这也可以看出ConcurrentHashMap的初始化只能由一个线程完成。如果获得了初始化权限，就用CAS方法将sizeCtl置为 -1 ，防止其他线程进入。初始化数组后，将sizeCtl的值改为0.75*n。 123456789101112131415161718192021222324252627/** * Initializes table, using the size recorded in sizeCtl. */ private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; //sizeCtl表示有其他线程正在进行初始化操作，把线程挂起。对于table的初始化工作，只能有一个线程在 进行。 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; //利用CAS方法把sizectl的值置为-1 表示本线程正在进行初始化 try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2);//相当于0.75*n 设置一个扩容的阈值 &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab; &#125; 扩容方法transfer当ConcurrentHashMap容量不足的时候，需要对table进行扩容。这个方法的基本思想跟HashMap是很像的，但是由于它是支持并发扩容的，所以要复杂的多。原因是它支持多线程进行扩容操作，而并没有加锁。 我想这样做的目的不仅仅是为了满足concurrent的要求，而是希望利用并发处理去减少扩容带来的时间影响。因为在扩容的时候，总是会涉及到从一个“数组”到另一个“数组”拷贝的操作，如果这个操作能够并发进行，那真真是极好的了。 整个扩容操作分为两个部分： 第一部分是构建一个nextTable，它的容量是原来的两倍，这个操作是单线程完成的。这个单线程的保证是通过RESIZE_STAMP_SHIFT这个常量经过一次运算来保证的，这个地方在后面会有提到； 第二个部分就是将原来table中的元素复制到nextTable中，这里允许多线程进行操作。 先来看一下单线程是如何完成的： 它的大体思想就是遍历、复制的过程。 首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素： 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要untreefi，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。 再看一下多线程是如何完成的： 如果遍历到的节点是forward节点，就向后继续遍历，再加上给节点上锁的机制，就完成了多线程的控制。 多线程遍历节点，处理了一个节点，就把对应点的值set为forward，另一个线程看到forward，就向后遍历。这样交叉就完成了复制工作，而且还很好的解决了线程安全的问题。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161 /** * 一个过渡的table表 只有在扩容的时候才会使用 */ private transient volatile Node&lt;K,V&gt;[] nextTable; /** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */ private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1];//构造一个nextTable对象 它的容量是原来的两倍 nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab);//构造一个连节点指针 用于 标志位 boolean advance = true;//并发扩容的关键属性 如果等于true 说明这个节点已经处理过 boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; //这个while循环体的作用就是在控制i-- 通过i--可以依次遍历原hash表中的节点 while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; //如果所有的节点都已经完成复制工作 就把nextTable赋值给table 清空临时对象 nextTable nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1);//扩容阈值设置为原来容量的1.5倍 依然相当于现 在容量的0.75倍 return; &#125; //利用CAS方法更新这个扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //如果遍历到的节点为空 则放入ForwardingNode指针 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //如果遍历到ForwardingNode节点，说明这个点已经被处理过了，则直接跳过 //这里是控制并发扩容的核心 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; //节点上锁 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; //如果fh&gt;=0 证明这是一个Node节点 if (fh &gt;= 0) &#123; int runBit = fh &amp; n; //以下的部分在完成的工作是构造两个链表 一个是原链表 另一个是原链表的反 序排列 Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //在nextTable的i位置上插入一个链表 setTabAt(nextTab, i, ln); //在nextTable的i+n的位置上插入另一个链表 setTabAt(nextTab, i + n, hn); //在table的i位置上插入forwardNode节点 表示已经处理过该节点 setTabAt(tab, i, fwd); //设置advance为true 返回到上面的while循环中 就可以执行i--操作 advance = true; &#125; //对TreeBin对象进行处理 与上面的过程类似 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; //构造正序和反序两个链表 for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; //如果扩容后已经不再需要tree的结构 反向转换为链表结构 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; //在nextTable的i位置上插入一个链表 setTabAt(nextTab, i, ln); //在nextTable的i+n的位置上插入另一个链表 setTabAt(nextTab, i + n, hn); //在table的i位置上插入forwardNode节点 表示已经处理过该节点 setTabAt(tab, i, fwd); //设置advance为true 返回到上面的while循环中 就可以执行i--操作 advance = true; &#125; &#125; &#125; &#125; &#125; &#125; Put方法现在来介绍put方法，这个put方法依然沿用HashMap的put方法的思想，根据hash值计算这个新插入的点在table中的位置i，如果i位置是空的，直接放进去，否则进行判断，如果i位置是树节点，按照树的方式插入新的节点，否则把i插入到链表的末尾。ConcurrentHashMap中依然沿用这个思想，有一个最重要的不同点就是ConcurrentHashMap不允许key或value为null值。另外由于涉及到多线程，put方法就要复杂一点。 在多线程中可能有以下两个情况： 如果一个或多个线程正在对ConcurrentHashMap进行扩容操作，当前线程也要进入扩容的操作中。这个扩容的操作之所以能被检测到，是因为transfer方法中在空结点上插入forward节点，如果检测到需要插入的位置被forward节点占有，就帮助进行扩容； 如果检测到要插入的节点是非空且不是forward节点，就对这个节点加锁，这样就保证了线程安全。尽管这个有一些影响效率，但是还是会比hashTable的synchronized要好得多。 整体流程就是首先定义不允许key或value为null的情况放入。 对于每一个放入的值，首先利用spread()方法对key的hashcode进行一次hash计算，由此来确定这个值在table中的位置。 如果这个位置是空的，那么直接放入，而且不需要加锁操作。 如果这个位置存在结点，说明发生了hash碰撞，首先判断这个节点的类型。如果是链表节点（fh&gt;0）,则得到的结点就是hash值相同的节点组成的链表的头节点。需要依次向后遍历确定这个新加入的值所在位置。如果遇到hash值与key值都与新加入节点是一致的情况，则只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点。 如果加入这个节点以后链表长度大于8，就把这个链表转换成红黑树。如果这个节点的类型已经是树节点的话，直接调用树节点的插入方法进行插入新的值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public V put(K key, V value) &#123; return putVal(key, value, false); &#125; /** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //不允许 key或value为null if (key == null || value == null) throw new NullPointerException(); //计算hash值 int hash = spread(key.hashCode()); int binCount = 0; //死循环 何时插入成功 何时跳出 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table为空的话，初始化table if (tab == null || (n = tab.length) == 0) tab = initTable(); //根据hash值计算出在table里面的位置 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //如果这个位置没有值 ，直接放进去，不需要加锁 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //当遇到表连接点时，需要进行整合表的操作 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; //结点上锁 这里的结点可以理解为hash值相同组成的链表的头结点 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; //fh〉0 说明这个节点是一个链表的节点 不是树的节点 if (fh &gt;= 0) &#123; binCount = 1; //在这里遍历链表所有的结点 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果hash值和key值相同 则修改对应结点的value值 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果遍历到了最后一个结点，那么就证明新的节点需要插入 就把它插入在 链表尾部 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //如果这个节点是树节点，就按照树的方式插入值 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //如果链表长度已经达到临界值8 就需要把链表转换为树结构 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //将当前ConcurrentHashMap的元素数量+1 addCount(1L, binCount); return null; &#125; helpTransfer方法这个方法被调用的时候，当前ConcurrentHashMap一定已经有了nextTable对象，首先拿到这个nextTable对象，调用transfer方法。根据上面的transfer方法可以看到，当本线程进入扩容方法的时候会直接进入复制阶段。 下面是该方法的源码： 12345678910111213141516171819202122/** * Helps transfer if a resize is in progress. */ final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length);//计算一个操作校验码 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table; &#125; treeifyBin方法这个方法用于将过长的链表转换为TreeBin对象。但是它并不是直接转换，而是进行一次容量判断，如果容量没有达到转换的要求，直接进行扩容操作并返回；如果满足条件才链表的结构抓换为TreeBin 。这与HashMap不同的是，它并没有把TreeNode直接放入红黑树，而是利用了TreeBin这个小容器来封装所有的TreeNode。 下面是该方法的源码： 12345678910111213141516171819202122232425262728private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY)//如果table.length&lt;64，就扩大一倍，然 后返回 tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //构造了一个TreeBin对象 把所有Node节点包装成TreeNode放进去 for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); //这里只是利用了TreeNode封装 而没有利用TreeNode的next域和parent域 if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; //在原来index的位置 用TreeBin替换掉原来的Node对象 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125; &#125; get方法给定一个key来确定value的时候，必须满足两个条件：key相同且hash值相同。 对于节点可能在链表或树上的情况，需要分别去查找。 下面是该方法源码： 123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //计算hash值 int h = spread(key.hashCode()); //根据hash值确定节点位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //如果搜索到的节点key与传入的key相同且不为null,直接返回这个节点 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //如果eh&lt;0 说明这个节点在树上 直接寻找 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //否则遍历链表 找到对应的值并返回 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; Size相关的方法对于ConcurrentHashMap来说，这个table里到底装了多少东西其实是个不确定的数量，因为不可能在调用size()方法的时候像GC的“stop the world”一样让其他线程都停下来让你去统计，因此只能说这个数量是个估计值。对于这个估计值，ConcurrentHashMap也是大费周章才计算出来的。 辅助定义为了统计元素个数，ConcurrentHashMap定义了一些变量和一个内部类： 1234567891011121314151617181920212223242526/** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */ @sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125; &#125; /******************************************/ /** * 实际上保存的是hashmap中的元素个数 利用CAS锁进行更新 但它并不用返回当前hashmap的元素个数 */ private transient volatile long baseCount; /** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */ private transient volatile int cellsBusy; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; mappingCount与Size方法这两个方法很类似。 从Java工程师给出的注释来看，应该使用mappingCount()代替size()方法。两个方法都没有直接返回basecount，而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 1234567891011121314151617181920212223242526272829303132public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125; /** * Returns the number of mappings. This method should be used * instead of &#123;@link #size&#125; because a ConcurrentHashMap may * contain more mappings than can be represented as an int. The * value returned is an estimate; the actual count may differ if * there are concurrent insertions or removals. * * @return the number of mappings * @since 1.8 */ public long mappingCount() &#123; long n = sumCount(); return (n &lt; 0L) ? 0L : n; // ignore transient negative values &#125; final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value;//所有counter的值求和 &#125; &#125; return sum; &#125; addCount方法在put方法结尾处调用了addCount方法，把当前ConcurrentHashMap的元素个数+1。 这个方法一共做了两件事： 更新baseCount的值 检测是否进行扩容。 123456789101112131415161718192021222324252627282930313233343536373839404142private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; //利用CAS方法更新baseCount的值 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; //如果check值大于等于0 则需要检验是否需要进行扩容操作 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); // if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //如果已经有其他线程在执行扩容操作 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //当前线程是唯一的或是第一个发起扩容的线程 此时nextTable=null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125; &#125; 参考： https://blog.csdn.net/J_Dark/article/details/72853937 https://blog.csdn.net/u010723709/article/details/48007881]]></content>
      <categories>
        <category>java面试准备</category>
        <category>集合框架</category>
      </categories>
      <tags>
        <tag>集合框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap]]></title>
    <url>%2F2019%2F04%2F18%2FLinkedHashMap%2F</url>
    <content type="text"><![CDATA[概述LinkedHashMap 继承自 HashMap，在 HashMap 基础上，通过维护一条双向链表，解决了 HashMap 不能随时保持遍历顺序和插入顺序一致的问题。除此之外，LinkedHashMap 对访问顺序也提供了相关支持。在一些场景下，该特性很有用，比如缓存。在实现上，LinkedHashMap 很多方法直接继承自 HashMap，仅为维护双向链表覆写了部分方法。所以，要看懂 LinkedHashMap 的源码，需要先看懂 HashMap 的源码。 原理LinkedHashMap继承自 HashMap，所以它的底层仍然是基于拉链式散列结构。该结构由数组和链表或红黑树组成，结构示意图大致如下： LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。其结构可能如下图： 上图中，淡蓝色的箭头表示前驱引用，红色箭头表示后继引用。 每当有新键值对节点插入，新节点最终会接在 tail 引用指向的节点后面。而 tail 引用则会移动到新的节点上，这样一个双向链表就建立起来了。 源码分析Entry的继承体系在对核心内容展开分析之前，这里先插队分析一下键值对节点的继承体系。先来看看继承体系结构图： 下面是LinkedHashMap.Entry的源码： 123456789/** * HashMap.Node subclass for normal LinkedHashMap entries. */ static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125; &#125; 上面的继承体系乍一看还是有点复杂的，同时也有点让人迷惑。 HashMap 的内部类 TreeNode 不继承它的了一个内部类 Node，却继承自 Node 的子类 LinkedHashMap 内部类 Entry。这里这样做是有一定原因的，这里先不说。 先来简单说明一下上面的继承体系。LinkedHashMap 内部类 Entry 继承自 HashMap 内部类 Node，并新增了两个引用，分别是 before 和 after。这两个引用的用途不难理解，也就是用于维护双向链表。同时，TreeNode 继承 LinkedHashMap 的内部类 Entry 后，就具备了和其他 Entry 一起组成链表的能力。 但是这里需要大家考虑一个问题。当我们使用 HashMap 时，TreeNode 并不需要具备组成链表能力。如果继承 LinkedHashMap 内部类 Entry ，TreeNode 就多了两个用不到的引用，这样做不是会浪费空间吗？简单说明一下这个问题（水平有限，不保证完全正确），这里这么做确实会浪费空间，但与 TreeNode 通过继承获取的组成链表的能力相比，这点浪费是值得的。在 HashMap 的设计思路注释中，有这样一段话： Because TreeNodes are about twice the size of regular nodes, weuse them only when bins contain enough nodes to warrant use(see TREEIFY_THRESHOLD). And when they become too small (due toremoval or resizing) they are converted back to plain bins. Inusages with well-distributed user hashCodes, tree bins arerarely used. 大致的意思是 TreeNode 对象的大小约是普通 Node 对象的2倍，我们仅在桶（bin）中包含足够多的节点时再使用。当桶中的节点数量变少时（取决于删除和扩容），TreeNode 会被转成 Node。当用户实现的 hashCode 方法具有良好分布性时，树类型的桶将会很少被使用。 通过上面的注释，我们可以了解到。一般情况下，只要 hashCode 的实现不糟糕，Node 组成的链表很少会被转成由 TreeNode 组成的红黑树。也就是说 TreeNode 使用的并不多，浪费那点空间是可接受的。假如 TreeNode 机制继承自 Node 类，那么它要想具备组成链表的能力，就需要 Node 去继承 LinkedHashMap 的内部类 Entry。这个时候就得不偿失了，浪费很多空间去获取不一定用得到的能力。 链表的建立能力链表的建立过程是在插入键值对节点时开始的。 初始情况下，让 LinkedHashMap 的 head 和 tail 引用同时指向新节点，链表就算建立起来了。随后不断有新节点插入，通过将新节点接在 tail 引用指向节点的后面，即可实现链表的更新。 Map 类型的集合类是通过 put(K,V) 方法插入键值对，LinkedHashMap 本身并没有覆写父类的 put 方法，而是直接使用了父类的实现。但在 HashMap 中，put 方法插入的是 HashMap 内部类 Node 类型的节点，该类型的节点并不具备与 LinkedHashMap 内部类 Entry 及其子类型节点组成链表的能力。那么，LinkedHashMap 是怎样建立链表的呢？在展开说明之前，我们先看一下 LinkedHashMap 插入操作相关的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// HashMap 中实现public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;// HashMap 中实现final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) &#123;...&#125; // 通过节点 hash 定位节点所在的桶位置，并检测桶中是否包含节点引用 if ((p = tab[i = (n - 1) &amp; hash]) == null) &#123;...&#125; else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) &#123;...&#125; else &#123; // 遍历链表，并统计链表长度 for (int binCount = 0; ; ++binCount) &#123; // 未在单链表中找到要插入的节点，将新节点接在单链表的后面 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) &#123;...&#125; break; &#125; // 插入的节点已经存在于单链表中 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) &#123;...&#125; afterNodeAccess(e); // 回调方法，后续说明 return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) &#123;...&#125; afterNodeInsertion(evict); // 回调方法，后续说明 return null;&#125;// HashMap 中实现Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next);&#125;// LinkedHashMap 中覆写Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); // 将 Entry 接在双向链表的尾部 linkNodeLast(p); return p;&#125;// LinkedHashMap 中实现private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; // last 为 null，表明链表还未建立 if (last == null) head = p; else &#123; // 将新节点 p 接在链表尾部 p.before = last; last.after = p; &#125;&#125; 上面省略了部分非关键的代码，根据上面的代码，可以知道 LinkedHashMap 插入操作的调用过程。如下： 我把 newNode(int hash, K key, V value, Node&lt;K,V&gt; e) 方法红色背景标注了出来，这一步比较关键。LinkedHashMap 覆写了该方法。在这个方法中，LinkedHashMap 创建了 Entry，并通过 linkNodeLast 方法将 Entry 接在双向链表的尾部，实现了双向链表的建立。 双向链表建立之后，我们就可以按照插入顺序去遍历 LinkedHashMap，大家可以自己写点测试代码验证一下插入顺序。 以上就是 LinkedHashMap 维护插入顺序的相关分析。本节的最后，再额外补充一些东西。大家如果仔细看上面的代码的话，会发现有两个以after开头方法，在上文中没有被提及。在 JDK 1.8 HashMap 的源码中，相关的方法有3个： 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; 根据这三个方法的注释可以看出，这些方法的用途是在增删查等操作后，通过回调的方式，让 LinkedHashMap 有机会做一些后置操作。上述三个方法的具体实现在 LinkedHashMap 中，本节先不分析这些实现，相关分析会在下面进行。 链表节点的删除过程与插入操作一样，LinkedHashMap 删除操作相关的代码也是直接用父类的实现。 在删除节点时，父类的删除逻辑并不会修复 LinkedHashMap 所维护的双向链表，这不是它的职责。那么删除及节点后，被删除的节点该如何从双链表中移除呢？ 在删除及节点后，回调方法 afterNodeRemoval 会被调用。LinkedHashMap 覆写该方法，并在该方法中完成了移除被删除节点的操作。相关源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// HashMap 中实现public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;// HashMap 中实现final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) &#123;...&#125; else &#123; // 遍历单链表，寻找要删除的节点，并赋值给 node 变量 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) &#123;...&#125; // 将要删除的节点从单链表中移除 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); // 调用删除回调方法进行后续操作 return node; &#125; &#125; return null;&#125;// LinkedHashMap 中覆写void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 将 p 节点的前驱后后继引用置空 p.before = p.after = null; // b 为 null，表明 p 是头节点 if (b == null) head = a; else b.after = a; // a 为 null，表明 p 是尾节点 if (a == null) tail = b; else a.before = b;&#125; 删除的过程并不复杂，上面这么多代码其实就做了三件事： 根据 hash 定位到桶位置 遍历链表或调用红黑树相关的删除方法 从 LinkedHashMap 维护的双链表中移除要删除的节点 举个例子说明一下，假如我们要删除下图键值为 3 的节点： 根据 hash 定位到该节点属于3号桶，然后在对3号桶保存的单链表进行遍历。 找到要删除的节点后，先从单链表中移除该节点，如下图所示： 然后再双向链表中移除该节点： 访问顺序的维护过程默认情况下，LinkedHashMap 是按插入顺序维护链表。 不过我们可以在初始化 LinkedHashMap，指定 accessOrder 参数为 true，即可让它按访问顺序维护链表，相关的构造方法源码如下： 1234567891011121314151617/** * Constructs an empty &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with the * specified initial capacity, load factor and ordering mode. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @param accessOrder the ordering mode - &lt;tt&gt;true&lt;/tt&gt; for * access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder; &#125; 访问顺序的原理上并不复杂，当我们调用get/getOrDefault/replace等方法时，只需要将这些方法访问的节点移动到链表的尾部即可。相应的源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// LinkedHashMap 中覆写public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; // 如果 accessOrder 为 true，则调用 afterNodeAccess 将被访问节点移动到链表最后 if (accessOrder) afterNodeAccess(e); return e.value;&#125;// LinkedHashMap 中覆写void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; // 如果 b 为 null，表明 p 为头节点 if (b == null) head = a; else b.after = a; if (a != null) a.before = b; /* * 这里存疑，父条件分支已经确保节点 e 不会是尾节点， * 那么 e.after 必然不会为 null，不知道 else 分支有什么作用 */ else last = b; if (last == null) head = p; else &#123; // 将 p 接在链表的最后 p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; 下面举例演示一下，帮助大家理解。 假设我们访问下图键值为3的节点，访问前结构为： 访问后，键值为3的节点将会被移动到双向链表的最后位置，其前驱和后继也会跟着更新。访问后的结构如下： 基于 LinkedHashMap 实现缓存前面介绍了 LinkedHashMap 是如何维护插入和访问顺序的，大家对 LinkedHashMap 的原理应该有了一定的认识。 本节我们来写一些代码实践一下，这里通过继承 LinkedHashMap 实现了一个简单的 LRU 策略的缓存。 在写代码之前，先介绍一下前置知识。 在前面分析链表建立过程时，我故意忽略了部分源码分析，本节就把忽略的部分补上，先看源码： 12345678910111213void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 根据条件判断是否移除最近最少被访问的节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;// 移除最近最少被访问条件之一，通过覆盖此方法可实现不同策略的缓存protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; 上面的源码的核心逻辑在一般情况下都不会被执行，所以之前并没有进行分析。 上面的代码做的事情比较简单，就是通过一些条件，判断是否移除最近最少被访问的节点。看到这里，大家应该知道上面两个方法的用途了。 当我们基于 LinkedHashMap 实现缓存时，通过覆写removeEldestEntry方法可以实现自定义策略的 LRU 缓存。比如我们可以根据节点数量判断是否移除最近最少被访问的节点，或者根据节点的存活时间判断是否移除该节点等。 本节所实现的缓存是基于判断节点数量是否超限的策略。在构造缓存对象时，传入最大节点数。当插入的节点数超过最大节点数时，移除最近最少被访问的节点。 实现代码如下：12345678910111213141516171819202122232425262728293031323334353637public class SimpleCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private static final int MAX_NODE_NUM = 100; private int limit; public SimpleCache() &#123; this(MAX_NODE_NUM); &#125; public SimpleCache(int limit) &#123; super(limit, 0.75f, true); this.limit = limit; &#125; public V save(K key, V val) &#123; return put(key, val); &#125; public V getOne(K key) &#123; return get(key); &#125; public boolean exists(K key) &#123; return containsKey(key); &#125; /** * 判断节点数是否超限 * @param eldest * @return 超限返回 true，否则返回 false */ @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; return size() &gt; limit; &#125;&#125; 测试代码如下：12345678910111213141516171819202122public class SimpleCacheTest &#123; @Test public void test() throws Exception &#123; SimpleCache&lt;Integer, Integer&gt; cache = new SimpleCache&lt;&gt;(3); for (int i = 0; i &lt; 10; i++) &#123; cache.save(i, i * i); &#125; System.out.println("插入10个键值对后，缓存内容："); System.out.println(cache + "\n"); System.out.println("访问键值为7的节点后，缓存内容："); cache.getOne(7); System.out.println(cache + "\n"); System.out.println("插入键值为1的键值对后，缓存内容："); cache.save(1, 1); System.out.println(cache); &#125;&#125; 测试结果如下： 在测试代码中，设定缓存大小为3。在向缓存中插入10个键值对后，只有最后3个被保存下来了，其他的都被移除了。然后通过访问键值为7的节点，使得该节点被移到双向链表的最后位置。当我们再次插入一个键值对时，键值为7的节点就不会被移除。 总结本文从 LinkedHashMap 维护双向链表的角度对 LinkedHashMap 的源码进行了分析，并在文章的结尾基于 LinkedHashMap 实现了一个简单的 Cache。在日常开发中，LinkedHashMap 的使用频率虽不及 HashMap，但它也个重要的实现。在 Java 集合框架中，HashMap、LinkedHashMap 和 TreeMap 三个映射类基于不同的数据结构，并实现了不同的功能。HashMap 底层基于拉链式的散列结构，并在 JDK 1.8 中引入红黑树优化过长链表的问题。基于这样结构，HashMap 可提供高效的增删改查操作。LinkedHashMap 在其之上，通过维护一条双向链表，实现了散列数据结构的有序遍历。TreeMap 底层基于红黑树实现，利用红黑树的性质，实现了键值对排序功能。]]></content>
      <categories>
        <category>java面试准备</category>
        <category>集合框架</category>
      </categories>
      <tags>
        <tag>集合框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2019%2F04%2F16%2FHashMap%2F</url>
    <content type="text"><![CDATA[概述HashMap 主要用来存放键值对，它基于哈希表的Map接口实现，是常用的Java集合之一。 JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。 JDK1.8 以后 HashMap 由 数组+链表+红黑树 组成。JDK1.8以后HashMap在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）且总容量大于64时，将链表转化为红黑树，以减少搜索时间。 HashMap是非线程安全类，在多线程环境下可能会存在问题。 原理HashMap 底层是基于散列算法实现，散列算法分为散列再探测和拉链式。HashMap 则使用了拉链式的散列算法，并在 JDK 1.8 中引入了红黑树优化过长的链表。数据结构示意图如下： 对于拉链式的散列算法，其数据结构是由数组和链表（或树形结构）组成。在进行增删查等操作时，首先要定位到元素的所在桶的位置，之后再从链表中定位该元素。 比如我们要查询上图结构中是否包含元素35，步骤如下： 定位元素35所处桶的位置，index = 35 % 16 = 3 在3号桶所指向的链表中继续查找，发现35在链表中。 上面就是 HashMap 底层数据结构的原理，HashMap 基本操作就是对拉链式散列算法基本操作的一层包装。不同的地方在于 JDK 1.8 中引入了红黑树，底层数据结构由数组+链表变为了数组+链表+红黑树，不过本质并未变。 底层数据结构分析JDK1.8之前 JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码： 1234567static final int hash(Object key) &#123; int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 对比一下 JDK1.7的 HashMap 的 hash 方法源码： 12345678static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 JDK1.8之后相比于之前的版本，JDK1.8在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）且总容量大于64时，将链表转化为红黑树，以减少搜索时间。 源码分析HashMap中的成员变量：12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍 transient Node&lt;k,v&gt;[] table; // 存放具体元素的集 transient Set&lt;map.entry&lt;k,v&gt;&gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor;&#125; 名称 用途 initialCapacity HashMap 初始容量 loadFactor 负载因子 threshold 当前 HashMap 所能容纳键值对数量的最大值，超过这个值，则需扩容 相关代码如下： 12345678910/** The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;/** The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f;final float loadFactor;/** The next size value at which to resize (capacity * load factor). */int threshold; 如果大家去看源码，会发现 HashMap 中没有定义 initialCapacity 这个变量。这个也并不难理解，从参数名上可看出，这个变量表示一个初始容量，只是构造方法中用一次，没必要定义一个变量保存。但如果大家仔细看上面 HashMap 的构造方法，会发现存储键值对的数据结构并不是在构造方法里初始化的。这就有个疑问了，既然叫初始容量，但最终并没有用与初始化数据结构，那传这个参数还有什么用呢？ Node节点类源码: 123456789101112131415161718192021222324252627282930313233343536373839// 继承自 Map.Entry&lt;K,V&gt;static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较 final K key;//键 V value;//值 // 指向下一个节点 Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; // 重写hashCode()方法 public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 重写 equals() 方法 public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 树节点类源码: 12345678910111213141516static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // 父 TreeNode&lt;K,V&gt; left; // 左 TreeNode&lt;K,V&gt; right; // 右 TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; // 判断颜色 TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; // 返回根节点 final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p;&#125; 构造方法 HashMap 的构造方法不多，只有四个。 HashMap 构造方法做的事情比较简单，一般都是初始化一些重要变量。比如 loadFactor 和 threshold，而底层的数据结构则是延迟到插入键值对时再进行初始化。 HashMap 相关构造方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** 构造方法 1 */public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;/** 构造方法 2 */public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;/** 构造方法 3 */public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125;/** 构造方法 4 */public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125;final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; // 判断table是否已经初始化 if (table == null) &#123; // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s &gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; 上面4个构造方法中，大家平时用的最多的应该是第一个了。 构造方法1很简单，仅将 loadFactor 变量设为默认值。 构造方法2调用了构造方法3，而构造方法3仍然只是设置了一些变量。 构造方法4则是将另一个 Map 中的映射拷贝一份到自己的存储结构中来，这个方法不是很常用。 初始阈值的计算过程默认情况下，HashMap 初始容量是16，负载因子为 0.75。这里并没有默认阈值，原因是阈值可由容量乘上负载因子计算而来（注释中有说明），即threshold = capacity * loadFactor。 但当你仔细看构造方法3时，会发现阈值并不是由上面公式计算而来，而是通过一个方法算出来的。这是不是可以说明 threshold 变量的注释有误呢？还是仅这里进行了特殊处理，其他地方遵循计算公式呢？关于这个疑问，这里也先不说明，后面在分析扩容方法时，再来解释这个问题。接下来，我们来看看初始化 threshold 的方法长什么样的的，源码如下： 123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 总结起来就一句话：找到大于或等于 cap 的最小2的幂。 下面我们来看看 tableSizeFor 方法的图解： 上面是 tableSizeFor 方法的计算过程图，这里cap = 536,870,913 = 2&lt;sup&gt;29&lt;/sup&gt; + 1，多次计算后，算出n + 1 = 1,073,741,824 = 2&lt;sup&gt;30&lt;/sup&gt;。通过图解应该可以比较容易理解这个方法的用途，这里就不多说了。 负载因子对于 HashMap 来说，负载因子是一个很重要的参数，该参数反应了 HashMap 桶数组的使用情况（假设键值对节点均匀分布在桶数组中）。 通过调节负载因子，可使 HashMap 时间和空间复杂度上有不同的表现。当我们调低负载因子时，HashMap 所能容纳的键值对数量变少。扩容时，重新将键值对存储新的桶数组里，键的键之间产生的碰撞会下降，链表长度变短。此时，HashMap 的增删改查等操作的效率将会变高，这里是典型的拿空间换时间。相反，如果增加负载因子（负载因子可以大于1），HashMap 所能容纳的键值对数量变多，空间利用率高，但碰撞率也高。这意味着链表长度变长，效率也随之降低，这种情况是拿时间换空间。 至于负载因子怎么调节，这个看使用场景了。一般情况下，我们用默认值就可以了。 查找HashMap 的查找操作比较简单，查找步骤与原理篇介绍一致。 即先定位键值对所在的桶的位置，然后再对链表或红黑树进行查找。通过这两步即可完成查找，该操作相关代码如下： 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 1. 定位键值对所在桶的位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; // 2. 如果 first 是 TreeNode 类型，则调用黑红树查找方法 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 2. 对链表进行查找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 查找的核心逻辑是封装在 getNode() 方法中的，getNode() 方法源码我已经写了一些注释，应该不难看懂。 我们先来看看查找过程的第一步 - 确定桶位置，其实现代码如下： 12// index = (n - 1) &amp; hashfirst = tab[(n - 1) &amp; hash] 这里通过(n - 1)&amp; hash即可算出桶的在桶数组中的位置，可能有的朋友不太明白这里为什么这么做，这里简单解释一下。 HashMap 中桶数组的大小 length 总是2的幂，此时，(n - 1) &amp; hash 等价于对 length 取余。但取余的计算效率没有位运算高，所以(n - 1) &amp; hash也是一个小的优化。 举个例子说明一下吧，假设 hash = 185，n = 16。计算过程示意图如下： 在上面源码中，除了查找相关逻辑，还有一个计算 hash 的方法。这个方法源码如下： 1234567/** * 计算键的 hash 值 */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 看这个方法的逻辑好像是通过位运算重新计算 hash，那么这里为什么要这样做呢？ 为什么不直接用键的 hashCode 方法产生的 hash 呢？ 我们再看一下上面求余(上面的例子是8位二进制)的计算图，图中的 hash 是由键的 hashCode 产生。计算余数时，由于 n 比较小，hash 只有低4位参与了计算，高位的计算可以认为是无效的。这样导致了计算结果只与低位信息有关，高位数据没发挥作用。 为了处理这个缺陷，我们可以上图中的 hash 高4位数据与低4位数据进行异或运算，即 hash ^ (hash &gt;&gt;&gt; 4)。 通过这种方式，让高位数据与低位数据进行异或，以此加大低位信息的随机性，变相的让高位数据参与到计算中。此时的计算过程如下： 在 Java 中，hashCode 方法产生的 hash 是 int 类型，32 位宽。前16位为高位，后16位为低位，所以要右移16位。 上面所说的是重新计算 hash 的一个好处。 除此之外，重新计算 hash 的另一个好处是可以增加 hash 的复杂度。当我们覆写 hashCode 方法时，可能会写出分布性不佳的 hashCode 方法，进而导致 hash 的冲突率比较高。通过移位和异或运算，可以让 hash 变得更复杂，进而影响 hash 的分布性。 遍历和查找一样，遍历操作也是大家使用频率比较高的一个操作。对于 遍历 HashMap，我们一般都会用下面的方式： 123456789for(Object key : map.keySet()) &#123; // do something&#125;或for(HashMap.Entry entry : map.entrySet()) &#123; // do something&#125; 从上面代码片段中可以看出，大家一般都是对 HashMap 的 key 集合或 Entry 集合进行遍历。上面代码片段中用 foreach 遍历 keySet 方法产生的集合，在编译时会转换成用迭代器遍历，等价于： 123456Set keys = map.keySet();Iterator ite = keys.iterator();while (ite.hasNext()) &#123; Object key = ite.next(); // do something&#125; 大家在遍历 HashMap 的过程中会发现，多次对 HashMap 进行遍历时，遍历结果顺序都是一致的。但这个顺序和插入的顺序一般都是不一致的。 产生上述行为的原因是怎样的呢？大家想一下原因。我先把遍历相关的代码贴出来，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; if (ks == null) &#123; ks = new KeySet(); keySet = ks; &#125; return ks;&#125;/** * 键集合 */final class KeySet extends AbstractSet&lt;K&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; public final Iterator&lt;K&gt; iterator() &#123; return new KeyIterator(); &#125; public final boolean contains(Object o) &#123; return containsKey(o); &#125; public final boolean remove(Object key) &#123; return removeNode(hash(key), key, null, false, true) != null; &#125; // 省略部分代码&#125;/** * 键迭代器 */final class KeyIterator extends HashIterator implements Iterator&lt;K&gt; &#123; public final K next() &#123; return nextNode().key; &#125;&#125;abstract class HashIterator &#123; Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() &#123; expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry // 寻找第一个包含链表节点引用的桶 do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; // 寻找下一个包含链表节点引用的桶 do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125; //省略部分代码&#125; 如上面的源码，遍历所有的键时，首先要获取键集合KeySet对象，然后再通过 KeySet 的迭代器KeyIterator进行遍历。KeyIterator 类继承自HashIterator类，核心逻辑也封装在 HashIterator 类中。 HashIterator 的逻辑并不复杂。在初始化时，HashIterator 先从桶数组中找到包含链表节点引用的桶，然后对这个桶指向的链表进行遍历。遍历完成后，再继续寻找下一个包含链表节点引用的桶，找到继续遍历。找不到，则结束遍历。 举个例子，假设我们遍历下图的结构： HashIterator 在初始化时，会先遍历桶数组，找到包含链表节点引用的桶，对应图中就是3号桶。 随后由 nextNode 方法遍历该桶所指向的链表。遍历完3号桶后，nextNode 方法继续寻找下一个不为空的桶，对应图中的7号桶。之后流程和上面类似，直至遍历完最后一个桶。 以上就是 HashIterator 的核心逻辑的流程，对应下图： 遍历上图的最终结果是 19 -&gt; 3 -&gt; 35 -&gt; 7 -&gt; 11 -&gt; 43 -&gt; 59。 为了验证正确性，简单写点测试代码跑一下看看。测试代码如下： 12345678910111213141516171819202122/** * 应在 JDK 1.8 下测试，其他环境下不保证结果和上面一致 */public class HashMapTest &#123; @Test public void testTraversal() &#123; HashMap&lt;Integer, String&gt; map = new HashMap(16); map.put(7, ""); map.put(11, ""); map.put(43, ""); map.put(59, ""); map.put(19, ""); map.put(3, ""); map.put(35, ""); System.out.println("遍历结果："); for (Integer key : map.keySet()) &#123; System.out.print(key + " -&gt; "); &#125; &#125;&#125; 遍历结果如下： 插入 插入逻辑分析首先肯定是先定位要插入的键值对属于哪个桶，定位到桶后，再判断桶是否为空。如果为空，则将键值对存入即可。如果不为空，则需将键值对接在链表最后一个位置，或者更新键值对。 这就是 HashMap 的插入流程，是不是觉得很简单。当然，大家先别高兴。这只是一个简化版的插入流程，真正的插入流程要复杂不少。首先 HashMap 是变长集合，所以需要考虑扩容的问题。其次，在 JDK 1.8 中，HashMap 引入了红黑树优化过长链表，这里还要考虑多长的链表需要进行优化，优化过程又是怎样的问题。引入这里两个问题后，大家会发现原本简单的操作，现在略显复杂了。 在本节中，我将先分析插入操作的源码，扩容、树化（链表转为红黑树，下同）以及其他和树结构相关的操作，随后将在独立的两小结中进行分析。 接下来，先来看一下插入操作的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table未初始化或者长度为0，进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) &amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素 else &#123; Node&lt;K,V&gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // hash值不相等，即key不相等；为红黑树结点 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 为链表结点 else &#123; // 在链表最末插入结点 for (int binCount = 0; ; ++binCount) &#123; // 到达链表的尾部 if ((e = p.next) == null) &#123; // 在尾部插入新结点 p.next = newNode(hash, key, value, null); // 结点数量达到阈值，转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; &#125; // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; &#125; &#125; // 表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) &#123; // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 结构性修改 ++modCount; // 实际大小大于阈值则扩容 if (++size &gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null;&#125; 插入操作的入口方法是 put(K,V)，但核心逻辑在V putVal(int, K, V, boolean, boolean) 方法中。 V putVal(int, K, V, boolean, boolean) 方法主要做了这么几件事情： 当桶数组 table 为空时，通过扩容的方式初始化 table 查找要插入的键值对是否已经存在，存在的话根据条件判断是否用新值替换旧值 如果不存在，则将键值对链入链表中，并根据链表长度决定是否将链表转为红黑树 判断键值对数量是否大于阈值，大于的话则进行扩容操作 扩容机制在 Java 中，数组的长度是固定的，这意味着数组只能存储固定量的数据。但在开发的过程中，很多时候我们无法知道该建多大的数组合适。建小了不够用，建大了用不完，造成浪费。 如果我们能实现一种变长的数组，并按需分配空间就好了。好在，我们不用自己实现变长数组，Java 集合框架已经实现了变长的数据结构，比如 ArrayList 和 HashMap。对于这类基于数组的变长数据结构，扩容是一个非常重要的操作。下面就来聊聊 HashMap 的扩容机制。 在详细分析之前，先来说一下扩容相关的背景知识： 在 HashMap 中，桶数组的长度均是2的幂，阈值大小为桶数组长度与负载因子的乘积。当 HashMap 中的键值对数量超过阈值时，进行扩容。 HashMap 的扩容机制与其他变长集合的套路不太一样，HashMap 按当前桶数组长度的2倍进行扩容，阈值也变为原来的2倍（如果计算过程中，阈值溢出归零，则按阈值公式重新计算）。扩容之后，要重新计算键值对的位置，并把它们移动到合适的位置上去。 以上就是 HashMap 的扩容大致过程，接下来我们来看看具体的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 如果 table 不为空，表明已经初始化过了 if (oldCap &gt; 0) &#123; // 当 table 容量超过容量最大值，则不再扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 按旧容量和阈值的2倍计算新容量和阈值的大小 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold /* * 初始化时，将 threshold 的值赋值给 newCap， * HashMap 使用 threshold 变量暂时保存 initialCapacity 参数的值 */ newCap = oldThr; else &#123; // zero initial threshold signifies using defaults /* * 调用无参构造方法时，桶数组容量为默认容量， * 阈值为默认容量与默认负载因子乘积 */ newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // newThr 为 0 时，按阈值计算公式进行计算 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; // 创建新的桶数组，桶数组的初始化也是在这里完成的 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 如果旧的桶数组不为空，则遍历桶数组，并将键值对映射到新的桶数组中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 重新映射时，需要对红黑树进行拆分 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; // 遍历链表，并将链表节点按原顺序进行分组 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将分组后的链表映射到新桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; resize()方法总共做了3件事，分别是： 计算新桶数组的容量 newCap 和新阈值 newThr 根据计算出的 newCap 创建新的桶数组，桶数组 table 也是在这里进行初始化的 将键值对节点重新映射到新的桶数组里。如果节点是 TreeNode 类型，则需要拆分红黑树。如果是普通节点，则节点按原顺序进行分组。 上面列的三点中，第二点创建新的桶数组就一行代码，不用说了。接下来，来说说第一点和第三点。 先说说 newCap 和 newThr 计算过程。该计算过程对应 resize 源码的第一和第二个条件分支，如下： 123456789101112// 第一个条件分支if ( oldCap &gt; 0) &#123; // 嵌套条件分支 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;...&#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) &#123;...&#125;&#125; else if (oldThr &gt; 0) &#123;...&#125;else &#123;...&#125;// 第二个条件分支if (newThr == 0) &#123;...&#125; 通过这两个条件分支对不同情况进行判断，进而算出不同的容量值和阈值。它们所覆盖的情况如下： 分支一： 条件 覆盖情况 备注 oldCap &gt; 0 桶数组 table 已经被初始化 oldThr &gt; 0 threshold &gt; 0，且桶数组未被初始化 调用 HashMap(int) 和 HashMap(int, float) 构造方法时会产生这种情况，此种情况下 newCap = oldThr，newThr 在第二个条件分支中算出 oldCap == 0 &amp;&amp; oldThr == 0 桶数组未被初始化，且 threshold 为 0 调用 HashMap() 构造方法会产生这种情况。 注意：这里把oldThr &gt; 0情况单独拿出来说一下。在这种情况下，会将 oldThr 赋值给 newCap，等价于newCap = threshold = tableSizeFor(initialCapacity)。我们在初始化时传入的 initialCapacity 参数经过 threshold 中转最终赋值给了 newCap。这也就解答了前面提的一个疑问：initialCapacity 参数没有被保存下来，那么它怎么参与桶数组的初始化过程的呢？ 分支一中的嵌套分支： 条件 覆盖情况 备注 oldCap &gt;= 230 桶数组容量大于或等于最大桶容量 230 这种情况下不再扩容 newCap &lt; 230 &amp;&amp; oldCap &gt; 16 新桶数组容量小于最大值，且旧桶数组容量大于 16 该种情况下新阈值 newThr = oldThr &lt;&lt; 1，移位可能会导致溢出 这里简单说明一下移位导致的溢出情况，当 loadFactor小数位为 0，整数位可被2整除且大于等于8时，在某次计算中就可能会导致 newThr 溢出归零。见下图： 分支二： 条件 覆盖情况 备注 newThr == 0 第一个条件分支未计算 newThr 或嵌套分支在计算过程中导致 newThr 溢出归零 说完 newCap 和 newThr 的计算过程，接下来再来分析一下键值对节点重新映射的过程。 在 JDK 1.8 中，重新映射节点需要考虑节点类型。对于树形节点，需先拆分红黑树再映射。对于链表类型节点，则需先对链表进行分组，然后再映射。需要的注意的是，分组后，组内节点相对位置保持不变。关于红黑树拆分的逻辑将会放在下一小节说明，先来看看链表是怎样进行分组映射的。 我们都知道往底层数据结构中插入节点时，一般都是先通过模运算计算桶位置，接着把节点放入桶中即可。事实上，我们可以把重新映射看做插入操作。在 JDK 1.7 中，也确实是这样做的。但在 JDK 1.8 中，则对这个过程进行了一定的优化，逻辑上要稍微复杂一些。在详细分析前，我们先来回顾一下 hash 求余的过程： 上图中，桶数组大小 n = 16，hash1 与 hash2 不相等。但因为只有后4位参与求余，所以结果相等。当桶数组扩容后，n 由16变成了32，对上面的 hash 值重新进行映射： 扩容后，参与模运算的位数由4位变为了5位。由于两个 hash 第5位的值是不一样，所以两个 hash 算出的结果也不一样。上面的计算过程并不难理解，继续往下分析。 假设我们上图的桶数组进行扩容，扩容后容量 n = 16，重新映射过程如下: 依次遍历链表，并计算节点 hash &amp; oldCap 的值，如下图所示： 如果值为0，将 loHead 和 loTail 指向这个节点。如果后面还有节点 hash &amp; oldCap 为0的话，则将节点链入 loHead 指向的链表中，并将 loTail 指向该节点。如果值为非0的话，则让 hiHead 和 hiTail 指向该节点。完成遍历后，可能会得到两条链表，此时就完成了链表分组： 最后再将这两条链接存放到相应的桶中，完成扩容。如下图： 从上图可以发现，重新映射后，两条链表中的节点顺序并未发生变化，还是保持了扩容前的顺序。以上就是 JDK 1.8 中 HashMap 扩容的代码讲解。另外再补充一下，JDK 1.8 版本下 HashMap 扩容效率要高于之前版本。如果大家看过 JDK 1.7 的源码会发现，JDK 1.7 为了防止因 hash 碰撞引发的拒绝服务攻击，在计算 hash 过程中引入随机种子。以增强 hash 的随机性，使得键值对均匀分布在桶数组中。在扩容过程中，相关方法会根据容量判断是否需要生成新的随机种子，并重新计算所有节点的 hash。而在 JDK 1.8 中，则通过引入红黑树替代了该种方式。从而避免了多次计算 hash 的操作，提高了扩容效率。 链表树化、红黑树链化与拆分JDK 1.8 对 HashMap 实现进行了改进。最大的改进莫过于在引入了红黑树处理频繁的碰撞，代码复杂度也随之上升。比如，以前只需实现一套针对链表操作的方法即可。而引入红黑树后，需要另外实现红黑树相关的操作。红黑树是一种自平衡的二叉查找树，本身就比较复杂。 在展开说明之前，先把树化的相关代码贴出来，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static final int TREEIFY_THRESHOLD = 8;/** * 当桶数组容量小于该值时，优先进行扩容，而不是树化 */static final int MIN_TREEIFY_CAPACITY = 64;static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;&#125;/** * 将普通节点链表转换成树形节点链表 */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; // 桶数组容量小于 MIN_TREEIFY_CAPACITY，优先进行扩容而不是树化 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; // hd 为头节点（head），tl 为尾节点（tail） TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; // 将普通节点替换成树形节点 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); // 将普通链表转成由树形节点链表 if ((tab[index] = hd) != null) // 将树形链表转换成红黑树 hd.treeify(tab); &#125;&#125;TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next);&#125; 在扩容过程中，树化要满足两个条件： 链表长度大于等于 TREEIFY_THRESHOLD 桶数组容量大于等于 MIN_TREEIFY_CAPACITY 第一个条件比较好理解，这里就不说了。这里来说说加入第二个条件的原因，个人觉得原因如下： 当桶数组容量比较小时，键值对节点 hash 的碰撞率可能会比较高，进而导致链表长度较长。这个时候应该优先扩容，而不是立马树化。毕竟高碰撞率是因为桶数组容量较小引起的，这个是主因。容量小时，优先扩容可以避免一些列的不必要的树化过程。同时，桶容量较小时，扩容会比较频繁，扩容时需要拆分红黑树并重新映射。所以在桶容量比较小的情况下，将长链表转成红黑树是一件吃力不讨好的事。 回到上面的源码中，我们继续看一下 treeifyBin 方法。该方法主要的作用是将普通链表转成为由 TreeNode 型节点组成的链表，并在最后调用 treeify 是将该链表转为红黑树。TreeNode 继承自 Node 类，所以 TreeNode 仍然包含 next 引用，原链表的节点顺序最终通过 next 引用被保存下来。我们假设树化前，链表结构如下： HashMap 在设计之初，并没有考虑到以后会引入红黑树进行优化。所以并没有像 TreeMap 那样，要求键类实现 comparable 接口或提供相应的比较器。但由于树化过程需要比较两个键对象的大小，在键类没有实现 comparable 接口的情况下，怎么比较键与键之间的大小了就成了一个棘手的问题。 为了解决这个问题，HashMap 是做了三步处理，确保可以比较出两个键的大小，如下： 比较键与键之间 hash 的大小，如果 hash 相同，继续往下比较 检测键类是否实现了 Comparable 接口，如果实现调用 compareTo 方法进行比较 如果仍未比较出大小，就需要进行仲裁了，仲裁方法为 tieBreakOrder（大家自己看源码吧） tie break 是网球术语，可以理解为加时赛的意思，起这个名字还是挺有意思的。 通过上面三次比较，最终就可以比较出孰大孰小。比较出大小后就可以构造红黑树了，最终构造出的红黑树如下： 橙色的箭头表示 TreeNode 的 next 引用。由于空间有限，prev 引用未画出。可以看出，链表转成红黑树后，原链表的顺序仍然会被引用仍被保留了（红黑树的根节点会被移动到链表的第一位），我们仍然可以按遍历链表的方式去遍历上面的红黑树。这样的结构为后面红黑树的切分以及红黑树转成链表做好了铺垫，我们继续往下分析。 红黑树拆分扩容后，普通节点需要重新映射，红黑树节点也不例外。按照一般的思路，我们可以先把红黑树转成链表，之后再重新映射链表即可。这种处理方式是大家比较容易想到的，但这样做会损失一定的效率。不同于上面的处理方式，HashMap 实现的思路则是上好佳（上好佳请把广告费打给我）。如上节所说，在将普通链表转成红黑树时，HashMap 通过两个额外的引用 next 和 prev 保留了原链表的节点顺序。这样再对红黑树进行重新映射时，完全可以按照映射链表的方式进行。这样就避免了将红黑树转成链表后再进行映射，无形中提高了效率。 以上就是红黑树拆分的逻辑，下面看一下具体实现吧： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 红黑树转链表阈值static final int UNTREEIFY_THRESHOLD = 6;final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; TreeNode&lt;K,V&gt; b = this; // Relink into lo and hi lists, preserving order TreeNode&lt;K,V&gt; loHead = null, loTail = null; TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; /* * 红黑树节点仍然保留了 next 引用，故仍可以按链表方式遍历红黑树。 * 下面的循环是对红黑树节点进行分组，与上面类似 */ for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) &#123; next = (TreeNode&lt;K,V&gt;)e.next; e.next = null; if ((e.hash &amp; bit) == 0) &#123; if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; &#125; else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; if (loHead != null) &#123; // 如果 loHead 不为空，且链表长度小于等于 6，则将红黑树转成链表 if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; tab[index] = loHead; /* * hiHead == null 时，表明扩容后， * 所有节点仍在原位置，树结构不变，无需重新树化 */ if (hiHead != null) loHead.treeify(tab); &#125; &#125; // 与上面类似 if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125; &#125;&#125; 从源码上可以看得出，重新映射红黑树的逻辑和重新映射链表的逻辑基本一致。不同的地方在于，重新映射后，会将红黑树拆分成两条由 TreeNode 组成的链表。如果链表长度小于 UNTREEIFY_THRESHOLD，则将链表转换成普通链表。否则根据条件重新将 TreeNode 链表树化。 举个例子说明一下，假设扩容后，重新映射上图的红黑树，映射结果如下： 红黑树链化前面说过，红黑树中仍然保留了原链表节点顺序。有了这个前提，再将红黑树转成链表就简单多了，仅需将 TreeNode 链表转成 Node 类型的链表即可。相关代码如下： 123456789101112131415161718final Node&lt;K,V&gt; untreeify(HashMap&lt;K,V&gt; map) &#123; Node&lt;K,V&gt; hd = null, tl = null; // 遍历 TreeNode 链表，并用 Node 替换 for (Node&lt;K,V&gt; q = this; q != null; q = q.next) &#123; // 替换节点类型 Node&lt;K,V&gt; p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd;&#125;Node&lt;K,V&gt; replacementNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(p.hash, p.key, p.value, next);&#125; 删除HashMap 的删除操作并不复杂，仅需三个步骤即可完成。 第一步是定位桶位置，第二步遍历链表并找到键值相等的节点，第三步删除节点。 相关源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; // 1. 定位桶位置 (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; // 如果键的值与链表第一个节点相等，则将 node 指向该节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; // 如果是 TreeNode 类型，调用红黑树的查找逻辑定位待删除节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; // 2. 遍历链表，找到待删除节点 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; // 3. 删除节点，并修复链表或红黑树 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 其他细节被 transient 所修饰 table 变量如果大家细心阅读 HashMap 的源码，会发现桶数组 table 被声明为 transient。 transient 表示易变的意思，在 Java 中，被该关键字修饰的变量不会被默认的序列化机制序列化。 我们再回到源码中，考虑一个问题：桶数组 table 是 HashMap 底层重要的数据结构，不序列化的话，别人还怎么还原呢？这里简单说明一下吧，HashMap 并没有使用默认的序列化机制，而是通过实现readObject/writeObject两个方法自定义了序列化的内容。这样做是有原因的，试问一句，HashMap 中存储的内容是什么？不用说，大家也知道是键值对。所以只要我们把键值对序列化了，我们就可以根据键值对数据重建 HashMap。有的朋友可能会想，序列化 table 不是可以一步到位，后面直接还原不就行了吗？这样一想，倒也是合理。但序列化 talbe 存在着两个问题： table 多数情况下是无法被存满的，序列化未使用的部分，浪费空间 同一个键值对在不同 JVM 下，所处的桶位置可能是不同的，在不同的 JVM 下反序列化 table 可能会发生错误。 以上两个问题中，第一个问题比较好理解，第二个问题解释一下。HashMap 的get/put/remove等方法第一步就是根据 hash 找到键所在的桶位置，但如果键没有覆写 hashCode 方法，计算 hash 时最终调用 Object 中的 hashCode 方法。但 Object 中的 hashCode 方法是 native 型的，不同的 JVM 下，可能会有不同的实现，产生的 hash 可能也是不一样的。也就是说同一个键在不同平台下可能会产生不同的 hash，此时再对在同一个 table 继续操作，就会出现问题。 综上所述，大家应该能明白 HashMap 不序列化 table 的原因了。 非线程安全的表现数据覆盖如果有A、B两个线程同时对hashMap进行插入操作，刚好插入的数据经过哈希计算后得到的哈希码相同，且该位置还没有其他的数据。假设A在完成hash冲突检测后就把资源让给了线程B，此时B判断该位置没有哈希冲突（线程A的数据还没插入），就完成了数据插入，之后A又得到了资源，将直接在该位置插入而不用再判断。这时候，线程A把线程B插入的数据覆盖了。 死锁问题假设有二个进程T1、T2，HashMap容量为2，T1线程放入key A、C、D、E。在T1线程中A、C Hash值相同，于是形成一个链接，假设为A-&gt;C，而D、E Hash值不同，于是需要扩容，需要把数据从老的Hash表中迁移到新的Hash表中(refresh)。这时T2进程闯进来了，T1暂时挂起，T2进程也准备放入新的key，这时也发现容量不足，也refresh一把。refresh之后原来的链表结构假设为C-&gt;A，之后T1进程继续执行，链接结构为A-&gt;C，这时就形成A.next = C,B.next = A的环形链表。一旦取值进入这个环形链表就会陷入死循环。 参考： http://www.tianxiaobo.com/2018/01/18/HashMap-%E6%BA%90%E7%A0%81%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90-JDK1-8/#4-%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/HashMap.md]]></content>
      <categories>
        <category>java面试准备</category>
        <category>集合框架</category>
      </categories>
      <tags>
        <tag>集合框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList]]></title>
    <url>%2F2019%2F04%2F15%2FLinkedList%2F</url>
    <content type="text"><![CDATA[概述LinkedList 是 Java 集合框架中一个重要的实现，其底层采用的双向链表结构。 123456789101112131415transient int size = 0; /** * Pointer to first node. * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */ transient Node&lt;E&gt; first; /** * Pointer to last node. * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */ transient Node&lt;E&gt; last; 和 ArrayList 一样，LinkedList 也支持空值和重复值。 由于 LinkedList 基于链表实现，存储元素过程中，无需像 ArrayList 那样进行扩容。但有得必有失，LinkedList 存储元素的节点需要额外的空间存储前驱和后继的引用。另一方面，LinkedList 在链表头部和尾部插入效率比较高，但在指定位置进行插入时，效率一般。原因是，在指定位置插入需要定位到该位置处的节点，此操作的时间复杂度为O(N)。 最后，LinkedList 是非线程安全的集合类，并发环境下，多个线程同时操作 LinkedList，会引发不可预知的错误。如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法： 1List list=Collections.synchronizedList(new LinkedList(...)); 内部结构分析LinkedList类中的一个内部私有类Node，这个类就代表双端链表的节点Node： 1234567891011private static class Node&lt;E&gt; &#123; E item;//节点值 Node&lt;E&gt; next;//后继节点 Node&lt;E&gt; prev;//前驱节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; 继承体系LinkedList的继承体系较为复杂，继承自 AbstractSequentialList，同时又实现了 List 和 Deque 接口。 LinkedList底层的链表结构使它支持高效的插入和删除操作，另外它实现了Deque接口，使得LinkedList类也具有 队列的特性，我们可以这样使用： 1Queue&lt;T&gt; queue = new LinkedList&lt;&gt;(); 继承体系图如下（删除了部分实现的接口）： LinkedList继承自 AbstractSequentialList ，AbstractSequentialList 又是什么呢？从实现上，AbstractSequentialList提供了一套基于顺序访问的接口。通过继承此类，子类仅需实现部分代码即可拥有完整的一套访问某种序列表（比如链表）的接口。深入源码，AbstractSequentialList 提供的方法基本上都是通过 ListIterator实现的，比如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185public abstract class AbstractSequentialList&lt;E&gt; extends AbstractList&lt;E&gt; &#123; /** * Sole constructor. (For invocation by subclass constructors, typically * implicit.) */ protected AbstractSequentialList() &#123; &#125; /** * Returns the element at the specified position in this list. * * &lt;p&gt;This implementation first gets a list iterator pointing to the * indexed element (with &lt;tt&gt;listIterator(index)&lt;/tt&gt;). Then, it gets * the element using &lt;tt&gt;ListIterator.next&lt;/tt&gt; and returns it. * * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public E get(int index) &#123; try &#123; return listIterator(index).next(); &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException("Index: "+index); &#125; &#125; /** * Replaces the element at the specified position in this list with the * specified element (optional operation). * * &lt;p&gt;This implementation first gets a list iterator pointing to the * indexed element (with &lt;tt&gt;listIterator(index)&lt;/tt&gt;). Then, it gets * the current element using &lt;tt&gt;ListIterator.next&lt;/tt&gt; and replaces it * with &lt;tt&gt;ListIterator.set&lt;/tt&gt;. * * &lt;p&gt;Note that this implementation will throw an * &lt;tt&gt;UnsupportedOperationException&lt;/tt&gt; if the list iterator does not * implement the &lt;tt&gt;set&lt;/tt&gt; operation. * * @throws UnsupportedOperationException &#123;@inheritDoc&#125; * @throws ClassCastException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; * @throws IllegalArgumentException &#123;@inheritDoc&#125; * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public E set(int index, E element) &#123; try &#123; ListIterator&lt;E&gt; e = listIterator(index); E oldVal = e.next(); e.set(element); return oldVal; &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException("Index: "+index); &#125; &#125; /** * Inserts the specified element at the specified position in this list * (optional operation). Shifts the element currently at that position * (if any) and any subsequent elements to the right (adds one to their * indices). * * &lt;p&gt;This implementation first gets a list iterator pointing to the * indexed element (with &lt;tt&gt;listIterator(index)&lt;/tt&gt;). Then, it * inserts the specified element with &lt;tt&gt;ListIterator.add&lt;/tt&gt;. * * &lt;p&gt;Note that this implementation will throw an * &lt;tt&gt;UnsupportedOperationException&lt;/tt&gt; if the list iterator does not * implement the &lt;tt&gt;add&lt;/tt&gt; operation. * * @throws UnsupportedOperationException &#123;@inheritDoc&#125; * @throws ClassCastException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; * @throws IllegalArgumentException &#123;@inheritDoc&#125; * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public void add(int index, E element) &#123; try &#123; listIterator(index).add(element); &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException("Index: "+index); &#125; &#125; /** * Removes the element at the specified position in this list (optional * operation). Shifts any subsequent elements to the left (subtracts one * from their indices). Returns the element that was removed from the * list. * * &lt;p&gt;This implementation first gets a list iterator pointing to the * indexed element (with &lt;tt&gt;listIterator(index)&lt;/tt&gt;). Then, it removes * the element with &lt;tt&gt;ListIterator.remove&lt;/tt&gt;. * * &lt;p&gt;Note that this implementation will throw an * &lt;tt&gt;UnsupportedOperationException&lt;/tt&gt; if the list iterator does not * implement the &lt;tt&gt;remove&lt;/tt&gt; operation. * * @throws UnsupportedOperationException &#123;@inheritDoc&#125; * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public E remove(int index) &#123; try &#123; ListIterator&lt;E&gt; e = listIterator(index); E outCast = e.next(); e.remove(); return outCast; &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException("Index: "+index); &#125; &#125; // Bulk Operations /** * Inserts all of the elements in the specified collection into this * list at the specified position (optional operation). Shifts the * element currently at that position (if any) and any subsequent * elements to the right (increases their indices). The new elements * will appear in this list in the order that they are returned by the * specified collection's iterator. The behavior of this operation is * undefined if the specified collection is modified while the * operation is in progress. (Note that this will occur if the specified * collection is this list, and it's nonempty.) * * &lt;p&gt;This implementation gets an iterator over the specified collection and * a list iterator over this list pointing to the indexed element (with * &lt;tt&gt;listIterator(index)&lt;/tt&gt;). Then, it iterates over the specified * collection, inserting the elements obtained from the iterator into this * list, one at a time, using &lt;tt&gt;ListIterator.add&lt;/tt&gt; followed by * &lt;tt&gt;ListIterator.next&lt;/tt&gt; (to skip over the added element). * * &lt;p&gt;Note that this implementation will throw an * &lt;tt&gt;UnsupportedOperationException&lt;/tt&gt; if the list iterator returned by * the &lt;tt&gt;listIterator&lt;/tt&gt; method does not implement the &lt;tt&gt;add&lt;/tt&gt; * operation. * * @throws UnsupportedOperationException &#123;@inheritDoc&#125; * @throws ClassCastException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; * @throws IllegalArgumentException &#123;@inheritDoc&#125; * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; try &#123; boolean modified = false; ListIterator&lt;E&gt; e1 = listIterator(index); Iterator&lt;? extends E&gt; e2 = c.iterator(); while (e2.hasNext()) &#123; e1.add(e2.next()); modified = true; &#125; return modified; &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException("Index: "+index); &#125; &#125; // Iterators /** * Returns an iterator over the elements in this list (in proper * sequence).&lt;p&gt; * * This implementation merely returns a list iterator over the list. * * @return an iterator over the elements in this list (in proper sequence) */ public Iterator&lt;E&gt; iterator() &#123; return listIterator(); &#125; /** * Returns a list iterator over the elements in this list (in proper * sequence). * * @param index index of first element to be returned from the list * iterator (by a call to the &lt;code&gt;next&lt;/code&gt; method) * @return a list iterator over the elements in this list (in proper * sequence) * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public abstract ListIterator&lt;E&gt; listIterator(int index);&#125; 所以只要继承类实现了 listIterator 方法，它不需要再额外实现什么即可使用。 对于随机访问集合类一般建议继承 AbstractList 而不是 AbstractSequentialList。 LinkedList和其父类一样，也是基于顺序访问。所以 LinkedList 继承了AbstractSequentialList，但 LinkedList 并没有直接使用父类的方法，而是重新实现了一套的方法。 源码分析构造方法LinkedList有两个构造方法，一个是空构造方法，一个是用已有的集合创建链表的构造方法： 123456789101112131415161718/** * Constructs an empty list. */public LinkedList() &#123; &#125; /** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection's * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */ public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c); &#125; 查找LinkedList 底层基于链表结构，无法向 ArrayList 那样随机访问指定位置的元素。 LinkedList 查找过程要稍麻烦一些，需要从链表头结点（或尾节点）向后（向前）查找，时间复杂度为 `O(N)`。相关源码如下： 根据指定位置取数据的方法get(int index)： 根据指定索引返回数据 12345678910111213141516171819202122232425public E get(int index) &#123; //检查index范围是否在size之内 checkElementIndex(index); //调用Node(index)去找到index对应的node然后返回它的值 return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; /* * 则从头节点开始查找，否则从尾节点查找 * 查找位置 index 如果小于节点数量的一半， */ if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; // 循环向后查找，直至 i == index for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 上面的代码比较简单，主要是通过遍历的方式定位目标位置的节点。获取到节点后，取出节点存储的值返回即可。这里面有个小优化，即通过比较 index 与节点数量 size/2 的大小，决定从头结点还是尾节点进行查找。 获取头结点(index = 0)1234567891011121314151617181920public E getFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return f.item;&#125;public E element() &#123; return getFirst();&#125;public E peek() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : f.item;&#125;public E peekFirst() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : f.item;&#125; 区别： getFirst()，element()，peek()，peekFirst() 这四个获取头结点方法的区别在于对链表为空时的处理，是抛出异常还是返回null。 其中getFirst() 和element() 方法将会在链表为空时，抛出NoSuchElementException。 获取尾结点(index = -1)1234567891011public E getLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return l.item;&#125;public E peekLast() &#123; final Node&lt;E&gt; l = last; return (l == null) ? null : l.item;&#125; 两者区别： getLast() 方法在链表为空时，会抛出NoSuchElementException，而peekLast() 则不会，只是会返回 null。 根据对象得到索引的方法int indexOf(Object o)： 从头遍历找 12345678910111213141516171819public int indexOf(Object o) &#123; int index = 0; if (o == null) &#123; //从头遍历 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) return index; index++; &#125; &#125; else &#123; //从头遍历 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) return index; index++; &#125; &#125; return -1;&#125; int lastIndexOf(Object o)： 从尾遍历找 12345678910111213141516171819public int lastIndexOf(Object o) &#123; int index = size; if (o == null) &#123; //从尾遍历 for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (x.item == null) return index; &#125; &#125; else &#123; //从尾遍历 for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (o.equals(x.item)) return index; &#125; &#125; return -1;&#125; 检查链表是否包含某对象contains(Object o)： 检查对象o是否存在于链表中 123456789101112/** * Returns &#123;@code true&#125; if this list contains the specified element. * More formally, returns &#123;@code true&#125; if and only if this list contains * at least one element &#123;@code e&#125; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;e==null&amp;nbsp;:&amp;nbsp;o.equals(e))&lt;/tt&gt;. * * @param o element whose presence in this list is to be tested * @return &#123;@code true&#125; if this list contains the specified element */ public boolean contains(Object o) &#123; return indexOf(o) != -1; &#125; 遍历链表的遍历过程也很简单，和上面查找过程类似，我们从头节点往后遍历就行了。但对于 LinkedList 的遍历还是需要注意一些，不然可能会导致代码效率低下。通常情况下，我们会使用 for each 遍历 LinkedList，而 for each 最终转换成迭代器形式。所以分析 LinkedList 的遍历的核心就是它的迭代器实现，相关代码如下： 1234567891011121314151617181920212223242526272829303132333435public ListIterator&lt;E&gt; listIterator(int index) &#123; checkPositionIndex(index); return new ListItr(index);&#125;private class ListItr implements ListIterator&lt;E&gt; &#123; private Node&lt;E&gt; lastReturned; private Node&lt;E&gt; next; private int nextIndex; private int expectedModCount = modCount; /** 构造方法将 next 引用指向指定位置的节点 */ ListItr(int index) &#123; // assert isPositionIndex(index); next = (index == size) ? null : node(index); nextIndex = index; &#125; public boolean hasNext() &#123; return nextIndex &lt; size; &#125; public E next() &#123; checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; // 调用 next 方法后，next 引用都会指向他的后继节点 nextIndex++; return lastReturned.item; &#125; // 省略部分方法&#125; 插入LinkedList 除了实现了 List 接口相关方法，还实现了 Deque 接口的很多方法，所以我们有很多种方式插入元素。 LinkedList 插入元素的过程实际上就是链表链入节点的过程，下面我们分析 List 接口中相关的插入方法： 将元素添加到链表尾部add(E e) 方法：将元素添加到链表尾部 在指定位置添加元素add(int index,E e)：在指定位置添加元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** 在链表尾部插入元素 */public boolean add(E e) &#123; linkLast(e); return true;&#125;/** 在链表指定位置插入元素 */public void add(int index, E element) &#123; checkPositionIndex(index);//检查索引是否处于[0-size]之间 // 判断 index 是不是链表尾部位置，如果是，直接将元素节点插入链表尾部即可 if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;/** 将元素节点插入到链表尾部 */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; // 创建节点，并指定节点前驱为链表尾节点 last，后继引用为空 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // 将 last 引用指向新节点 last = newNode; // 判断尾节点是否为空，为空表示当前链表还没有节点 if (l == null) first = newNode; else l.next = newNode; // 让原尾节点后继引用 next 指向新的尾节点 size++; modCount++;&#125;/** 将元素节点插入到 succ 之前的位置 */void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; // 1. 初始化节点，并指明前驱和后继节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); // 2. 将 succ 节点前驱引用 prev 指向新节点 succ.prev = newNode; // 判断尾节点是否为空，为空表示当前链表还没有节点 if (pred == null) first = newNode; else pred.next = newNode; // 3. succ 节点前驱的后继引用指向新节点 size++; modCount++;&#125; 上面是插入过程的源码，我对源码进行了比较详细的注释，应该不难看懂。上面两个 add 方法只是对操作链表的方法做了一层包装，核心逻辑在 linkBefore 和 linkLast 中。 这里以 linkBefore 为例，linkBefore方法需要给定两个参数，一个插入节点的值，一个指定的node，所以我们又调用了Node(index)去找到index对应的node，它的逻辑流程如下： 创建新节点，并指明新节点的前驱和后继 将 succ 的前驱引用指向新节点 如果 succ 的前驱不为空，则将 succ 前驱的后继引用指向新节点 对应于下图： 将集合插到链表尾部addAll(Collection c )：将集合插入到链表尾部 123public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c);&#125; 将集合从指定位置开始插入addAll(int index, Collection c)： 将集合从指定位置开始插入 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; //1:检查index范围是否在size之内 checkPositionIndex(index); //2:toArray()方法把集合的数据存到对象数组中 Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; //3：得到插入位置的前驱节点和后继节点 Node&lt;E&gt; pred, succ; //如果插入位置为尾部，前驱节点为last，后继节点为null if (index == size) &#123; succ = null; pred = last; &#125; //否则，调用node()方法得到后继节点，再得到前驱节点 else &#123; succ = node(index); pred = succ.prev; &#125; // 4：遍历数据将数据插入 for (Object o : a) &#123; @SuppressWarnings("unchecked") E e = (E) o; //创建新节点 Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); //如果插入位置在链表头部 if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; &#125; //如果插入位置在尾部，重置last节点 if (succ == null) &#123; last = pred; &#125; //否则，将插入的链表与先前链表连接起来 else &#123; pred.next = succ; succ.prev = pred; &#125; size += numNew; modCount++; return true;&#125; 上面可以看出addAll()方法通常包括下面四个步骤： 检查index范围是否在size之内 toArray()方法把集合的数据存到对象数组中 得到插入位置的前驱和后继节点 遍历数据，将数据插入到指定位置 将元素添加到链表头部addFirst(E e)： 将元素添加到链表头部 1234567891011121314151617public void addFirst(E e) &#123; linkFirst(e);&#125;private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f);//新建节点，以头节点为后继节点 first = newNode; //如果链表为空，last节点也指向该节点 if (f == null) last = newNode; //否则，将头节点的前驱指针指向新节点，也就是指向前一个元素 else f.prev = newNode; size++; modCount++;&#125; 将元素添加到链表尾部addLast(E e)： 将元素添加到链表尾部，与 add(E e) 方法一样 123public void addLast(E e) &#123; linkLast(e);&#125; 删除如果大家看懂了上面的插入源码分析，那么再看删除操作实际上也很简单了。删除操作通过解除待删除节点与前后节点的链接，即可完成任务。过程比较简单，看源码吧： 删除指定元素remove(Object o): 删除指定元素 删除指定位置的元素remove(int index)：删除指定位置的元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//删除指定元素public boolean remove(Object o) &#123; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; // 遍历链表，找到要删除的节点 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); // 将节点从链表中移除 return true; &#125; &#125; &#125; return false;&#125;//删除指定位置的元素public E remove(int index) &#123; checkElementIndex(index); // 通过 node 方法定位节点，并调用 unlink 将节点从链表中移除 return unlink(node(index));&#125;/** 将某个节点从链表中移除 */E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; // prev 为空，表明删除的是头节点 if (prev == null) &#123; first = next; &#125; else &#123; // 将 x 的前驱的后继指向 x 的后继 prev.next = next; // 将 x 的前驱引用置空，断开与前驱的链接 x.prev = null; &#125; // next 为空，表明删除的是尾节点 if (next == null) &#123; last = prev; &#125; else &#123; // 将 x 的后继的前驱指向 x 的前驱 next.prev = prev; // 将 x 的后继引用置空，断开与后继的链接 x.next = null; &#125; // 将 item 置空，方便 GC 回收 x.item = null; size--; modCount++; return element;&#125; 和插入操作一样，删除操作方法也是对底层方法的一层保证，核心逻辑在底层 unlink 方法中。所以长驱直入，直接分析 unlink 方法吧。unlink 方法的逻辑如下（假设删除的节点既不是头节点，也不是尾节点）： 将待删除节点 x 的前驱的后继指向 x 的后继 将待删除节点 x 的前驱引用置空，断开与前驱的链接 将待删除节点 x 的后继的前驱指向 x 的前驱 将待删除节点 x 的后继引用置空，断开与后继的链接 对应下图： 删除头节点remove() ，removeFirst()，pop()： 删除头节点 123456789101112131415161718192021222324252627282930313233 public E pop() &#123; return removeFirst(); &#125; public E remove() &#123; return removeFirst(); &#125; public E removeFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f); &#125;/** * Unlinks non-null first node f. */ private E unlinkFirst(Node&lt;E&gt; f) &#123; // assert f == first &amp;&amp; f != null; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC first = next; if (next == null) last = null; else next.prev = null; size--; modCount++; return element; &#125; 删除尾节点removeLast()，pollLast()： 删除尾节点 123456789101112131415161718192021222324252627282930 public E removeLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l); &#125; public E pollLast() &#123; final Node&lt;E&gt; l = last; return (l == null) ? null : unlinkLast(l); &#125;/** * Unlinks non-null last node l. */ private E unlinkLast(Node&lt;E&gt; l) &#123; // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element; &#125; 区别： 区别在于对于链表为空时的处理，removeLast()在链表为空时将抛出NoSuchElementException，而pollLast()方法返回null。 参考： http://www.tianxiaobo.com/2018/01/31/LinkedList-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-JDK-1-8/#4%E6%80%BB%E7%BB%93 https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/LinkedList.md]]></content>
      <categories>
        <category>java面试准备</category>
        <category>集合框架</category>
      </categories>
      <tags>
        <tag>集合框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList]]></title>
    <url>%2F2019%2F04%2F15%2FArrayList%2F</url>
    <content type="text"><![CDATA[概述ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 它继承于 AbstractList，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 在我们学数据结构的时候就知道了线性表的顺序存储，插入删除元素的时间复杂度为O(n),求表长以及增加元素，取第 i 元素的时间复杂度为O(1) ArrayList 继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能。 ArrayList 实现了RandomAccess 接口， RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了Cloneable 接口，即覆盖了函数 clone()，能被克隆。 ArrayList 实现java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 和 Vector 不同，ArrayList 中的操作不是线程安全的！所以，建议在单线程中才使用 ArrayList，而在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 源码分析构造方法ArrayList有三个构造方法，相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125; &#125; /** *默认构造函数，DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection&lt;? extends E&gt; c) &#123; // elementData = c.toArray(); //如果指定集合元素个数不为0 if ((size = elementData.length) != 0) &#123; // c.toArray 可能返回的不是Object类型的数组所以加上下面的语句用于判断， //这里用到了反射里面的getClass()方法 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 用空数组代替 this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; 上面的代码比较简单，三个构造方法做的事情并不复杂，目的都是初始化底层数组 elementData。 区别在于无参构造方法会将 elementData 初始化一个空数组，插入元素时，扩容将会按默认值重新初始化数组。而有参的构造方法则会将 elementData 初始化为参数值大小（&gt;= 0）的数组。一般情况下，我们用默认的构造方法即可。倘若在可知道将会向 ArrayList 插入多少元素的情况下，应该使用有参构造方法。按需分配，避免浪费。 方法参数为Collection集合的构造参数旨在构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 插入对于数组（线性表）结构，插入操作分为两种情况。一种是在元素序列尾部插入，另一种是在元素序列其他位置插入。ArrayList 的源码里也体现了这两种插入情况，如下： 12345678910111213141516171819202122232425/** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) &#123; rangeCheckForAdd(index); //检测是否需要扩容 ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; 对于在元素序列尾部插入，这种情况比较简单，只需两个步骤即可： 检测数组是否有足够的空间插入 将新元素插入至序列尾部 如下图： 如果是在元素序列指定位置（假设该位置合理）插入，则情况稍微复杂一点，需要三个步骤： 检测数组是否有足够的空间 将 index 及其之后的所有元素向后移一位 将新元素插入至 index 处 如下图： 总结：从上图可以看出，将新元素插入至序列指定位置，需要先将该位置及其之后的元素都向后移动一位，为新元素腾出位置。这个操作的时间复杂度为O(N)，频繁移动元素可能会导致效率问题，特别是集合中元素数量较多时。在日常开发中，若非所需，我们应当尽量避免在大集合中调用第二个插入方法。 删除不同于插入操作，ArrayList 没有无参删除方法。所以其只能删除指定位置的元素或删除指定元素，这样就无法避免移动元素（除非从元素序列的尾部删除）。相关代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; &#125; /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; /** * 从列表中删除所有元素。 */ public void clear() &#123; modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; &#125; 上面的删除方法并不复杂，这里以第一个删除方法为例，删除一个元素步骤如下： 获取指定位置 index 处的元素值 将 index + 1 及之后的元素向前移动一位 将最后一个元素置空，并将 size 值减 1 返回被删除值，完成删除操作 如下图： 现在，考虑这样一种情况。我们往 ArrayList 插入大量元素后，又删除很多元素，此时底层数组会空闲处大量的空间。因为 ArrayList 没有自动缩容机制，导致底层数组大量的空闲空间不能被释放，造成浪费。对于这种情况，ArrayList 也提供了相应的处理方法，如下： 1234567891011/** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存 储。 */ public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125; &#125; 通过上面的方法，我们可以手动触发 ArrayList 的缩容机制。这样就可以释放多余的空间，提高空间利用率。 如下图： 扩容对于变长数据结构，当结构中没有空余空间可供使用时，就需要进行扩容。在 ArrayList 中，当空间用完，其会按照原数组空间的1.5倍进行扩容。相关源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125;/** 计算最小容量 */private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity; &#125;/** 扩容的入口方法 */private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125; //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 接下来我们一步步分析ArrayList扩容机制，这里以无参构造函数创建的 ArrayList 为例分析。 先来看看add 方法 12345678910/** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123; //添加元素之前，先调用ensureCapacityInternal方法 ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; 再来看看ensureCapacityInternal() 方法 可以看到 add 方法 首先调用了ensureCapacityInternal(size + 1) 12345678910private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity; &#125; private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); &#125; 当要add进第1个元素时，minCapacity为1，在Math.max()方法比较后，minCapacity 为10。 ensureExplicitCapacity()方法 如果调用 ensureCapacityInternal() 方法就一定会执行grow()方法，下面我们来研究一下这个方法的源码！ 123456789//判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; 我们来仔细分析一下： 当我们要 add 进第1个元素到 ArrayList 时，elementData.length 为0（因为还是一个空的 list ），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为10。此时，minCapacity - elementData.length &gt; 0成立，所以会进入 grow(minCapacity) 方法。 当add第2个元素时，minCapacity 为2，此时elementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length &gt; 0不成立，所以不会进入（执行）grow(minCapacity) 方法。 添加第3、4···到第10个元素时，依然不会执行grow方法，数组容量都为10。 直到添加第11个元素，minCapacity(为11)比elementData.length（为10）要大，进入grow方法进行扩容。 grow()方法 123456789101112131415161718192021222324/** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的 新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍！（JDK1.6版本以后） “&gt;&gt;”（移位运算符）：&gt;&gt;1 右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity / 2。对于大数据的2进制运算，位移运算符比那些普通运算符的运算要快很多，因为程序仅仅移动一下而已，不去计算,这样提高了效率，节省了资源。 我们再来通过例子探究一下grow() 方法 ： 当add第1个元素时，oldCapacity 为0，经比较后第一个if判断成立，newCapacity = minCapacity(为10)。但是第二个if判断不会成立，即newCapacity 不比 MAX_ARRAY_SIZE大，则不会进入 hugeCapacity 方法。数组容量为10，add方法中 return true，size增为1。 当add第11个元素进入grow方法时，newCapacity为15，比minCapacity（为11）大，第一个if判断不成立。新容量没有大于数组最大size，不会进入hugeCapacity方法。数组容量扩为15，add方法中return true，size增为11。 以此类推······ 这里补充一点比较重要，但是容易被忽视掉的知识点： java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! hugeCapacity()方法 从上面 grow() 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果minCapacity大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8。 1234567891011private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); //对minCapacity和MAX_ARRAY_SIZE进行比较 //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小 //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小 //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; ensureCapacity()方法 ArrayList 源码中有一个 ensureCapacity 方法不知道大家注意到没有，这个方法 ArrayList 内部没有被调用过，所以很显然是提供给用户调用的，那么这个方法有什么作用呢？ 1234567891011121314151617/** 如有必要，增加此 ArrayList 实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 * * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; 总结：最好在 add 大量元素之前用 ensureCapacity 方法，以减少增量重新分配的次数 遍历ArrayList 实现了 RandomAccess 接口（该接口是个标志性接口），表明它具有随机访问的能力。ArrayList 底层基于数组实现，所以它可在常数阶的时间内完成随机访问，效率很高。对 ArrayList 进行遍历时，一般情况下，我们喜欢使用 for each 循环遍历，但这并不是推荐的遍历方式。ArrayList 具有随机访问的能力，如果在一些效率要求比较高的场景下，更推荐下面这种方式： 123for (int i = 0; i &lt; list.size(); i++) &#123; list.get(i);&#125; 至于原因也不难理解，for each 最终会被转换成迭代器遍历的形式，效率不如上面的遍历方式。 关于遍历时的删除遍历时删除是一个不正确的操作，即使有时候代码不出现异常，但执行逻辑也会出现问题。关于这个问题，阿里巴巴 Java 开发手册里也有所提及。这里引用一下： 【强制】不要在 for each 循环里进行元素的 remove/add 操作。remove 元素请使用 Iterator 方式，如果并发操作，需要对 Iterator 对象加锁。 相关代码（稍作修改）如下： 12345678910 List&lt;String&gt; a = new ArrayList&lt;String&gt;(); a.add("1"); a.add("2"); for (String temp : a) &#123; System.out.println(temp); if("1".equals(temp))&#123; a.remove(temp); &#125; &#125;&#125; 相信有些朋友应该看过这个，并且也执行过上面的程序。上面的程序执行起来不会虽不会出现异常，但代码执行逻辑上却有问题，只不过这个问题隐藏的比较深。 我们把 temp 变量打印出来，会发现只打印了数字1，2没打印出来。初看这个执行结果确实很让人诧异，不明原因。如果死抠上面的代码，我们很难找出原因，此时需要稍微转换一下思路。 我们都知道 Java 中的 for each 是个语法糖，编译成字节码后会被转成用迭代器遍历的方式。所以我们可以把上面的代码转换一下，等价于下面形式： 1234567891011List&lt;String&gt; a = new ArrayList&lt;&gt;();a.add("1");a.add("2");Iterator&lt;String&gt; it = a.iterator();while (it.hasNext()) &#123; String temp = it.next(); System.out.println("temp: " + temp); if("1".equals(temp))&#123; a.remove(temp); &#125;&#125; 这个时候，我们再去分析一下 ArrayList 的迭代器源码就能找出原因，代码如下： 123456789101112131415161718192021222324252627282930 private class Itr implements Iterator&lt;E&gt; &#123; int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; public boolean hasNext() &#123; return cursor != size; &#125; @SuppressWarnings("unchecked") public E next() &#123; // 并发修改检测，检测不通过则抛出异常 checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; &#125; final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; // 省略不相关的代码&#125; 我们一步一步执行一下上面的代码，第一次进入 while 循环时，一切正常，元素 1 也被删除了。但删除元素 1 后，就无法再进入 while 循环，此时 it.hasNext() 为 false。原因是删除元素 1 后，元素计数器 size = 1，而迭代器中的 cursor 也等于 1，从而导致 it.hasNext() 返回false。归根结底，上面的代码段没抛异常的原因是，循环提前结束，导致 next 方法没有机会抛异常。不信的话，大家可以把代码稍微修改一下，即可发现问题： 123456789101112List&lt;String&gt; a = new ArrayList&lt;&gt;();a.add("1");a.add("2");a.add("3");Iterator&lt;String&gt; it = a.iterator();while (it.hasNext()) &#123; String temp = it.next(); System.out.println("temp: " + temp); if("1".equals(temp))&#123; a.remove(temp); &#125;&#125; 以上是关于遍历时删除的分析，在日常开发中，我们要避免上面的做法。正确的做法使用迭代器提供的删除方法，而不是直接删除。 System.arraycopy()和Arrays.copyOf()方法System.arraycopy() 方法：123456789101112131415/** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证 capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组 中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; Arrays.copyOf()方法：123456789/** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() &#123; //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); &#125; 两者的联系与区别联系：看两者源代码可以发现copyOf()内部调用了System.arraycopy()方法 区别： arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置。 copyOf()是系统自动在内部新建一个数组，并返回该数组。 参考： https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/ArrayList.md#systemarraycopy%E5%92%8Carrayscopyof%E6%96%B9%E6%B3%95 http://www.tianxiaobo.com/2018/01/31/LinkedList-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-JDK-1-8/#4%E6%80%BB%E7%BB%93]]></content>
      <categories>
        <category>java面试准备</category>
        <category>集合框架</category>
      </categories>
      <tags>
        <tag>集合框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对Java中interrupt、interrupted和isInterrupted的理解]]></title>
    <url>%2F2019%2F03%2F27%2F%E5%AF%B9Java%E4%B8%ADinterrupt%E3%80%81interrupted%E5%92%8CisInterrupted%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[interrupt()interrupt方法用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。 注意：线程中断仅仅是设置线程的中断状态位，不会停止线程。 支持线程中断的方法（也就是线程中断后会抛出interruptedException的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。 当处于阻塞状态的线程调用interrupt()时，会抛出InterruptedException，此时线程会退出阻塞，并会清除中断标记。也就是我们调用sleep、wait等此类可中断（throw InterruptedException）方法时，一旦方法抛出InterruptedException，当前调用该方法的线程的中断状态就会被jvm自动清除了，就是说我们调用该线程的isInterrupted 方法时是返回false。 如果你想保持中断状态，可以再次调用interrupt方法设置中断状态。这样做的原因是，java的中断并不是真正的中断线程，而只设置标志位（中断位）来通知用户。如果你捕获到中断异常，说明当前线程已经被中断，不需要继续保持中断位。 下面给出例子来看看： 1234567891011121314151617181920212223242526public class Interrupt &#123; public static void main(String[] args) throws Exception &#123; Thread t = new Thread(new Worker()); t.start(); Thread.sleep(200); t.interrupt(); System.out.println("Main thread stopped."); &#125; public static class Worker implements Runnable &#123; public void run() &#123; System.out.println("Worker started."); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; System.out.println("Worker IsInterrupted: " + Thread.currentThread().isInterrupted()); &#125; System.out.println("Worker stopped."); &#125; &#125;&#125; 内容很简答：主线程main启动了一个子线程Worker，然后让worker睡500ms，而main睡200ms，之后main调用worker线程的interrupt方法去中断worker，worker被中断后打印中断的状态。 下面是执行结果： 1234Worker started.Main thread stopped.Worker IsInterrupted: falseWorker stopped. 打印的结果充分地印证了当可中断的方法抛出异常时，线程中断的标记也会跟着被jvm所清除，因此调用isInterrupted()方法返回的是false。 interrupted()和isInterrupted()我们先看看这两个方法的源码： 1234567891011121314 public static boolean interrupted() &#123; return currentThread().isInterrupted(true); &#125; public boolean isInterrupted() &#123; return isInterrupted(false); &#125; /**2 * Tests if some Thread has been interrupted. The interrupted state3 * is reset or not based on the value of ClearInterrupted that is4 * passed.5 */6 private native boolean isInterrupted(boolean ClearInterrupted); 原来这是一个本地方法，看不到源码。不过没关系，通过参数名我们就能知道，这个参数代表是否要清除状态位。 如果这个参数为true，说明返回线程的状态位后，要清掉原来的状态位（恢复成原来情况）。这个参数为false，就是直接返回线程的状态位。 这两个方法有两个主要区别： interrupted()是作用于当前线程，静态方法，isInterrupted()是作用于调用该方法的线程对象所对应的线程；(线程对象对应的线程不一定是当前运行的线程。例如我们可以在A线程中去调用B线程对象的isInterrupted()方法) 这两个方法最终都会调用同一个方法，只不过参数一个是true，一个是false； 这两个方法很好区分，只有当前线程才能清除自己的中断位。(对应interrupted()方法) interrupted()是静态方法，返回的是当前线程的中断状态。例如，如果当前线程被中断（没有抛出中断异常，否则中断状态就会被清除），你调用interrupted()方法，第一次会返回true。然后，当前线程的中断状态被方法内部清除了。第二次调用时就会返回false。如果你刚开始一直调用isInterrupted()，则会一直返回true，除非中间线程的中断状态被其他操作清除了。 下面我们看看例子： 12345678910111213141516171819202122232425262728293031323334public class Interrupt &#123; public static void main(String[] args) throws Exception &#123; Thread t = new Thread(new Worker()); t.start(); Thread.sleep(200); t.interrupt(); System.out.println("Main thread stopped."); &#125; public static class Worker implements Runnable &#123; public void run() &#123; System.out.println("Worker started."); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; Thread curr = Thread.currentThread(); //再次调用interrupt方法中断自己，将中断状态设置为“中断” curr.interrupt(); System.out.println("Worker IsInterrupted: " + curr.isInterrupted()); System.out.println("Worker IsInterrupted: " + curr.isInterrupted()); System.out.println("Static Call: " + Thread.interrupted());//clear status System.out.println("---------After Interrupt Status Cleared----------"); System.out.println("Static Call: " + Thread.interrupted()); System.out.println("Worker IsInterrupted: " + curr.isInterrupted()); System.out.println("Worker IsInterrupted: " + curr.isInterrupted()); &#125; System.out.println("Worker stopped."); &#125; &#125;&#125; 执行结果： 12345678910Worker started.Main thread stopped.Worker IsInterrupted: trueWorker IsInterrupted: trueStatic Call: true---------After Interrupt Status Cleared----------Static Call: falseWorker IsInterrupted: falseWorker IsInterrupted: falseWorker stopped. 从执行结果也可以看到，前两次调用isInterrupted方法都返回true，说明isInterrupted方法不会改变线程的中断状态，而接下来调用静态的interrupted()方法，第一次返回了true，表示线程被中断，第二次则返回了false，因为第一次调用的时候已经清除了中断状态。最后两次调用isInterrupted()方法就肯定返回false了。 参考： https://www.cnblogs.com/zhuhongchang/p/8467803.html https://my.oschina.net/itblog/blog/787024]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring之注解实现原理深度解析]]></title>
    <url>%2F2019%2F03%2F21%2FSpring%E4%B9%8B%E6%B3%A8%E8%A7%A3%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[概述Spring框架的核心就是IOC,通过controller一类注解的bean的实例化过程可以大体总结spring注解的工作原理： 1）利用asm技术扫描class文件，转化成Springbean结构，把符合扫描规则的（主要是是否有相关的注解标注，例如@Component）bean注册到Spring 容器中beanFactory。 2）注册处理器，包括注解处理器 3）实例化处理器（包括注解处理器），并将其注册到容器的beanPostProcessors列表中。 4）创建bean的过程中，属性注入或者初始化bean时会调用对应的注解处理器进行处理。 例如注解@Autowired： 对于这个注解，您需要在xml中配置这个注解的处理器AutowiredAnnotationBeanPostProcessor，这个处理器会扫描容器中所有的bean对象，发现bean中拥有@Autowired注解的时候会自动去找到容器中和这个注解修饰类型匹配的bean对象，并注入到对应的地方去。 元注解 @Target：表示该注释可以用于什么地方，下面是源码： 123456789101112@Documented@Retention(RetentionPolicy.RUNTIME) // 保留到运行时@Target(ElementType.ANNOTATION_TYPE)public @interface Target &#123; /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to. */ ElementType[] value();&#125; 注解作用位置： 12345678910111213141516171819202122public enum ElementType &#123; TYPE, // 类，接口（包括注解），enum; FIELD, // 属性域 METHOD, // 方法 PARAMETER, // 参数 CONSTRUCTOR, // 构造函数 LOCAL_VARIABLE, // 局部变量 ANNOTATION_TYPE, // 注解类型 PACKAGE, // 包 /** * Type parameter declaration * @since 1.8 */ TYPE_PARAMETER, // 表明可以标注 类型参数 /** * Use of a type * @since 1.8 */ TYPE_USE // 可以注解 任何类型名称&#125; @Retention 表示需要在什么级别保存该注释信息，下面是源码： 1234567891011121314151617181920/* Indicates how long annotations with the annotated type are to * be retained. If no Retention annotation is present on * an annotation type declaration, the retention policy defaults to * &#123;@code RetentionPolicy.CLASS&#125;. * * A Retention meta-annotation has effect only if the * meta-annotated type is used directly for annotation. It has no * effect if the meta-annotated type is used as a member type in * another annotation type.*/@Documented // 表明 注解会被包含在Java API文档中。@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention &#123; /** * Returns the retention policy. * @return the retention policy */ RetentionPolicy value();&#125; 123456789101112131415161718192021public enum RetentionPolicy &#123; /** * Annotations are to be discarded by the compiler. */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * * @see java.lang.reflect.AnnotatedElement */ RUNTIME&#125; RetentionPolicy.SOURCE 保留在源码级别，被编译器抛弃，如@Override；RetentionPolicy.CLASS 被编译器保留在编译后的class文件，但是被VM抛弃；RetentionPolicy.RUNTIME 保留至运行时，可以被反射读取。如 @Retention 元注解本身。 @Documented 将注释包含在JavaDoc中 @Inheried 允许子类继承父类中的注解 Java如何识别注解关键词：Java反射 java.lang.reflect 包，实现反射功能的工具类 注解处理类库：java.lang.reflect.AnnotatedElement 程序通过反射获取了某个类的AnnotatedElement对象之后, 程序就可以调用该对象如下的方法来访问Annotation的信息： Annotation[] getAnnotations() 12345678910111213/** * Returns annotations that are &lt;em&gt;present&lt;/em&gt; on this element. * * If there are no annotations &lt;em&gt;present&lt;/em&gt; on this element, the return * value is an array of length 0. * * The caller of this method is free to modify the returned array; it will * have no effect on the arrays returned to other callers. * * @return annotations present on this element * @since 1.5 */ Annotation[] getAnnotations(); default boolean isAnnotationPresent(Class&lt;? extends Annotation&gt; annotationClass) 123456789101112131415161718192021/** * Returns true if an annotation for the specified type * is &lt;em&gt;present&lt;/em&gt; on this element, else false. This method * is designed primarily for convenient access to marker annotations. * * &lt;p&gt;The truth value returned by this method is equivalent to: * &#123;@code getAnnotation(annotationClass) != null&#125; * * &lt;p&gt;The body of the default method is specified to be the code * above. * * @param annotationClass the Class object corresponding to the * annotation type * @return true if an annotation for the specified annotation * type is present on this element, else false * @throws NullPointerException if the given annotation class is null * @since 1.5 */ default boolean isAnnotationPresent(Class&lt;? extends Annotation&gt; annotationClass) &#123; return getAnnotation(annotationClass) != null; &#125; default T getDeclaredAnnotation(Class annotationClass) 1234567891011121314151617181920212223242526272829303132/** * Returns this element's annotation for the specified type if * such an annotation is &lt;em&gt;directly present&lt;/em&gt;, else null. * * This method ignores inherited annotations. (Returns null if no * annotations are directly present on this element.) * * @implSpec The default implementation first performs a null check * and then loops over the results of &#123;@link * #getDeclaredAnnotations&#125; returning the first annotation whose * annotation type matches the argument type. * * @param &lt;T&gt; the type of the annotation to query for and return if directly present * @param annotationClass the Class object corresponding to the * annotation type * @return this element's annotation for the specified annotation type if * directly present on this element, else null * @throws NullPointerException if the given annotation class is null * @since 1.8 */ default &lt;T extends Annotation&gt; T getDeclaredAnnotation(Class&lt;T&gt; annotationClass) &#123; Objects.requireNonNull(annotationClass); // Loop over all directly-present annotations looking for a matching one for (Annotation annotation : getDeclaredAnnotations()) &#123; if (annotationClass.equals(annotation.annotationType())) &#123; // More robust to do a dynamic cast at runtime instead // of compile-time only. return annotationClass.cast(annotation); &#125; &#125; return null; &#125; 为了处理注解，注解处理器做3件事情： 读取配置文件中管理的bean 实例化bean 注解处理器获取实例bean中的注解并操作 123456789101112131415161718192021222324252627// 自定义的注解处理器public class ClassPathXMLApplicationContext &#123; public ClassPathXMLApplicationContext(String configFileName) &#123; // 读取配置文件中管理的bean readXMLConfigFile(configFileName); // 实例化bean instanceBean(); // 向容器注册bean registerAnnotationBean(); &#125; // 读取配置文件中的bean private void readXMLConfigFile() &#123; &#125; // 实例化bean private void instanceBean() &#123; &#125; // 向容器注册bean private void registerAnnotationBean() &#123; &#125;&#125; Spring是如何实现注解的扫描注册的管理注解bean定义的两个容器： AnnotationConfigApplicationContext AnnotationConfigWebApplicationContext AnnotationConfigApplicationContext的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package org.springframework.context.annotation;public class AnnotationConfigApplicationContext extends GenericApplicationContext implements AnnotationConfigRegistry &#123; // 读取器：读取注解的Bean，并注册到容器中。 private final AnnotatedBeanDefinitionReader reader; // 扫描器：扫描类路径中注解的Bean，并注册到容器中。 private final ClassPathBeanDefinitionScanner scanner; public AnnotationConfigApplicationContext() &#123; // reader和scanner，功能类似，使用场景不同：annotatedClasses, basePakages. this.reader = new AnnotatedBeanDefinitionReader(this); this.scanner = new ClassPathBeanDefinitionScanner(this); &#125; public AnnotationConfigApplicationContext(DefaultListableBeanFactory beanFactory) &#123; super(beanFactory); this.reader = new AnnotatedBeanDefinitionReader(this); this.scanner = new ClassPathBeanDefinitionScanner(this); &#125;/********************************* * 1 区 ***/ public AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) &#123; this(); register(annotatedClasses); refresh(); &#125; public AnnotationConfigApplicationContext(String... basePackages) &#123; this(); scan(basePackages); refresh(); &#125;/*********************************/ @Override public void setEnvironment(ConfigurableEnvironment environment) &#123; super.setEnvironment(environment); this.reader.setEnvironment(environment); this.scanner.setEnvironment(environment); &#125; public void setBeanNameGenerator(BeanNameGenerator beanNameGenerator) &#123; this.reader.setBeanNameGenerator(beanNameGenerator); this.scanner.setBeanNameGenerator(beanNameGenerator); getBeanFactory().registerSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, beanNameGenerator); &#125; public void setScopeMetadataResolver(ScopeMetadataResolver scopeMetadataResolver) &#123; this.reader.setScopeMetadataResolver(scopeMetadataResolver); this.scanner.setScopeMetadataResolver(scopeMetadataResolver); &#125;/*************************** * 2 区 ***/ public void register(Class&lt;?&gt;... annotatedClasses) &#123; Assert.notEmpty(annotatedClasses, "At least one annotated class must be specified"); this.reader.register(annotatedClasses); &#125; public void scan(String... basePackages) &#123; Assert.notEmpty(basePackages, "At least one base package must be specified"); this.scanner.scan(basePackages); &#125;/***************************/ @Override protected void prepareRefresh() &#123; this.scanner.clearCache(); super.prepareRefresh(); &#125;&#125; AnnotationCnofigApplicationContext类的源码中的 “1 区”：AnnotationConfigApplicationContext的基本功能在构造函数中完成 实例化reader和scanner，然后调用 register(Class&lt;?&gt;… annotatedClasses)或scan(String… basePackages)方法，最后刷新。 register(Class&lt;?&gt;… annotatedClasses)方法调用reader.register(Class&lt;?&gt;… annotatedClasses) scan(String… basePackages)方法调用scanner.scan(String… basePackages) AnnotatedBeanDefinitionReader 和 ClassPathBeanDefinitionScanner 功能类似，二者取其一。 AnnotationConfigApplicationContext 类的源码中的 “2 区”：如下两个方法完成了Spring的扫描注册功能： reader.register(Class&lt;?&gt;… annotatedClasses) scanner.scan(String… basePackages) reader.register(Class&lt;?&gt;… annotatedClasses) 的源码： 12345678910111213141516171819202122232425262728293031323334353637383940public void register(Class&lt;?&gt;... annotatedClasses) &#123; for (Class&lt;?&gt; annotatedClass : annotatedClasses) &#123; registerBean(annotatedClass); &#125; &#125;public void registerBean(Class&lt;?&gt; annotatedClass) &#123; registerBean(annotatedClass, null, (Class&lt;? extends Annotation&gt;[]) null); &#125;// 注解功能实现区@SuppressWarnings("unchecked")public void registerBean(Class&lt;?&gt; annotatedClass, String name, Class&lt;? extends Annotation&gt;... qualifiers) &#123; AnnotatedGenericBeanDefinition abd = new AnnotatedGenericBeanDefinition(annotatedClass); if (this.conditionEvaluator.shouldSkip(abd.getMetadata())) &#123; return; &#125; ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(abd); abd.setScope(scopeMetadata.getScopeName()); String beanName = (name != null ? name : this.beanNameGenerator.generateBeanName(abd, this.registry)); AnnotationConfigUtils.processCommonDefinitionAnnotations(abd); if (qualifiers != null) &#123; for (Class&lt;? extends Annotation&gt; qualifier : qualifiers) &#123; if (Primary.class == qualifier) &#123; abd.setPrimary(true); &#125; else if (Lazy.class == qualifier) &#123; abd.setLazyInit(true); &#125; else &#123; abd.addQualifier(new AutowireCandidateQualifier(qualifier)); &#125; &#125; &#125; BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(abd, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, this.registry);&#125; scanner.scan(String… basePackages) 源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public int scan(String... basePackages) &#123; int beanCountAtScanStart = this.registry.getBeanDefinitionCount(); doScan(basePackages); // 备注：重点关注此处 // Register annotation config processors, if necessary. if (this.includeAnnotationConfig) &#123; AnnotationConfigUtils.registerAnnotationConfigProcessors(this.registry); &#125; return (this.registry.getBeanDefinitionCount() - beanCountAtScanStart); &#125;// 注解功能实现区protected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Assert.notEmpty(basePackages, "At least one base package must be specified"); /** * BeanDefinitionHoder: Holder for a BeanDefinition with name and aliases. * BeanDefinition: A BeanDefinition describes a bean instance, * which has property values, constructor argument values, and * further information supplied by concrete implementations. */ Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;BeanDefinitionHolder&gt;(); for (String basePackage : basePackages) &#123; // 多个包路径 Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage); for (BeanDefinition candidate : candidates) &#123; ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate); candidate.setScope(scopeMetadata.getScopeName()); String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry); if (candidate instanceof AbstractBeanDefinition) &#123; postProcessBeanDefinition((AbstractBeanDefinition) candidate, beanName); &#125; if (candidate instanceof AnnotatedBeanDefinition) &#123; AnnotationConfigUtils.processCommonDefinitionAnnotations((AnnotatedBeanDefinition) candidate); &#125; if (checkCandidate(beanName, candidate)) &#123; BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); beanDefinitions.add(definitionHolder); registerBeanDefinition(definitionHolder, this.registry); &#125; &#125; &#125; return beanDefinitions;&#125; 总结]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring之声明式事务管理]]></title>
    <url>%2F2019%2F03%2F19%2FSpring%E4%B9%8B%E5%A3%B0%E6%98%8E%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Spring声明式事务异常回滚机制前言Spring事务管理是根据异常来进行回滚操作； Spring与Mybatis整合时，虽然在Service方法中并没有checked异常，但是如果数据库有异常发生，默认会进行事务回滚。 Spring如果不添加rollbackFor等属性，Spring碰到Unchecked Exceptions都会回滚，不仅仅是RuntimeException，也包括Error。 如果在事务方法中捕获异常并进行处理，一定要继续抛出异常并在Spring事务管理中进行rollbak-for配置。 概述首先看一个Service层的方法，该方法调用两个DAO层的方法分别往数据库中的两张表中插入数据，实现一个业务逻辑。而且根据业务规则，这两个插入动作是要进行事务控制的原子操作，即要么一起成功，要么一起失败，否则会导致数据一致性问题。 123456@Overridepublic void SaveDispatchInfoService(GxluProjectInfo projInfo, GxluDispatchResultSchedule dispatchResultSchedule) throws Exception&#123; projectInfoDAO.updateOrderInfoByOrderId(projInfo); //此句执行完后，事务已经被提交了 gxluDispatchResultScheduleDAO.saveInstance(dispatchResultSchedule);//若此句执行发生异常，已无法将上一句执行的结果进行回滚，产生数据一致性问题&#125; 说明：若在这个Service层的方法上不加任何事务注解，则事务的边界为DAO层的方法。即当程序执行完第一个DAO方法的调用后，事务已经被提交了。（可以通过pl/sql developer工具连接到数据库后，发现此时记录已经被插入到表中了来验证）。因此，第二句的DAO层的方法如果一旦执行错误，已经无法将上一句DAO层方法以及插入到数据库中的数据进行回滚，导致数据一致性问题。 Spring通过@Transactional注解实现声明式事务。一般事务的边界定义在Service层的方法中。 1234567@Override@Transactionalpublic void SaveDispatchInfoService(GxluProjectInfo projInfo, GxluDispatchResultSchedule dispatchResultSchedule) throws Exception&#123; projectInfoDAO.updateOrderInfoByOrderId(projInfo);//此句执行完后，事务没有被提交。 gxluDispatchResultScheduleDAO.saveInstance(dispatchResultSchedule);//此句若执行失败，若抛出的异常为RuntimeExcepition类型或其子型的异常对象，则上句DAO层语句执行的结果会被回滚。否则，上句执行的结果会被提交。这样也会导致业务原子信被破坏，导致数据不一致的情况。&#125;//当程序执完整个Service方法，且上述两句都执行成功，则事务才被提交 说明：若如上述代码一样，只加了一个默认的@Transactional标注，而不加任何标注参数。则此时，当第一个DAO层方法执行完成后，事务没有被提交（可以通过pl/sql developer工具连接到数据库后，发现此时记录没有被插入到表中来验证）。当程序执完整个Service方法，且上述两句DAO层方法都执行成功，则事务才被提交（可以通过pl/sql developer工具连接到数据库后，发现此时两条记录都已经被插入到表中来验证）。 若第二句DAO层方法执行失败，Spring会根据异常类型来做不同的处理，即：若执行失败，若抛出的异常为RuntimeExcepition类型或其子型的异常对象，则上句DAO层语句执行的结果会被回滚。否则，上句执行的结果会被提交。这样也会导致业务原子性被破坏，导致数据不一致的情况。 为了保证在Service层的方法中，无论遇到任何类型的异常，都让事务进行回滚，而不是让之前已经执行成功的语句提交，则必须在@Transactional注解中增加如下参数rollbackFor： 1234567@Override@Transactional(propagation=Propagation.REQUIRED, rollbackFor=Exception.class)public void SaveDispatchInfoService(GxluProjectInfo projInfo, GxluDispatchResultSchedule dispatchResultSchedule) throws Exception&#123; projectInfoDAO.updateOrderInfoByOrderId(projInfo); gxluDispatchResultScheduleDAO.saveInstance(dispatchResultSchedule);&#125; 说明：由于Exception是所有异常类的超类，这样对于任何异常，Spring都会进行回滚，而不会将引发异常之前已经执行成功的提交掉。 对于那些我们在代码中主动抛出的异常，我们可以将rollbackFor参数指定为这些特定异常，即遇到该异常，就进行回滚操作。 1234567891011121314151617181920@Override@Transactional(propagation=Propagation.REQUIRED, rollbackFor=ArcpipeException.class)public void insertRouteConditions(List&lt;GxluProjectInfo&gt; projInfos) throws ArcpipeException,JAXBException,RemoteException,Exception&#123; //插入路由选择条件（起始、终止）到数据库 for (GxluProjectInfo projInfo : projInfos) &#123; projectInfoDAO.insertRouteCondition(projInfo); &#125; //将选择条件推送给综资Webservice Body bodyOut=convertBean2dto(projInfos); String configConditionXml = JaxbUtil.convertToXml(bodyOut); String configConditionResultXml=configConditionItfService.submitConfigCondition2Purdo(configConditionXml); Body bodyIn = JaxbUtil.converyToJavaBean(configConditionResultXml, Body.class); if(!"1".equals(bodyIn.getConfigConditionResultInfo().getErrorCode())) &#123; throw new ArcpipeException(); //若抛出该异常，则整个事务将被回滚 &#125;&#125; 说明：上述Service方法作为一个原子业务逻辑，实现了两个操作，一个是调用DAO层方法往数据库里面插入数据，另一个是调用另一个系统的Webservice接口，将数据推送过去。若第一个DAO层方法执行成功，而第二个操作由于网络等原因导致调用Webservice接口失败，则程序会主动抛出ArcpipeException异常，而由于事务注解中已经设置了参数rollbackFor=ArcpipeException.class，因此该事务会被回滚掉。 或者我们在自定义异常的时候，让自定义的异常继承自RuntimeException，这样抛出的时候才会被Spring默认的事务处理。]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql内连接、外连接、全连接]]></title>
    <url>%2F2019%2F03%2F18%2FMysql%E5%86%85%E8%BF%9E%E6%8E%A5%E3%80%81%E5%A4%96%E8%BF%9E%E6%8E%A5%E3%80%81%E5%85%A8%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[前言用两个表（a_table、b_table），关联字段a_table.a_id和b_table.b_id来演示一下MySQL的内连接、外连接（ 左(外)连接、右(外)连接、全(外)连接）。 MySQL版本：Server version: 5.6.31 MySQL Community Server (GPL) 数据库表：a_table、b_table 主题：内连接、左连接（左外连接）、右连接（右外连接）、全连接（全外连接）、交叉连接 前提建表语句：12345CREATE TABLE `a_table` ( `a_id` int(11) DEFAULT NULL, `a_name` varchar(10) DEFAULT NULL, `a_part` varchar(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 12345CREATE TABLE `b_table` ( `b_id` int(11) DEFAULT NULL, `b_name` varchar(10) DEFAULT NULL, `b_part` varchar(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 表测试数据： 内连接(等值连接)关键字：inner join on 语句： 1select * from a_table a inner join b_table bon a.a_id = b.b_id 执行结果： 说明：组合两个表中的记录，返回关联字段相符的记录，也就是返回两个表的交集（阴影）部分。 左连接关键字：left join on / left outer join on 语句： 1select * from a_table a left join b_table bon a.a_id = b.b_id 执行结果： 说明： left join 是left outer join的简写，它的全称是左外连接，是外连接中的一种。 左(外)连接，左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录，并且右表记录不足的地方均为NULL。 右连接关键字：right join on / right outer join on 语句： 1select * from a_table a right outer join b_table b on a.a_id = b.b_id 执行结果： 说明： right join是right outer join的简写，它的全称是右外连接，是外连接中的一种。 与左(外)连接相反，右(外)连接，左表(a_table)只会显示符合搜索条件的记录，而右表(b_table)的记录将会全部表示出来，并且左表记录不足的地方均为NULL。 全连接MySql不支持全连接查询，可以用union代替. 为做全连接演示，所以在Oracle新建两张： 表message1： 表message2： 关键字：FULL JOIN 语句： 123select * from message1 aFULL JOIN message2 bon a.id=b.id 查询结果如图所示： 全连接的图片效果： 总结：Full outer join展示的是a表和b表的全部信息(a和b的并集)。但需要注意的是，对于没有匹配的记录(即a.id和b.id没有一一对应的)，则会以null做为值。可以使用IF NULL判断。 交叉连接(笛卡尔积) 不带where条件子句，它返回的是被连接的两个表的笛卡尔积，返回结果的行数等于两个表行数的乘积。 例如上表中：message1表的3条数据*message2表的4条数据，最后得到12条数据，就是下面笛卡尔积查询出来的总记录数。 查询语句： 12select * from message1 across JOIN message2 b 查询结果如图所示： 如果带where，返回或显示的是匹配条件成立的行数。 查询语句： 1234select * from message1 across JOIN message2 bwherea.id=b.id 查询结果如图所示： 总结：1.CROSS JOIN(笛卡尔积)是两个表每一个字段相互匹配，得出的结果就是笛卡尔积。笛卡尔积也等同于交叉连接。 2.CROSS JOIN(笛卡尔积)带条件查询， 查询结果跟等值连接(也叫内连接)、where连接的查询结果是一样，并且后面加条件只能用where，不能用on。 部分JOIN连接简单优化部分指的是：inner join、left join和right join 优化方法： a.找出驱动表和被驱动表，在被驱动表上建立索引，可提高连接性能。b.内连接inner join和左连接left join差不多,都需要优化右表。而右连接right join需要优化左表。比较：左连接和内连接优于右连接，左连接和内连接的比较取决于需求，单纯看性能是差不多的。 备注：索引一般为（主键、唯一索引、前缀索引等）左连接left join中，左表为驱动表，右表为被驱动表；右连接right join中，右表为驱动表，左表为被驱动表。 参考： https://blog.csdn.net/plg17/article/details/78758593 https://blog.csdn.net/qq_39629277/article/details/82882004]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[error、exception、throwable]]></title>
    <url>%2F2019%2F03%2F09%2Ferror%E3%80%81exception%E3%80%81throwable%2F</url>
    <content type="text"><![CDATA[前言在一个方法的运行过程中，如果发生了异常，则这个方法（或者是Java虚拟机）生成一个代表该异常的对象（它包含了异常的详细信息），并把它交给运行时系统，运行时系统寻找相应的代码来处理这一异常。我们把生成异常对象并把它提交给运行时系统的过程称为抛出(throw)异常。 当Java运行时系统得到一个异常对象时，它将会沿着方法的调用栈逐层回溯，寻找处理这一异常的代码。找到能够处理这类异常的方法后，运行时系统把当前异常对象交给这个方法进行处理，这一过程称为捕获（catch）异常。 错误和异常Throwable类是 Java 语言中所有错误或异常的超类。 Throwable有两个子类：Error和Exception Exception（异常）：是应用程序中可能的可预测、可恢复问题。一般大多数异常表示中度到轻度的问题。异常一般是在特定环境下产生的，通常出现在代码的特定方法和操作中。在 EchoInput 类中，当试图调用 readLine方法时，可能出现 IOException 异常。 Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、ArithmeticException）和 ArrayIndexOutOfBoundException。 Error（错误）：表示运行应用程序中较严重问题，不可恢复。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。 可控制异常和不可控制异常的划分Java中的异常分为两大类： Checked Exception（非Runtime Exception） Java认为Checked异常都是可以被处理的异常，所以Java程序必须显式的处理Checked异常，如果程序没有处理checked异常，程序在编译时候将发生错误。我们比较熟悉的Checked异常有： java.lang.ClassNotFoundException java.lang.NoSuchMetodException java.io.IOException Unchecked Exception（Runtime Exception） Runtime Exception如除数是0、数组下标越界等，其产生频繁，处理麻烦，若显示申明或者捕获将会对程序的可读性和运行效率影响很大。所以由系统自动检测并将它们交给缺省的异常处理程序。当然如果你有处理要求也可以显示捕获它们。我们比较熟悉的RumtimeException类的子类有： java.lang.ArithmeticException java.lang.ArrayStoreExcetpion java.lang.ClassCastException java.lang.IndexOutOfBoundsException java.lang.NullPointerException java.lang.Error：Throwable的子类，用于标记严重错误。合理的应用程序不应该去try/catch这种错误。绝大多数的错误都是非正常的，就根本不该出现的。 java.lang.Exception：Throwable的子类，用于指示一种合理的程序想去catch的条件。即它仅仅是一种程序运行条件，而非严重错误，并且鼓励用户程序去catch它。 Error和RuntimeException 及其子类都是不可控制的异常（unchecked exceptions），而所有其他的Exception类都是可控制的异常（checked exceptions）。 Checked Exceptions：通常是从一个可以恢复的程序中抛出来的，并且最好能够从这种异常中使用程序恢复。比如FileNotFoundException, ParseException等。可控制的异常发生在编译阶段，必须要使用try…catch（或者throws）进行处理，否则编译不通过。 Unchecked Exceptions：通常是如果一切正常的话本不该发生的异常，但是的确发生了。发生在运行期，具有不确定性，主要是由于程序的逻辑问题所引起的。比如ArrayIndexOutOfBoundException, ClassCastException等。从语言本身的角度讲，程序不该去catch这类异常。虽然能够从诸如RuntimeException这样的异常中catch并恢复，但是并不鼓励终端程序员这么做。 因为这类错误本身就是bug，应该被修复，出现此类错误时程序就应该立即停止执行。 因此，面对Errors和Unchecked Exceptions应该让程序自动终止执行，程序员不该做诸如try/catch这样的事情，而是应该查明原因，修改代码逻辑。 处理方法Java中凡是继承自Exception，而不继承自RuntimeException类的异 常都是非运行时异常。 对于非运行时异常（Checked Exception），必须要对其进行处理，否则无法通过编译。处理方式有两种： 使用try..catch..finally进行捕获； 在产生异常的方法声明后面写上throws 某一个Exception类型，如throws Exception，将异常抛出到外面一层去。 对于运行时异常（Runtime exception），可以对其进行处理，也可以不处理。推荐不对运行时异常进行处理。 一般而言，Checked Exception 表示这个Exception 必须要被处理，也就是说程序设计者应该已经知道可能会收到某个Exception(因为要try catch住) ，所以程序设计者应该能针对这些不同的Checked Exception 做出不同的处理。 而Runtime Exception 通常会暗示着程序上的错误，这种错误会导致程序设计者无法处理，而造成程序无法继续执行下去。]]></content>
      <categories>
        <category>java面试准备</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈进程和线程]]></title>
    <url>%2F2019%2F03%2F06%2F%E6%B5%85%E8%B0%88%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[浅谈进程和线程定义 进程定义：一般是对于系统来讲，进程（process）是具有一定独立功能的程序，操作系统利用进程把工作划分为一些功能单元。进程是进行资源分配和调度的一个独立单位。它还拥有一个私有的虚拟地址空间，该空间仅能被它所包含的线程访问。 线程定义：线程（thread）是进程中所包含的一个或多个执行单元。它只能归属于一个进程并且只能访问该进程所拥有的资源。线程是CPU调度和分派的基本单位，没有单独地址空间，有独立的栈，局部变量，寄存器，程序计数器等。 关系 一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行。 线程属于进程，不能独立执行;每个进程至少要有一个线程，成为主线程。 一个进程无法直接访问另一个进程的资源;同一进程内的多个线程共享进程的资源。 区别进程和线程的主要差别在于它们是不同的操作系统资源管理方式。 进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 线程作为调度和分配的基本单位，进程作为拥有资源的基本单位。 不仅进程之间可以并发执行，同一个进程的多个线程之间也可并发执行。 线程的划分尺度小于进程，使得多线程程序的并发性高。 进程是拥有资源的一个独立单位，线程不拥有系统资源，但可以访问隶属于进程的资源。 在创建或撤消进程时，由于系统都要为之分配和回收资源，导致系统的开销明显大于创建或撤消线程时的开销。 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。 补充：粗略谈下synchronized关键字synchronized 关键字，代表这个方法加锁,相当于不管哪一个线程（例如线程A），运行到这个方法时,都要检查有没有其它线程B（或者C、 D等）正在用这个方法(或者该类的其他同步方法)，有的话要等正在使用synchronized方法的线程B（或者C 、D）运行完这个方法后再运行此线程A，没有的话,锁定调用者，然后直接运行。 它包括两种用法：synchronized 方法和 synchronized 块。 Java语言的关键字，可用来给对象和方法或者代码块加锁。当它锁定一个方法或者一个代码块的时候，同一时刻最多只有一个线程执行这段代码。当两个并发线程访问同一个对象object中的这个加锁同步代码块时，一个时间内只能有一个线程得到执行。另一个线程必须等待当前线程执行完这个代码块以后才能执行该代码块。然而，当一个线程访问object的一个加锁代码块时，另一个线程仍可以访问该object中的非加锁代码块。 几种进程间的通信方式 管道（pipe）：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有血缘关系的进程间使用。进程的血缘关系通常指父子进程关系。 有名管道（named pipe）：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间通信。 信号量（semophore）：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它通常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列（message queue）：消息队列是由消息组成的链表，存放在内核中，并由消息队列标识符标识。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号（signal）：信号是一种比较复杂的通信方式，用于通知接收进程某一事件已经发生。 共享内存（shared memory）：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的IPC方式，它是针对其他进程间的通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。 套接字（socket）：套接口也是一种进程间的通信机制，与其他通信机制不同的是它可以用于不同及其间的进程通信。 几种线程间的通信机制1、锁机制 互斥锁：提供了以排它方式阻止数据结构被并发修改的方法。 读写锁：允许多个线程同时读共享数据，而对写操作互斥。 条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 2、信号量机制：包括无名线程信号量与有名线程信号量 3、信号机制：类似于进程间的信号处理。 线程间通信的主要目的是用于线程同步，所以线程没有像进程通信中用于数据交换的通信机制。 线程的调度 sleep()：在睡眠的时候，会释放cpu让给其他线程执行。即使没有其他线程抢占cpu，也需要等待睡眠时间到了以后才能真正的指定。 yield()：执行的时候会让出cpu , 但是会立马同其他的线程抢占cpu。注意：yield()只会让位给优先级一样或者比它优先级高的线程，而且不能由用户指定暂停多长时间。 当线程执行了sleep方法之后，线程将转入到睡眠状态，直到时间结束，而执行yield方法，直接转入到就绪状态。 join()：在哪个线程被调用，就插入到哪个线程前面。]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java锁的种类和区别]]></title>
    <url>%2F2019%2F03%2F04%2Fjava%E9%94%81%E7%9A%84%E7%A7%8D%E7%B1%BB%E5%92%8C%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[java中锁有很多种，本文将主要介绍各种锁的分类和区别，主要如下： 公平锁/非公平锁 可重入锁 独享锁/共享锁 互斥锁/读写锁 乐观锁/悲观锁 分段锁 偏向锁/轻量级锁/重量级锁 自旋锁 上面是很多锁的名词，这些分类并不是全是指锁的状态，有的指锁的特性，有的指锁的设计。 下面总结的内容是对每个锁的名词进行一定的解释。 公平锁/非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁。非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。 非公平锁的优点在于吞吐量比公平锁大。对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以并没有任何办法使其变成公平锁。 可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。对于Java ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是Re entrant Lock重新进入锁。对于Synchronized而言,也是一个可重入锁，可重入锁的一个好处是可一定程度避免死锁。 1234567synchronized void setA() throws Exception&#123; Thread.sleep(1000); setB();&#125;synchronized void setB() throws Exception&#123; Thread.sleep(1000);&#125; 上面的代码就是一个可重入锁的一个特点。 如果不是可重入锁的话，当前线程就不会获得setB方法的锁，因此setB可能不会被当前线程执行，可能造成死锁。 独享锁/共享锁独享锁是指该锁一次只能被一个线程所持有。共享锁是指该锁可被多个线程所持有。 对于Java ReentrantLock而言，其是独享锁。 对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。对于Synchronized而言，当然是独享锁。 互斥锁/读写锁注意：上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。互斥锁在Java中的具体实现就是ReentrantLock读写锁在Java中的具体实现就是ReadWriteLock 乐观锁/悲观锁乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。 悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。 悲观锁适合写操作非常多的场景 乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升 悲观锁在Java中的使用，就是利用各种锁。 乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。 分段锁分段锁其实是一种锁的设计，并不是具体的一种锁。 对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表，同时又是一个ReentrantLock。（Segment继承了ReentrantLock)当需要put元素的时候，并不是对整个HashMap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。在统计size的时候，就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 偏向锁/轻量级锁/重量级锁这三种锁是指锁的状态，并且是针对Synchronized。 在Java 5通过引入锁升级的机制来实现高效Synchronized。 这三种锁的状态是通过对象监视器在对象头中的字段来表明的。偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 自旋锁在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁。这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。 参考：https://blog.csdn.net/qq_35181209/article/details/77652278]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAS算法]]></title>
    <url>%2F2019%2F03%2F04%2FCAS%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[什么是CAS？Compare And Set（或Compare And Swap），CAS是解决多线程并行情况下使用锁造成性能损耗的一种机制，采用这种无锁的原子操作可以实现线程安全，避免加锁的笨重性。 JDK5增加了并发包java.util.concurrent.*，其下面的类使用CAS算法实现了区别于synchronized同步锁的一种乐观锁。 JDK5之前Java语言是靠synchronized关键字保证同步的，这是一种独占锁，也是是悲观锁。 CAS 利用 CPU指令 保证了操作的原子性，以达到锁的效果，循环这个指令，直到成功为止。java提供的CAS原子操作类AtomicInteger等，核心就是CAS（CompareAndSwap）。 注意：原子操作和锁是一样的一种可以保证线程安全的方式，如何让线程安全就看如何使用锁或者如何使用原子操作。CAS使用了正确的原子操作，所以保证了线程安全。 CAS算法理解对CAS的理解，CAS是一种无锁算法。 CAS有3个操作数： 内存值V 旧的预期值A 要修改的新值B 当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。 CAS比较与交换的伪代码可以表示为： do{ ​ 备份旧数据； ​ 基于旧数据构造新数据； }while(!CAS( 内存地址，备份的旧数据，新数据 )) 场景理解 注：t1，t2线程是同时更新同一变量56的值 因为t1和t2线程都同时去访问同一变量56，所以他们会把主内存的值完全拷贝一份到自己的工作内存空间，所以t1和t2线程的预期值都为56。 假设t1在与t2线程竞争中线程t1能去更新变量的值，而其他线程都失败。（失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次发起尝试）。t1线程去更新变量值改为57，然后写到内存中。此时对于t2来说，内存值变为了57，与预期值56不一致，就操作失败了（想改的值不再是原来的值）。 （上图通俗的解释是：CPU去更新一个值，但如果想改的值不再是原来的值，操作就失败，因为很明显，有其它操作先改变了这个值。） 当两者进行比较时，如果相等，则证明共享数据没有被修改，替换成新值，然后继续往下运行；如果不相等，说明共享数据已经被修改，放弃已经所做的操作，然后重新执行刚才的操作。 容易看出 CAS 操作是基于共享数据不会被修改的假设，采用了类似于数据库的commit-retry 的模式。当同步冲突出现的机会很少时，这种假设能带来较大的性能提升。 原子操作类当高并发的情况下，对于基本数据类型或者引用数据类型的操作，可能会产生线程安全问题，为了避免多线程问题的处理方式一般有加锁，但是加锁会影响性能，所以这个时候可以考虑使用原子操作类。CAS由于是在硬件方面保证的原子性，不会锁住当前线程，所以执行效率是很高的。常见的原子操作类： CAS算法在JDK中的应用在原子类变量中，如java.util.concurrent.atomic中的AtomicXXX，都使用了这些底层的JVM支持为数字类型的引用类型提供一种高效的CAS操作，而在java.util.concurrent中的大多数类在实现时都直接或间接的使用了这些原子变量类。 Java 1.7中AtomicInteger.incrementAndGet()的实现源码为： 这段代码是一个无限循环，也就是CAS的自旋，循环体中做了三件事： 获取当前值 当前值+1，计算出目标值 进行CAS操作，如果成功则跳出循环，如果失败则重复上述步骤 compareAndSet方法的实现很简单，只有一行代码。 这里涉及到两个重要的对象，一个是unsafe，一个是valueOffset。 什么是unsafe呢？Java语言不像C，C++那样可以直接访问底层操作系统，但是JVM为我们提供了一个后门，这个后门就是unsafe。unsafe为我们提供了硬件级别的原子操作。 至于valueOffset对象，是通过unsafe.objectFiledOffset方法得到，所代表的是AtomicInteger对象value成员变量在内存中的偏移量。我们可以简单的把valueOffset理解为value变量的内存地址。 我们上面说过，CAS机制中使用了3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。 而unsafe的compareAndSwapInt方法的参数包括了这三个基本元素：valueOffset参数代表了V，expect参数代表了A，update参数代表了B。 正是unsafe的compareAndSwapInt方法保证了Compare和Swap操作之间的原子性操作。 注意：其中current为从内存中拷贝的旧的预期值，next为想要修改的新值。 由此可见，AtomicInteger.incrementAndGet的实现用了乐观锁技术，调用了类sun.misc.Unsafe库里面的 CAS算法，用CPU指令来实现无锁自增。 所以，AtomicInteger.incrementAndGet的自增比用synchronized的锁效率倍增。 CAS的缺点CAS虽然很高效的解决了原子操作问题，但是CAS仍然存在三大问题： 循环时间长开销很大 只能保证一个共享变量的原子操作。 ABA问题。 循环时间长开销很大：如果并发量很高，我们可以看到getAndAddInt方法(JDK1.8)执行时，如果CAS失败，会一直进行尝试。如果CAS长时间一直不成功，可能会给CPU带来很大的开销。 自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 只能保证一个共享变量的原子操作： CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作。 所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作，以此保证原子性。 什么是ABA问题？ABA问题怎么解决？如果内存地址V初次读取的值是A，并且在准备赋值的时候检查到它的值仍然为A，那我们就能说它的值没有被其他线程改变过了吗？ 如果在这段期间它的值曾经被改成了B，后来又被改回为A，那CAS操作就会误认为它从来没有被改变过。这个漏洞称为CAS操作的“ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的compareAndSet方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 Java并发包为了解决这个问题，提供了一个带有标记的原子引用类AtomicStampedReference，它可以通过控制变量值的版本来保证CAS的正确性。因此，在使用CAS前要考虑清楚 ABA 问题是否会影响程序并发的正确性。如果需要解决ABA问题，改用传统的互斥同步可能会比原子类更高效。 解决ABA问题：AtomicMarkableReference：内部是一个boolean类型的版本号，可以记录是否被更改过AtomicStampedReference：内部是一个int类型的版本号，可以记录被更改的次数 例如：使用AtomicStampedReference，避免ABA问题，查看内部是int类型的版本号 12345678910111213141516171819202122232425262728293031323334public class AtomicStampedReferenceDemo &#123; public static void main(String[] args) throws InterruptedException &#123; //设置初始化版本号是0 AtomicStampedReference&lt;String&gt; atomicStampedReference = new AtomicStampedReference("a1", 0); //初始的值和版本号 String reference = atomicStampedReference.getReference(); int stamp = atomicStampedReference.getStamp(); Thread thread1 = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("目前的值：" + reference + "............版本号：" + stamp + "，修改结果：" + atomicStampedReference.compareAndSet(reference, "a2", stamp, stamp + 1)); &#125; &#125;); Thread thread2 = new Thread(new Runnable() &#123; @Override public void run() &#123; String reference = atomicStampedReference.getReference(); System.out.println("目前的值：" + reference + "............版本号：" + atomicStampedReference.getStamp() + "，修改结果：" + atomicStampedReference.compareAndSet(reference, "a2", stamp, stamp + 1)); &#125; &#125;); thread1.start(); thread1.join(); thread2.start(); thread2.join(); &#125;&#125; 运行结果： 拓展-乐观锁和悲观锁乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁常见的两种实现方式1. 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2. CAS算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 面试必备之深入理解自旋锁什么是自旋锁？自旋锁（spinlock）：是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。 获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。 它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 Java如何实现自旋锁下面是个简单的例子： 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125; 简单分析：lock()方法利用的CAS，当第一个线程A获取锁的时候，能够成功获取到，不会进入while循环。如果此时线程A没有释放锁，另一个线程B又来获取锁，此时由于不满足CAS，所以就会进入while循环，不断判断是否满足CAS，直到A线程调用unlock()方法释放了该锁。 自旋锁存在的问题 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 自旋锁的优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） 可重入的自旋锁和不可重入的自旋锁的那段代码，仔细分析一下就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。而且，即使第二次能够成功获取，那么当第一次释放锁的时候，第二次获取到的锁也会被释放，而这是不合理的。 为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数。 12345678910111213141516171819202122232425public class ReentrantSpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); private int count; public void lock() &#123; Thread current = Thread.currentThread(); if (current == cas.get()) &#123; // 如果当前线程已经获取到了锁，线程数增加一，然后返回 count++; return; &#125; // 如果没获取到锁，则通过CAS自旋 while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread cur = Thread.currentThread(); if (cur == cas.get()) &#123; if (count &gt; 0) &#123;// 如果大于0，表示当前线程多次获取了该锁，释放锁通过count减一来模拟 count--; &#125; else &#123;// 如果count==0，可以将锁释放，这样就能保证获取锁的次数与释放锁的次数是一致的了。 cas.compareAndSet(cur, null); &#125; &#125; &#125;&#125; 关于自旋锁，大家可以看一下这篇文章，非常不错：《 面试必备之深入理解自旋锁》 参考： https://www.jianshu.com/p/21be831e851e https://blog.csdn.net/qq_28822933/article/details/83341633 https://blog.csdn.net/qq_32998153/article/details/79529704 https://juejin.im/post/5b4977ae5188251b146b2fc8 https://blog.csdn.net/qq_34337272/article/details/81252853]]></content>
      <categories>
        <category>java面试准备</category>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring常见面试题总结]]></title>
    <url>%2F2019%2F03%2F03%2FSpring%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Spring是什么?Spring是一个轻量级的IoC和AOP容器框架。目的是解决企业应用开发的复杂性，使用基本的JavaBean来完成以前只可能由EJB完成的事情，并提供了更多的企业应用功能，Spring的用途不仅限于服务器端的开发，从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。 Spring 的优点？（1）spring属于低侵入式设计，代码的污染极低； （2）spring的DI机制降低了业务对象替换的复杂性； （3）容器提供了AOP技术，利用它很容易实现如权限拦截，运行期监控等功能； （4）降低了组件之间的耦合性 ，实现了软件各层之间的解耦； （5）容器提供单例模式支持； （6）可以使用容器提供的众多服务，如事务管理，消息服务等； （7）容器提供了众多的辅助类，能加快应用的开发； （8）spring对于主流的应用框架提供了集成支持，如hibernate，JPA，Struts等 （9）独立于各种应用服务器 （10）Spring的高度开放性，并不强制应用完全依赖于Spring，开发者可以自由选择spring的部分或全部。 Spring的AOP理解AOP，一般称为面向方面（切面）编程，作为面向对象的一种补充，用于解剖封装好的对象内部，找出其中对多个对象产生影响的公共行为，并将其封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），切面将那些与业务无关，却被业务模块共同调用的逻辑提取并封装起来，减少了系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理。 AOP实现的关键在于AOP框架自动创建的AOP代理，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。 （1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ织入到Java字节码中，运行的时候就是增强之后的AOP对象。 （2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理： ①JDK动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK动态代理的核心是InvocationHandler接口和Proxy类。生成的代理对象的方法调用都会委托到InvocationHandler.invoke()方法，当我们调用代理类对象的方法时，这个“调用”会转送到invoke方法中，代理类对象作为proxy参数传入，参数method标识了我们具体调用的是代理类的哪个方法，args为这个方法的参数。 ②如果目标类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法，覆盖方法时可以添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 （3）静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。 Spring的IoC理解（1）IOC就是控制反转。就是对象的创建权反转交给Spring，由容器控制程序之间的依赖关系，作用是实现了程序的解耦合，而非传统实现中，由程序代码直接操控。(依赖)控制权由应用代码本身转到了外部容器，由容器根据配置文件去创建实例并管理各个实例之间的依赖关系，控制权的转移，是所谓反转，并且由容器动态的将某种依赖关系注入到组件之中。BeanFactory 是Spring IoC容器的具体实现与核心接口，提供了一个先进的配置机制，使得任何类型的对象的配置成为可能，用来包装和管理各种bean。 （2）最直观的表达就是，IOC让对象的创建不用去new了，可以由spring自动生产，这里用的就是java的反射机制，通过反射在运行时动态的去创建、调用对象。spring就是根据配置文件在运行时动态的去创建对象，并调用对象的方法的。 （3）Spring的IOC有三种注入方式 ： 第一是根据属性注入，也叫set方法注入； 第二种是根据构造方法进行注入； 第三种是根据注解进行注入。 详细的说： （4）IoC，控制反转：将对象交给容器管理，你只需要在spring配置文件总配置相应的bean，以及设置相关的属性，让spring容器生成类的实例对象以及管理对象。在spring容器启动的时候，spring会把你在配置文件中配置的bean都初始化以及装配好，然后在你需要调用的时候，就把它已经初始化好的那些bean分配给你需要调用这些bean的类。就是将对象的控制权反转给spring容器管理。 （5）DI机制（Dependency Injection，依赖注入）：可以说是IoC的其中一个内容，在容器实例化对象的时候主动的将被调用者（或者说它的依赖对象）注入给调用对象。比如对象A需要操作数据库，以前我们总是要在A中自己编写代码来获得一个Connection对象，有了 spring我们就只需要告诉spring，A中需要一个Connection，至于这个Connection怎么构造，何时构造，A不需要知道。在系统运行时，spring会在适当的时候制造一个Connection，然后像打针一样，注射到A当中，这样就完成了对各个对象之间关系的控制。 总结：IoC让相互协作的组件保持松散的耦合，而AOP编程允许你把遍布于应用各层的功能分离出来形成可重用的功能组件。BeanFactory和ApplicationContext有什么区别？BeanFactory和ApplicationContext是Spring的两大核心接口，而其中ApplicationContext是BeanFactory的子接口。它们都可以当做Spring的容器，生成Bean实例的，并管理容器中的Bean。 （1）BeanFactory：是Spring里面最底层的接口，提供了最简单的容器的功能，负责读取bean配置文档，管理bean的加载与实例化，维护bean之间的依赖关系，负责bean的生命周期，但是无法支持spring的aop功能和web应用。 （2）ApplicationContext接口作为BeanFactory的派生，因而具有BeanFactory所有的功能。而且ApplicationContext还在功能上做了扩展，以一种更面向框架的方式工作以及对上下文进行分层和实现继承，相较于BeanFactorty，ApplicationContext还提供了以下的功能： ①默认初始化所有的Singleton，也可以通过配置取消预初始化。 ②继承MessageSource，因此支持国际化。 ③资源访问，比如访问URL和文件。 ④事件机制。 ⑤同时加载多个配置文件。 ⑥以声明式方式启动并创建Spring容器。 ⑦载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层。 （3） ①BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化，这样，我们就不能发现一些存在的Spring的配置问题。如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常。②而ApplicationContext则相反，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。 ApplicationContext启动后预载入所有的单实例Bean，通过预载入单实例bean ,确保当你需要的时候，你就不用等待，因为它们已经创建好了。 ③相对于基本的BeanFactory，ApplicationContext 唯一的不足是占用内存空间。当应用程序配置Bean较多时，程序启动较慢。 （4）BeanFactory通常以编程的方式被创建，ApplicationContext还能以声明的方式创建，如使用ContextLoader。 （5）BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。 解释Spring支持的几种bean的作用域Spring容器中的bean可以分为5个范围： （1）singleton：这种bean范围是默认的，这种范围确保不管接受到多少个请求，每个容器中只有一个bean的实例，单例的模式由bean factory自身来维护。（2）prototype：原形范围与单例范围相反，为每一个bean请求提供一个实例。（3）request：在请求bean范围内会每一个来自客户端的网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收。（4）Session：与请求范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效。 （5）global-session：global-session和Portlet应用相关。当你的应用部署在Portlet容器中工作时，它包含很多portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。全局作用域与Servlet中的session作用域效果相同。 请解释Spring Bean的生命周期？ 其他地址：https://www.cnblogs.com/zrtqsk/p/3735273.html 首先说一下Servlet的生命周期：实例化，初始init，接收请求service，销毁destroy； Spring上下文中的Bean生命周期也类似，如下： （1）实例化一个Bean－－也就是我们常说的new； （2）按照Spring上下文对实例化的Bean进行配置－－也就是IOC注入； （3）如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String)方法，此处传递的就是Spring配置文件中Bean的id值； （4）如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory(setBeanFactory(BeanFactory)传递的是Spring工厂自身（可以用这个方式来获取其它Bean，只需在Spring配置文件中配置一个普通的Bean就可以）； （5）如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文（同样这个方式也可以实现步骤4的内容，但比4更好，因为ApplicationContext是BeanFactory的子接口，有更多的实现方法）； （6）如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization(Object obj, String s)方法，BeanPostProcessor经常被用作是Bean内容的更改，并且由于这个是在Bean初始化结束时调用那个的方法，也可以被应用于内存或缓存技术； （7）如果Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法。 （8）如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法、； 注：以上工作完成以后就可以应用这个Bean了，那这个Bean是一个Singleton的，所以一般情况下我们调用同一个id的Bean会是在内容地址相同的实例，当然在Spring配置文件中也可以配置非Singleton。 （9）当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用那个其实现的destroy()方法； （10）最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。 注意：我们这里描述的是应用Spring上下文Bean的生命周期，如果应用Spring的工厂也就是BeanFactory的话去掉第5步就Ok了。 Spring中bean的加载过程（1）获取配置文件资源； （2）对获取的xml资源进行一定的处理检验； （3）处理包装资源； （4）解析处理包装过后的资源； （5）加载提取bean并注册(添加到beanDefinitionMap中) Spring框架中的单例Beans是线程安全的么？Spring框架并没有对单例bean进行任何多线程的封装处理。 关于单例bean的线程安全和并发问题需要开发者自行去搞定。 但实际上，大部分的Spring bean并没有可变的状态(比如Serview类和DAO类)，所以在某种程度上说Spring的单例bean是线程安全的。如果你的bean有多种状态的话（比如 View Model 对象），就需要自行保证线程安全。 最浅显的解决办法就是将多态bean的作用域由“singleton”变更为“prototype”。 Spring如何处理线程并发问题？Spring使用ThreadLocal解决线程安全问题。 我们知道在一般情况下，只有有状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域。就是因为Spring对一些Bean(如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等)中非线程安全状态采用ThreadLocal进行处理，让它们也成为线程安全的状态，因为有状态的Bean就可以在多线程中共享了。 ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。 （1）在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。这时该变量是多个线程共享的，使用同步机制要求程序慎密地分析什么时候对变量进行读写，什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。 （2）而ThreadLocal则从另一个角度来解决多线程的并发访问。ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。 （3）概括起来说，对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式，而ThreadLocal采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 请解释Spring自动装配模式的区别？在Spring框架中共有5种自动装配： （1）no：这是Spring框架的默认设置，在该设置下自动装配是关闭的，开发者需要自行在bean定义中用标签明确的设置依赖关系。 （2）byName：该选项可以根据bean名称设置依赖关系。当向一个bean中自动装配一个属性时，容器将根据bean的名称自动在配置文件中查询一个匹配的bean。如果找到的话，就装配这个属性，如果没找到的话就报错。 （3）byType：该选项可以根据bean类型设置依赖关系。当向一个bean中自动装配一个属性时，容器将根据bean的类型自动在在配置文件中查询一个匹配的bean。如果找到的话，就装配这个属性，如果没找到的话就报错。 （4）constructor：构造器的自动装配和byType模式类似，但是仅仅适用于与有构造器相同参数的bean，如果在容器中没有找到与构造器参数类型一致的bean，那么将会抛出异常。 （5）autodetect：该模式自动探测使用构造器自动装配或者byType自动装配。首先，首先会尝试找合适的带参数的构造器，如果找到的话就是用构造器自动装配，如果在bean内部没有找到相应的构造器或者是无参构造器，容器就会自动选择byTpe的自动装配方式。 Spring 控制器的加载过程：（XML版）（1）Web容器创建； （2）上下文创建，但未初始化； （3）监听器创建，并注册到Context上； （4）上下文初始化； （5）通知到监听者，Spring配置文件/@Configuration加载； （6）Load-on-startup&gt;0的ServletConfig创建，springMVC的DispatcherServlet此时创建。 PS：Spring容器时SpringMVC的父容器。Spring的AOP在Spring的上下文创建时就会创建；如果想要代理SpringMVC的控制层，需要将配置写到SpringMVC的配置文件下。 Spring 框架中都用到了哪些设计模式？（1）代理模式—在AOP和remoting中被用的比较多。 （2）单例模式—在spring配置文件中定义的bean默认为单例模式。 （3）工厂模式—BeanFactory用来创建对象的实例。 （4）模板方法—用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。 （5）前端控制器—Spring提供了DispatcherServlet来对请求进行分发。 （6）视图帮助(View Helper )—Spring提供了一系列的JSP标签，高效宏来辅助将分散的代码整合在视图里。 （7）依赖注入—贯穿于BeanFactory / ApplicationContext接口的核心理念。 Spring事务的种类和各自的区别spring支持编程式事务管理和声明式事务管理两种方式： （1）编程式事务管理使用TransactionTemplate或者直接使用底层的PlatformTransactionManager。对于编程式事务管理，spring推荐使用TransactionTemplate。 （2）声明式事务管理建立在AOP之上的。其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。声明式事务最大的优点就是不需要通过编程的方式管理事务，这样就不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中做相关的事务规则声明(或通过基于@Transactional注解的方式)，便可以将事务规则应用到业务逻辑中。 （3）显然声明式事务管理要优于编程式事务管理，这正是spring倡导的非侵入式的开发方式。声明式事务管理使业务代码不受污染，一个普通的POJO对象，只要加上注解就可以获得完全的事务支持。和编程式事务相比，声明式事务唯一不足地方是，后者的最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。 Spring的事务传播行为Spring事务的传播行为说的是当一个方法调用另一个方法时，事务该如何操作。（1）PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。 （2）PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘ （3）PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 （4）PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 （5）PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 （6）PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 （7）PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 Spring事务的实现方式和实现原理（1）划分处理单元——IOC： 由于spring解决的问题是对单个数据库进行局部事务处理的，具体的实现首相用spring中的IOC划分了事务处理单元。并且将对事务的各种配置放到了ioc容器中（设置事务管理器，设置事务的传播特性及隔离机制）。 （2）AOP拦截需要进行事务处理的类： Spring事务处理模块是通过AOP功能来实现声明式事务处理的，具体操作（比如事务实行的配置和读取，事务对象的抽象），用TransactionProxyFactoryBean接口来使用AOP功能，生成proxy代理对象，通过TransactionInterceptor完成对代理方法的拦截，将事务处理的功能编织到拦截的方法中。 读取ioc容器事务配置属性，转化为spring事务处理需要的内部数据结构（TransactionAttributeSourceAdvisor），转化为TransactionAttribute表示的数据对象。 （3）对事物处理实现（事务的生成、提交、回滚、挂起）： spring委托给具体的事务处理器实现。实现了一个抽象和适配。适配的具体事务处理器：DataSource数据源支持、hibernate数据源事务处理支持、JDO数据源事务处理支持，JPA、JTA数据源事务处理支持。这些支持都是通过设计PlatformTransactionManager、AbstractPlatforTransaction一系列事务处理的支持。为常用数据源支持提供了一系列的TransactionManager。 （4）结合： PlatformTransactionManager实现了TransactionInterception接口，让其与TransactionProxyFactoryBean结合起来，形成一个Spring声明式事务处理的设计体系。 解释一下Spring AOP里面的几个名词（1）切面（Aspect）：一个关注点的模块化，这个关注点可能会横切多个对象。事务管理是J2EE应用中一个关于横切关注点的很好的例子。 在Spring AOP中，切面可以使用通用类（基于模式的风格） 或者在普通类中以 @Aspect 注解（@AspectJ风格）来实现。 （2）连接点（Joinpoint）：在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。 在Spring AOP中，一个连接点 总是 代表一个方法的执行。 通过声明一个org.aspectj.lang.JoinPoint类型的参数可以使通知（Advice）的主体部分获得连接点信息。 （3）通知（Advice）：在切面的某个特定的连接点（Joinpoint）上执行的动作。通知有各种类型，其中包括“around”、“before”和“after”等通知。 通知的类型将在后面部分进行讨论。许多AOP框架，包括Spring，都是以拦截器做通知模型， 并维护一个以连接点为中心的拦截器链。 （4）切入点（Pointcut）：匹配连接点（Joinpoint）的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。 切入点表达式如何和连接点匹配是AOP的核心：Spring缺省使用AspectJ切入点语法。 （5）引入（Introduction）：（也被称为内部类型声明（inter-type declaration））。声明额外的方法或者某个类型的字段。 Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用一个引入来使bean实现 IsModified 接口，以便简化缓存机制。 （6）目标对象（Target Object）： 被一个或者多个切面（aspect）所通知（advise）的对象。也有人把它叫做 被通知（advised） 对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个 被代理（proxied） 对象。 （7）织入（Weaving）：把切面（aspect）连接到其它的应用程序类型或者对象上，并创建一个被通知（advised）的对象。 这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。 Spring和其他纯Java AOP框架一样，在运行时完成织入。 切入点（pointcut）和连接点（join point）匹配的概念是AOP的关键，这使得AOP不同于其它仅仅提供拦截功能的旧技术。 切入点使得定位通知（advice）可独立于OO层次。 例如，一个提供声明式事务管理的around通知可以被应用到一组横跨多个对象中的方法上（例如服务层的所有业务操作）。 通知有哪些类型？（1）前置通知（Before advice）：在某连接点（join point）之前执行的通知，但这个通知不能阻止连接点前的执行（除非它抛出一个异常）。 （2）返回后通知（After returning advice）：在某连接点（join point）正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。（3）抛出异常后通知（After throwing advice）：在方法抛出异常退出时执行的通知。（4）后通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。（5）环绕通知（Around Advice）：包围一个连接点（join point）的通知，如方法调用。这是最强大的一种通知类型。 环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它们自己的返回值或抛出异常来结束执行。 环绕通知是最常用的一种通知类型。大部分基于拦截的AOP框架，例如Nanning和JBoss4，都只提供环绕通知。]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring IOC和Spring AOP]]></title>
    <url>%2F2019%2F03%2F02%2FSpring%20IOC%E5%92%8CSpring%20AOP%2F</url>
    <content type="text"><![CDATA[Spring IOCIOC原理IOC的意思是控件反转也就是由容器控制程序之间的关系，这也是spring的优点所在，把控件权交给了外部容器。之前的写法，由程序代码直接操控，而现在控制权由应用代码中转到了外部容器。控制权的转移是所谓反转。换句话说之前用new的方式获取对象，现在由Spring给你，至于怎么给你就是DI了。 什么是DI机制(依赖注入)这里说DI又要说到IOC，依赖注入（Dependecy Injection）和控制反转（Inversion of Control）是同一个概念。具体的讲：当某个角色需要另外一个角色协助的时候，在传统的程序设计过程中，通常由调用者来创建被调用者的实例。但是在Spring中 创建被调用者的工作不再由调用者来完成，因此称为控制反转。在Spring中创建被调用者的工作由Spring来完成，然后注入调用者 因此也称为依赖注入。 Spring以动态灵活的方式来管理对象 ， 注入的四种方式： 构造方法注入 注解注入 setter注入 注意：如果通过set方法注入属性，那么spring会通过默认的空参构造方法来实例化对象，所以如果在类中写了一个带有参数的构造方法，一定要把空参数的构造方法写上，否则spring没有办法实例化对象，导致报错。 IOC的实现原理Java反射机制概念我们考虑一个场景，如果我们在程序运行时，一个对象想要检视自己所拥有的成员属性，该如何操作？ 再考虑另一个场景，如果我们想要在运行期获得某个类的Class信息如它的属性、构造方法、一般方法后再考虑是否创建它的对象，这种情况该怎么办呢？这就需要用到反射！ 对于反射，官方给出的概念：反射是Java语言的一个特性，它允许程序在运行时（注意不是编译的时候）来进行自我检查并且对内部的成员进行操作。例如它允许一个Java类获取它所有的成员变量和方法并且显示出来。 反射主要是指程序可以访问，检测和修改它本身状态或行为的一种能力，并能根据自身行为的状态和结果，调整或修改应用所描述行为的状态和相关的语义。在Java中，只要给定类的名字，那么就可以通过反射机制来获得类的所有信息。 反射是Java中一种强大的工具，能够使我们很方便的创建灵活的代码，这些代码可以在运行时装配，无需在组件之间进行源代码链接。但是如果反射使用不当会导致高成本！类中有什么信息，利用反射机制就能可以获得什么信息，不过前提是得知道类的名字。 作用 在运行时判断任意一个对象所属的类； 在运行时获取类的对象； 在运行时访问java对象的属性，方法，构造方法等。 首先要搞清楚为什么要用反射机制？直接创建对象不就可以了吗，这就涉及到了动态与静态的概念。 静态编译：在编译时确定类型，绑定对象，即通过。 动态编译：运行时确定类型，绑定对象。动态编译最大限度发挥了Java的灵活性，体现了多态的应用，有以降低类之间的藕合性。 反射机制的深度解析Java的反射机制的实现要借助于4个类：class，Constructor，Field，Method； Class：类对象，Constructor－类的构造器对象，Field－类的属性对象，Method－类的方法对象。 Class反射对象描述类语义结构，可以从Class对象中获取构造函数、成员变量、方法类等类元素的反射对象，并以编程的方式通过这些反射对象对目标类对象进行操作。这些反射对象类在java.reflect包中定义，下面是最主要的三个反射类： Constructor：类的构造函数反射类 通过Class.getDeclaredConstructors()方法可以获得类的所有构造函数反射对象数组。 在JDK5.0中，还可以通过getDeclaredConstructor(Class… parameterTypes)获取拥有特定入参的构造函数反射对象。 Constructor的一个主要方法是newInstance(Object[] initargs)，通过该方法可以创建一个对象类的实例，相当于new关键字。在JDK5.0中该方法演化为更为灵活的形式：newInstance (Object… initargs)。 Method：类方法的反射类 通过Class.getDeclaredMethods()方法可以获取类的所有方法反射类对象数组Method[]。 在JDK5.0中可以通过getDeclaredMethod(String name, Class… parameterTypes)获取特定方法，name为方法名，Class…为方法入参类型列表。 Method最主要的方法是invoke(Object obj, Object[] args)，obj表示操作的目标对象，args为方法入参。 Field：类的成员变量的反射类 通过Class.getDeclaredFields()方法可以获取类的成员变量反射对象数组 通过Class.getDeclaredField(String name)则可获取某个特定名称的成员变量反射对象。 Field类最主要的方法是set(Object obj, Object value)，obj表示操作的目标对象，通过value为目标对象的成员变量设置值。如果成员变量为基础类型，用户可以使用Field类中提供的带类型名的值设置方法，比如setBoolean(Object obj, boolean value)、setInt(Object obj, int value)等。 基本运用 获取Class对象 使用 Class 类的 forName 静态方法: 1234public static Class&lt;?&gt; forName(String className)``` 比如在 JDBC 开发中常用此方法加载数据库驱动: Class.forName(driver); 直接获取某一个对象的 class，比如: 12Class&lt;?&gt; klass = int.class;Class&lt;?&gt; classInt = Integer.TYPE; 调用某个对象的 getClass() 方法，比如: 12StringBuilder str = new StringBuilder("123");Class&lt;?&gt; klass = str.getClass(); 判断是否为某个类的实例 一般地，我们用 instanceof 关键字来判断是否为某个类的实例。同时我们也可以借助反射中 Class 对象的 isInstance() 方法来判断是否为某个类的实例，它是一个 native 方法： 1public native boolean isInstance(Object obj); 创建实例对象 使用Class对象的newInstance()方法来创建Class对象对应类的实例 12Class&lt;?&gt; c = String.class;Object str = c.newInstance(); 先通过Class对象获取指定的Constructor对象，再调用Constructor对象的newInstance()方法来创建实例。这种方法可以用指定的构造器构造类的实例 1234567//获取String所对应的Class对象Class&lt;?&gt; c = String.class;//获取String类带一个String参数的构造器Constructor constructor = c.getConstructor(String.class);//根据构造器创建实例Object obj = constructor.newInstance("23333");System.out.println(obj); 获取方法 获取某个Class对象的方法集合，主要有以下几个方法： getDeclaredMethods 方法返回类或接口声明的所有方法，包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法。 1public Method[] getDeclaredMethods() throws SecurityException getMethods 方法返回某个类的所有公用（public）方法，包括其继承类的公用方法。 1public Method[] getMethods() throws SecurityException getMethod 方法返回一个特定的方法，其中第一个参数为方法名称，后面的参数为方法的参数对应Class的对象。 1public Method getMethod(String name, Class&lt;?&gt;... parameterTypes) 例子如下： 123456789101112131415161718192021222324252627282930package org.ScZyhSoft.common;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class test1 &#123; public static void test() throws IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException &#123; Class&lt;?&gt; c = methodClass.class; Object object = c.newInstance(); Method[] methods = c.getMethods(); Method[] declaredMethods = c.getDeclaredMethods(); //获取methodClass类的add方法 Method method = c.getMethod("add", int.class, int.class); //getMethods()方法获取的所有方法 System.out.println("getMethods获取的方法："); for(Method m:methods) System.out.println(m); //getDeclaredMethods()方法获取的所有方法 System.out.println("getDeclaredMethods获取的方法："); for(Method m:declaredMethods) System.out.println(m); &#125; &#125;class methodClass &#123; public final int fuck = 3; public int add(int a,int b) &#123; return a+b; &#125; public int sub(int a,int b) &#123; return a+b; &#125;&#125; 程序的运行结果如下： 123456789101112131415getMethods获取的方法：public int org.ScZyhSoft.common.methodClass.add(int,int)public int org.ScZyhSoft.common.methodClass.sub(int,int)public final void java.lang.Object.wait() throws java.lang.InterruptedExceptionpublic final void java.lang.Object.wait(long,int) throws java.lang.InterruptedExceptionpublic final native void java.lang.Object.wait(long) throws java.lang.InterruptedExceptionpublic boolean java.lang.Object.equals(java.lang.Object)public java.lang.String java.lang.Object.toString()public native int java.lang.Object.hashCode()public final native java.lang.Class java.lang.Object.getClass()public final native void java.lang.Object.notify()public final native void java.lang.Object.notifyAll()getDeclaredMethods获取的方法：public int org.ScZyhSoft.common.methodClass.add(int,int)public int org.ScZyhSoft.common.methodClass.sub(int,int) 获取构造器信息 获取类构造器的用法与上述获取方法的用法类似。主要是通过Class类的getConstructor方法得到Constructor类的一个实例，而Constructor类有一个newInstance方法可以创建一个对象实例: 1public T newInstance(Object ... initargs) 获取类的成员变量信息 主要是这几个方法，在此不再赘述： getFiled：访问公有的成员变量 getDeclaredField：所有已声明的成员变量，但不能得到其父类的成员变量 getFileds 和 getDeclaredFields 方法用法同上（参照 Method） 调用方法 当我们从类中获取了一个方法后，我们就可以用 invoke() 方法来调用这个方法。invoke 方法的原型为: 12345678910111213141516public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException &#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, obj, modifiers); &#125; &#125; MethodAccessor ma = methodAccessor; // read volatile if (ma == null) &#123; ma = acquireMethodAccessor(); &#125; return ma.invoke(obj, args); &#125; 例如： 123456789101112131415161718192021public class test1 &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException &#123; Class&lt;?&gt; klass = methodClass.class; //创建methodClass的实例 Object obj = klass.newInstance(); //获取methodClass类的add方法 Method method = klass.getMethod("add",int.class,int.class); //调用method对应的方法 =&gt; add(1,4) Object result = method.invoke(obj,1,4); System.out.println(result); &#125;&#125;class methodClass &#123; public final int fuck = 3; public int add(int a,int b) &#123; return a+b; &#125; public int sub(int a,int b) &#123; return a+b; &#125;&#125; IOC容器技术剖析IOC中最基本的技术就是“反射(Reflection)”编程，通俗来讲就是根据给出的类名（字符串方式）来动态地生成对象，这种编程方式可以让对象在生成时才被决定到底是哪一种对象。只是在Spring中要生产的对象都在配置文件中给出定义，目的就是提高灵活性和可维护性。 目前C#、Java和PHP5等语言均支持反射，其中PHP5的技术书籍中，有时候也被翻译成“映射”。有关反射的概念和用法，大家应该都很清楚。反射的应用是很广泛的，很多的成熟的框架，比如像Java中的Hibernate、Spring框架，.Net中NHibernate、Spring.NET框架都是把”反射“做为最基本的技术手段。 我们可以把IOC容器的工作模式看做是工厂模式的升华，可以把IOC容器看作是一个工厂，这个工厂里要生产的对象都在配置文件中给出定义，然后利用编程语言提供的反射机制，根据配置文件中给出的类名生成相应的对象。从实现来看，IOC是把以前在工厂方法里写死的对象生成代码，改变为由配置文件来定义，也就是把工厂和对象生成这两者独立分隔开来，目的就是提高灵活性和可维护性。 工厂模式Spring AOP什么是AOP？面向切面编程（AOP）完善Spring的依赖注入（DI），面向切面编程在spring中主要表现为两个方面： 面向切面编程提供声明式事务管理 Spring支持用户自定义的切面 面向切面编程（aop）是对面向对象编程（oop）的补充。 面向对象编程将程序分解成各个层次的对象，面向切面编程将程序运行过程分解成各个切面。 AOP从程序运行角度考虑程序的结构，提取业务处理过程的切面，oop是静态的抽象，aop是动态的抽象， 是对应用执行过程中的步骤进行抽象，从而获得步骤之间的逻辑划分。 具体使用场景： Authentication：权限 Caching：缓存 Context passing：内容传递 Error handling：错误处理 Lazy loading：懒加载 Debugging：调试 logging, tracing, profiling and monitoring：记录跟踪 优化 校准 Performance optimization：性能优化 Persistence：持久化 Resource pooling：资源池 Synchronization：同步 Transactions：事务 使用AOP需要的一些概念 Aspect（切面）：Aspect 声明类似于 Java 中的类声明，在 Aspect 中会包含着一些 Pointcut 以及相应Advice。 Joint point（连接点）：表示在程序中明确定义的点，典型的包括方法调用，对类成员的访问以及异常处理程序块的执行等等，它自身还可以嵌套其它 joint point。 Pointcut（切点）：表示一组 joint point，这些 joint point 或是通过逻辑关系组合起来，或是通过通配、正则表达式等方式集中起来，它定义了相应的 Advice 将要发生的地方。 Advice（增强）：Advice 定义了在 Pointcut 里面定义的程序点具体要做的操作，它通过 before、after 和around 来区别是在每个 joint point 之前、之后还是代替执行的代码。 Target（目标对象）：织入 Advice 的目标对象.，即被代理类。 Weaving（织入）：将 Aspect 和其他对象连接起来, 并创建 Adviced object 的过程。 1.通知(Advice) 通知定义了在切入点代码执行时间点附近需要做的工作。 Spring支持五种类型的通知： Before(前) org.apringframework.aop.MethodBeforeAdvice After-returning(返回后) org.springframework.aop.AfterReturningAdvice After-throwing(抛出后) org.springframework.aop.ThrowsAdvice Arround(周围) org.aopaliance.intercept.MethodInterceptor Introduction(引入) org.springframework.aop.IntroductionInterceptor 2.连接点(Joinpoint) 程序能够应用通知的一个“时机”，这些“时机”就是连接点，例如方法调用时、异常抛出时、方法返回后等等。 3.切入点(Pointcut) 通知定义了切面要发生的“故事”，连接点定义了“故事”发生的时机，那么切入点就定义了“故事”发生的地点。例如某个类或方法的名称，Spring中允许我们方便的用正则表达式来指定。 4.切面(Aspect) 通知、连接点、切入点共同组成了切面：时间、地点和要发生的“故事”。 5.引入(Introduction) 引入允许我们向现有的类添加新的方法和属性(Spring提供了一个方法注入的功能）。 6.目标(Target) 即被通知的对象，如果没有AOP，那么通知的逻辑就要写在目标对象中，有了AOP之后它可以只关注自己要做的事，解耦合！ 7.代理(proxy) 应用通知的对象，详细内容参见设计模式里面的动态代理模式。 8.织入(Weaving) 把切面应用到目标对象来创建新的代理对象的过程，织入一般发生在如下几个时机: (1)编译时：当一个类文件被编译时进行织入，这需要特殊的编译器才可以做的到，例如AspectJ的织入编译器； (2)类加载时：使用特殊的ClassLoader在目标类被加载到程序之前增强类的字节代码； (3)运行时：切面在运行的某个时刻被织入,SpringAOP就是以这种方式织入切面的，原理应该是使用了JDK的动态代理技术。 Spring AOP的实现机制 Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理。 JDK动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK动态代理的核心是InvocationHandler接口和Proxy类。如果目标类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。 CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成某个类的子类，注意，CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 下面我们来了解一下什么是代理模式？ 代理模式介绍什么是代理模式？就是用一个新的对象来伪装原来的对象，从而实现一些“不可告人”的动作。 什么情况下会使用代理模式？简单来说，就是不能或者不想直接引用一个对象。什么是不能？比如我在内网中想访问外网的资源，但是因为网关的控制，访问不了。那什么是不想呢？比如我在网页上要显示一张图片，但是图片太大了，会拉慢页面的加载速度，我想用一张小一点的图片代替。 来看一张类结构图： Subject：原对象的抽象 RealSubject：原对象的实现 Proxy： 代理对象 通过代理模式，客户端访问时同原来一样，但访问的前后已经做了额外的操作(可能你的信息和数据就被窃取了)。 好了，来看一个正常点的例子。做IT的一般都需要翻墙，比如去YouTube上看点MV啥的(说好的正常呢)，但是正常访问肯定是要被屏蔽的，所以就要通过一些工具去穿过重重防守的GTW。一般的方式就是本地的工具将你的访问信息加密后，交给一个未被屏蔽的国外的服务器，然后服务器解密这些访问信息，去请求原始的访问地址，再将请求得到的资源和信息回传给你自己的本地。我们以浏览器来举例。 浏览器接口： 1234public interface Browser &#123; void visitInternet();&#125; Chrome的实现类： 1234567public class ChromeBrowser implements Browser&#123; public void visitInternet() &#123; System.out.println("visit YouTube"); &#125;&#125; 如果直接访问肯定是要挂掉的，我们通过解密和加密的两个方法简单模拟翻墙的过程。 123456789101112131415161718public class ChromeBrowser implements Browser&#123; public void visitInternet() &#123; encrypt(); System.out.println("visit YouTube"); decrypt(); &#125; // 加密 private void encrypt()&#123; System.out.println("encrypt ..."); &#125; // 解密 private void decrypt()&#123; System.out.println("decrypt ..."); &#125;&#125; 总结：虽然这样就可以访问成功了，但直接将加密和解密的方式写死在原对象里，不仅侵入了原有的代码结构且会显得很LOW。 静态代理 根据上面的代理模式的类图，最简单的方式就是写一个静态代理，为ChromeBrowser写一个代理类。 12345678910111213141516171819202122232425public class ChromeBrowserProxy implements Browser&#123; private ChromeBrowser browser; public ChromeBrowserProxy(ChromeBrowser chromeBrowser) &#123; this.browser = chromeBrowser; &#125; public void visitInternet() &#123; encrypt(); browser.visitInternet(); decrypt(); &#125; // 加密 private void encrypt()&#123; System.out.println("encrypt ..."); &#125; // 解密 private void decrypt()&#123; System.out.println("decrypt ..."); &#125;&#125; ChromeBrowserProxy同样实现Browser接口，客户端访问时不再直接访问ChromeBrowser，而是通过它的代理类。 下面是静态代理的测试类： 1234567public class StaticProxyTest &#123; public static void main(String[] args) &#123; Browser browser = new ChromeBrowserProxy(new ChromeBrowser()); browser.visitInternet(); &#125;&#125; 总结：这种方式解决了对原对象的代码侵入，但是出现了另一个问题。如果我有好几个浏览器，难道每个浏览器的实现类都要写一个代理类吗？太LOW太LOW。我们需要更牛B的方式：JDK动态代理。 JDK动态代理在JDK中提供了一种代理的实现方式，可以动态地创建代理类，就是java.lang.reflect包中的Proxy类提供的newProxyInstance方法。 1Proxy.newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) classLoader是创建代理类的类加载器 interfaces是原对象实现的接口 InvocationHandler是回调方法的接口 真正的代理过程通过InvocationHandler接口中的invoke方法来实现： 1public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; proxy是代理对象 method是执行的方法 args是执行方法的参数数组 还是以Chrome浏览器举例，JdkBrowserProxy实现InvocationHandler接口，并通过构造方法传入被代理的对象，然后在invoke方法中实现代理的过程。 12345678910111213141516171819202122232425262728293031323334public class JdkBrowserProxy implements InvocationHandler&#123; private Browser browser; public JdkBrowserProxy(Browser browser) &#123; this.browser = browser; &#125; public Browser getProxy()&#123; return (Browser) Proxy.newProxyInstance(browser.getClass().getClassLoader(), browser.getClass().getInterfaces(), this); &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; encrypt();// 程序执行前加入逻辑，MethodBeforeAdviceInterceptor Object retVal = method.invoke(browser, args); decrypt();// 程序执行后加入逻辑，MethodAfterAdviceInterceptor return retVal; &#125; /** * 加密 */ private void encrypt()&#123; System.out.println("encrypt ..."); &#125; /** * 解密 */ private void decrypt()&#123; System.out.println("decrypt ..."); &#125;&#125; 来看测试方法： 1234567public class JdkDynamicProxyTest &#123; public static void main(String[] args) &#123; Browser browser = new JdkBrowserProxy(new ChromeBrowser()).getProxy(); browser.visitInternet(); &#125;&#125; 总结：JDK的动态代理基本能够解决大部分的需求，唯一的缺点就是它只能代理接口中的方法。如果被代理对象没有实现接口，或者想代理没在接口中定义的方法，JDK的动态代理就无能为力了，此时就需要CGLIB动态代理。 CGLIB动态代理cglib是一种强大的，高性能高品质的代码生成库，用来在运行时扩展JAVA的类以及实现指定接口。 通过cglib提供的Enhancer类的create静态方法来创建代理类。 1Enhancer.create(Class type, Callback callback) type是原对象的Class对象 callback是回调方法接口 cglib中的callback通过实现它的MethodInterceptor接口的intercept方法来进行回调： 1public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args, MethodProxy proxy) throws Throwable; obj是被代理的对象 method是执行的方法 args是执行方法的参数数组 proxy用来执行未被拦截的原方法 cglib代理类不局限于上面的浏览器的例子，而是通过泛型来实现通用，并且使用单例模式减少代理类的重复创建。 1234567891011121314151617181920212223242526272829303132333435363738public class CglibBrowserProxy implements MethodInterceptor&#123; private static CglibBrowserProxy proxy = new CglibBrowserProxy(); private CglibBrowserProxy()&#123; &#125; //获取静态实例的方法 public static CglibBrowserProxy getInstance()&#123; return proxy; &#125; public &lt;T&gt; T getProxy(Class&lt;T&gt; clazz)&#123; return (T) Enhancer.create(clazz, this); &#125; public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; encrypt();// 程序执行前加入逻辑，MethodBeforeAdviceInterceptor Object retVal = proxy.invokeSuper(obj, args); decrypt();// 程序执行后加入逻辑，MethodAfterAdviceInterceptor return retVal; &#125; /** * 加密 */ private void encrypt()&#123; System.out.println("encrypt ..."); &#125; /** * 解密 */ private void decrypt()&#123; System.out.println("decrypt ..."); &#125;&#125; 然后在ChromeBrowser添加一个听音乐的方法，它并未在Browser接口定义： 123public void listenToMusic()&#123; System.out.println("listen to Cranberries");&#125; 来看下客户端测试： 12345678public class CglibDynamicProxyTest &#123; public static void main(String[] args) &#123; ChromeBrowser browser = CglibBrowserProxy.getInstance().getProxy(ChromeBrowser.class); browser.visitInternet(); browser.listenToMusic(); &#125;&#125; 总结：可以发现没有使用Browser接口来接受代理对象，而是直接使用ChromeBrowser对象。这样的方式就可以代理ChromeBrowser中未在Chrome接口中的方法。 抛出问题：如果想让一个对象调用它未实现的接口中的方法，即后面AOP里所说的引用增强，原生的cglib怎么实现呢？ CGLIB引入增强引入增强听上去很高大上，其实它的实现原理就以下几步： 通过CGLIB创建代理对象，并使其实现指定接口 在MethodIntercept的回调方法中，判断执行方法是否为接口中的方法，如果是，则通过反射调用接口的实现类。 创建一个新接口Game，它定义了开始的方法： 1234public interface Game &#123; void start();&#125; 让代理类实现Game接口，并在intercept方法中判断执行方法是接口方法还是原对象的方法： 12345678910111213141516171819202122232425262728293031323334public class CglibIntroductionBrowserProxy implements MethodInterceptor,Game&#123; private static CglibIntroductionBrowserProxy proxy = new CglibIntroductionBrowserProxy(); private CglibIntroductionBrowserProxy()&#123; &#125; public static CglibIntroductionBrowserProxy getInstance()&#123; return proxy; &#125; public &lt;T&gt; T getProxy(Class&lt;T&gt; clazz)&#123; return (T) Enhancer.create(clazz, new Class[]&#123; Game.class &#125;, this); &#125; public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; // 程序执行前加入逻辑，MethodBeforeAdviceInterceptor Object retVal; if(method.getDeclaringClass().isInterface())&#123; method.setAccessible(true); retVal = method.invoke(this, args); &#125;else&#123; retVal = proxy.invokeSuper(obj, args); &#125; // 程序执行后加入逻辑，MethodAfterAdviceInterceptor return retVal; &#125; public void start() &#123; System.out.println("start a game"); &#125;&#125; 来看测试类： 12345678910public class CglibIntroductionDynamicProxyTest &#123; public static void main(String[] args) &#123; Browser browser = CglibIntroductionBrowserProxy.getInstance().getProxy(ChromeBrowser.class); browser.visitInternet(); Game game = (Game) browser; game.start(); &#125;&#125; 总结：可以发现执行接口方法时，通过jdk的反射机制来实现的。而调用其自身方法，则是通过cglib来触发的。 最后补充几点 JDK动态代理的代理对象只能通过接口去接收，如果用原对象接收，会报类型转换异常。 cglib不能拦截final修饰的方法，调用时只会执行原有方法。 cglib是在运行时通过操作字节码来完成类的扩展和改变，除了代理，还支持很多强大的操作，比如bean的生成和属性copy，动态创建接口以及融合多个对象等，具体见https://github.com/cglib/cglib/wiki/Tutorial。 参考： https://my.oschina.net/u/2377110/blog/1504596 https://my.oschina.net/u/2377110/blog/1504596 https://blog.csdn.net/dreamrealised/article/details/12885739]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Bean]]></title>
    <url>%2F2019%2F02%2F28%2FSpring%20Bean%2F</url>
    <content type="text"><![CDATA[前言在 Spring 中，那些组成应用程序的主体及由 Spring IOC 容器所管理的对象，被称之为 bean。简单地讲，bean 就是由 IOC 容器初始化、装配及管理的对象，除此之外，bean 就与应用程序中的其他对象没有什么区别了。而 bean 的定义以及 bean 相互间的依赖关系将通过配置元数据来描述。 Spring中的bean默认都是单例的，这些单例Bean在多线程程序下如何保证线程安全呢？ 例如对于Web应用来说，Web容器对于每个用户请求都创建一个单独的Sevlet线程来处理请求，引入Spring框架之后，每个Action都是单例的，那么对于Spring托管的单例Service Bean，如何保证其安全呢？ Spring的单例是基于BeanFactory也就是Spring容器的，单例Bean在此容器内只有一个，Java的单例是基于 JVM，每个 JVM 内只有一个实例。 在大多数情况下，单例 bean 是很理想的方案。不过，有时候你可能会发现你所使用的类是易变的，它们会保持一些状态，因此重用是不安全的。在这种情况下，将 class 声明为单例的就不是那么明智了。因为对象会被污染，稍后重用的时候会出现意想不到的问题，所以 Spring 定义了多种作用域的bean。 bean的作用域创建一个bean定义，其实质是用该bean定义对应的类来创建真正实例的“配方”。把bean定义看成一个配方很有意义，它与class很类似，只根据一张“处方”就可以创建多个实例。不仅可以控制注入到对象中的各种依赖和配置值，还可以控制该对象的作用域。这样可以灵活选择所建对象的作用域，而不必在Java Class级定义作用域。 Spring Framework支持五种作用域，分别阐述如下表： 五种作用域中，request、session 和 global session 三种作用域仅在基于web的应用中使用（不必关心你所采用的是什么web应用框架），只能用在基于 web 的 Spring ApplicationContext(另外一种IOC容器)环境。 singleton——唯一 bean 实例 当一个 bean 的作用域为 singleton，那么Spring IoC容器中只会存在一个共享的 bean 实例，并且所有对 bean 的请求，只要 id 与该 bean 定义相匹配，则只会返回bean的同一实例。 singleton 是单例类型(对应于单例模式)，就是在创建起容器时就同时自动创建了一个bean的对象，不管你是否使用，但我们可以指定Bean节点的 lazy-init=”true” 来延迟初始化bean**，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。 每次获取到的对象都是同一个对象。注意，singleton 作用域是Spring中的缺省作用域。 要在XML中将 bean 定义成 singleton ，可以这样配置： 1&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" scope="singleton"&gt; 也可以通过 @Scope 注解（它可以显示指定bean的作用范围）的方式： 12345@Service@Scope("singleton")public class ServiceImpl&#123;&#125; prototype——每次请求都会创建一个新的 bean 实例 当一个bean的作用域为 prototype，表示一个 bean 定义对应多个对象实例。 prototype 作用域的 bean 会导致在每次对该 bean 请求（将其注入到另一个 bean 中，或者以程序的方式调用容器的 getBean() 方法）时都会创建一个新的 bean 实例。prototype 是原型类型，它在我们创建容器的时候并没有实例化，而是当我们获取bean的时候才会去创建一个对象，而且我们每次获取到的对象都不是同一个对象。 根据经验，对有状态的 bean 应该使用 prototype 作用域，而对无状态的 bean 则应该使用 singleton 作用域。 在 XML 中将 bean 定义成 prototype ，可以这样配置： 123&lt;bean id="account" class="com.foo.DefaultAccount" scope="prototype"/&gt; 或者&lt;bean id="account" class="com.foo.DefaultAccount" singleton="false"/&gt; 通过 @Scope 注解的方式实现就不做演示了。 request——每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效 request只适用于Web程序，每一次 HTTP 请求都会产生一个新的bean，同时该bean仅在当前HTTP request内有效，当请求结束后，该对象的生命周期即告结束。 在 XML 中将 bean 定义成 request ，可以这样配置： 1&lt;bean id="loginAction" class=cn.csdn.LoginAction" scope="request"/&gt; session——每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效 session只适用于Web程序，session 作用域表示该针对每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效.与request作用域一样，可以根据需要放心的更改所创建实例的内部状态，而别的 HTTP session 中根据 userPreferences 创建的实例，将不会看到这些特定于某个 HTTP session 的状态变化。当HTTP session最终被废弃的时候，在该HTTP session作用域内的bean也会被废弃掉。 在 XML 中将 bean 定义成 session ，可以这样配置： 1&lt;bean id="userPreferences" class="com.foo.UserPreferences" scope="session"/&gt; globalSession globalSession作用域类似于标准的 HTTP session 作用域，不过仅仅在基于 portlet 的 web 应用中才有意义。Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portle t所共享。在global session 作用域中定义的 bean 被限定于全局portlet Session的生命周期范围内。 在 XML 中将 bean 定义成 globalSession ，可以这样配置： 1&lt;bean id="user" class="com.foo.Preferences "scope="globalSession"/&gt; bean的生命周期概述Spring的IOC容器功能非常强大，负责Spring的Bean的创建和管理等功能。而Spring bean是整个Spring应用中很重要的一部分，了解Spring Bean的生命周期对我们了解整个Spring框架会有很大的帮助。 BeanFactory、ApplicationContext是Spring两种很重要的容器。前者提供了最基本的依赖注入的支持，而后者在继承前者的基础进行了功能的拓展，例如增加了事件传播，资源访问和国际化的消息访问等功能。本文主要介绍了ApplicationContext和BeanFactory两种容器的Bean的生命周期。 Spring Bean的生命周期总的来说是这样子的： Bean容器找到配置文件中 Spring Bean 的定义。 Bean容器利用Java Reflection API创建一个Bean的实例。 如果涉及到一些属性值 利用set方法设置一些属性值。 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 如果Bean实现了BeanFactoryAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 与上面的类似，如果实现了其他*Aware接口，就调用相应的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessBeforeInitialization()方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessAfterInitialization()方法 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。 当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。 ApplicationContext Bean生命周期 上图修正为“调用disposableBean的destroy()方法” ApplicationContext容器中，Bean的生命周期流程如上图所示，流程大致如下： 1.首先容器启动后，会对scope为singleton且非懒加载的bean进行实例化。 2.按照Bean定义信息配置信息，注入所有的属性。 3.如果Bean实现了BeanNameAware接口，会回调该接口的setBeanName()方法，传入该Bean的id，此时该Bean就获得了自己在配置文件中的id。 4.如果Bean实现了BeanFactoryAware接口,会回调该接口的setBeanFactory()方法，传入该Bean的BeanFactory，这样该Bean就获得了自己所在的BeanFactory。 5.如果Bean实现了ApplicationContextAware接口,会回调该接口的setApplicationContext()方法，传入该Bean的ApplicationContext，这样该Bean就获得了自己所在的ApplicationContext。 6.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessBeforeInitialzation()方法。 7.如果Bean实现了InitializingBean接口，则会回调该接口的afterPropertiesSet()方法。 8.如果Bean配置了init-method方法，则会执行init-method配置的方法。 9.如果有Bean实现了BeanPostProcessor接口，则会回调该接口的postProcessAfterInitialization()方法。 10.经过流程9之后，就可以正式使用该Bean了。对于scope为singleton的Bean,Spring的ioc容器中会缓存一份该bean的实例，而对于scope为prototype的Bean,每次被调用都会new一个新的对象，其生命周期就交给调用方管理了，不再是Spring容器进行管理了。 11.容器关闭后，如果Bean实现了DisposableBean接口，则会回调该接口的destroy()方法。 12.如果Bean配置了destroy-method方法，则会执行destroy-method配置的方法。至此，整个Bean的生命周期结束。 示例： 我们定义了一个Person类，该类实现了BeanNameAware,BeanFactoryAware,ApplicationContextAware,InitializingBean,DisposableBean五个接口。在applicationContext.xml文件中配置了该Bean的id为person1,并且配置了init-method和destroy-method,为该Bean配置了属性name为jack的值，然后定义了一个MyBeanPostProcessor方法,该方法实现了BeanPostProcessor接口，在applicationContext.xml文件中配置了该方法的Bean,其代码如下所示: applicationContext.xml文件，代码如下： 123456789101112131415161718192021&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.2.xsd"&gt; &lt;bean id="person1" destroy-method="myDestroy" init-method="myInit" class="com.test.spring.life.Person"&gt; &lt;property name="name"&gt; &lt;value&gt;jack&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置自定义的后置处理器 --&gt; &lt;bean id="postProcessor" class="com.pingan.spring.life.MyBeanPostProcessor" /&gt;&lt;/beans&gt; Person类，代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Person implements BeanNameAware, BeanFactoryAware, ApplicationContextAware, InitializingBean, DisposableBean &#123; private String name; public Person() &#123; System.out.println("PersonService类构造方法"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; System.out.println("set方法被调用"); &#125; //自定义的初始化函数 public void myInit() &#123; System.out.println("myInit被调用"); &#125; //自定义的销毁方法 public void myDestroy() &#123; System.out.println("myDestroy被调用"); &#125; public void destroy() throws Exception &#123; // TODO Auto-generated method stub System.out.println("destory被调用"); &#125; public void afterPropertiesSet() throws Exception &#123; // TODO Auto-generated method stub System.out.println("afterPropertiesSet被调用"); &#125; public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; // TODO Auto-generated method stub System.out.println("setApplicationContext被调用"); &#125; public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; // TODO Auto-generated method stub System.out.println("setBeanFactory被调用,beanFactory"); &#125; public void setBeanName(String beanName) &#123; // TODO Auto-generated method stub System.out.println("setBeanName被调用,beanName:" + beanName); &#125; public String toString() &#123; return "name is :" + name; &#125; MyBeanPostProcessor类，实现了BeanPostProcessor接口 123456789101112131415161718public class MyBeanPostProcessor implements BeanPostProcessor &#123; public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; // TODO Auto-generated method stub System.out.println("postProcessBeforeInitialization被调用"); return bean; &#125; public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; // TODO Auto-generated method stub System.out.println("postProcessAfterInitialization被调用"); return bean; &#125;&#125; 测试类，代码如下： 1234567891011121314151617public class AcPersonServiceTest &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub System.out.println("开始初始化容器"); ApplicationContext ac = new ClassPathXmlApplicationContext("com/test/spring/life/applicationContext.xml"); System.out.println("xml加载完毕"); Person person1 = (Person) ac.getBean("person1"); System.out.println(person1); System.out.println("关闭容器"); ((ClassPathXmlApplicationContext)ac).close(); &#125;&#125; 我们启动容器，可以看到整个初始化的过程： 123456789101112131415161718192021开始初始化容器九月 25, 2016 10:44:50 下午 org.springframework.context.support.ClassPathXmlApplicationContext prepareRefresh信息: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@b4aa453: startup date [Sun Sep 25 22:44:50 CST 2016]; root of context hierarchy九月 25, 2016 10:44:50 下午 org.springframework.beans.factory.xml.XmlBeanDefinitionReader loadBeanDefinitions信息: Loading XML bean definitions from class path resource [com/test/spring/life/applicationContext.xml]Person类构造方法set方法被调用setBeanName被调用,beanName:person1setBeanFactory被调用,beanFactorysetApplicationContext被调用postProcessBeforeInitialization被调用afterPropertiesSet被调用myInit被调用postProcessAfterInitialization被调用xml加载完毕name is :jack关闭容器九月 25, 2016 10:44:51 下午 org.springframework.context.support.ClassPathXmlApplicationContext doClose信息: Closing org.springframework.context.support.ClassPathXmlApplicationContext@b4aa453: startup date [Sun Sep 25 22:44:50 CST 2016]; root of context hierarchydestory被调用myDestroy被调用 BeanFactory Bean生命周期 上图修正为“调用disposableBean的destroy()方法” BeanFactoty容器中, Bean的生命周期如上图所示，与ApplicationContext相比，有如下几点不同： 1.BeanFactory容器中，不会调用ApplicationContextAware接口的setApplicationContext()方法。 2.BeanPostProcessor接口的postProcessBeforeInitialzation()方法和postProcessAfterInitialization()方法不会自动调用，必须自己通过代码手动注册。 3.BeanFactory容器启动的时候，不会去实例化所有Bean,包括所有scope为singleton且非懒加载的Bean也是一样，而是在调用的时候去实例化。 示例： 我们还是使用前面定义好的Person类和MyBeanPostProcessor类，以及ApplicationContext.xml文件。 BfPersonServiceTest测试类的main函数实现如下： 12345678910111213141516public class BfPersonServiceTest &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub System.out.println("开始初始化容器"); ConfigurableBeanFactory bf = new XmlBeanFactory(new ClassPathResource("com/pingan/spring/life/applicationContext.xml")); System.out.println("xml加载完毕"); //beanFactory需要手动注册beanPostProcessor类的方法 bf.addBeanPostProcessor(new MyBeanPostProcessor()); Person person1 = (Person) bf.getBean("person1"); System.out.println(person1); System.out.println("关闭容器"); bf.destroySingletons(); &#125;&#125; 启动容器，我们可以看到整个调用流程： 12345678910111213141516开始初始化容器九月 26, 2016 12:27:05 上午 org.springframework.beans.factory.xml.XmlBeanDefinitionReader loadBeanDefinitions信息: Loading XML bean definitions from class path resource [com/pingan/spring/life/applicationContext.xml]xml加载完毕PersonService类构造方法set方法被调用setBeanName被调用,beanName:person1setBeanFactory被调用,beanFactorypostProcessBeforeInitialization被调用afterPropertiesSet被调用myInit被调用postProcessAfterInitialization被调用name is :jack关闭容器destory被调用myDestroy被调用 initialization 和 destroy 有时我们需要在Bean属性值set好之后和Bean销毁之前做一些事情，比如检查Bean中某个属性是否被正常的设置好值了。Spring框架提供了多种方法让我们可以在Spring Bean的生命周期中执行initialization和pre-destroy方法。 实现InitializingBean和DisposableBean接口 这两个接口都只包含一个方法。 通过实现InitializingBean接口的afterPropertiesSet()方法可以在Bean属性值设置好之后做一些操作，实现DisposableBean接口的destroy()方法可以在销毁Bean之前做一些操作。 例子如下： 12345678910public class GiraffeService implements InitializingBean,DisposableBean &#123; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("执行InitializingBean接口的afterPropertiesSet方法"); &#125; @Override public void destroy() throws Exception &#123; System.out.println("执行DisposableBean接口的destroy方法"); &#125;&#125; 注意：这种方法比较简单，但是不建议使用。因为这样会将Bean的实现和Spring框架耦合在一起。 在bean的配置文件中指定init-method和destroy-method方法 Spring允许我们创建自己的 init 方法和 destroy 方法，只要在 Bean 的配置文件中指定 init-method 和 destroy-method 的值就可以在 Bean 初始化时和销毁之前执行一些操作。 例子如下： 12345678910public class GiraffeService &#123; //通过&lt;bean&gt;的destroy-method属性指定的销毁方法 public void destroyMethod() throws Exception &#123; System.out.println("执行配置的destroy-method"); &#125; //通过&lt;bean&gt;的init-method属性指定的初始化方法 public void initMethod() throws Exception &#123; System.out.println("执行配置的init-method"); &#125;&#125; 配置文件中的配置： 12&lt;bean name="giraffeService" class="com.giraffe.spring.service.GiraffeService" init-method="initMethod" destroy-method="destroyMethod"&gt;&lt;/bean&gt; 注意：自定义的init-method和post-method方法可以抛异常但是不能有参数。这种方式比较推荐，因为可以自己创建方法，无需将Bean的实现直接依赖于spring的框架。 使用@PostConstruct和@PreDestroy注解 除了xml配置的方式，Spring 也支持用 @PostConstruct和 @PreDestroy注解来指定 init 和 destroy 方法。这两个注解均在javax.annotation 包中。为了注解可以生效，需要在配置文件中定义org.springframework.context.annotation.CommonAnnotationBeanPostProcessor或context:annotation-config 例子如下： 12345678910public class GiraffeService &#123; @PostConstruct public void initPostConstruct()&#123; System.out.println("执行PostConstruct注解标注的方法"); &#125; @PreDestroy public void preDestroy()&#123; System.out.println("执行preDestroy注解标注的方法"); &#125;&#125; 配置文件中的配置： 1&lt;bean class="org.springframework.context.annotation.CommonAnnotationBeanPostProcessor" /&gt; 实现*Aware接口，在Bean中使用Spring框架的一些对象有些时候我们需要在 Bean 的初始化中使用 Spring 框架自身的一些对象来执行一些操作。比如获ServletContext 的一些参数，获取 ApplicaitionContext 中的 BeanDefinition 的名字，获取 Bean 在容器中的名字等等。为了让 Bean可以获取到框架自身的一些对象，Spring 提供了一组名为*Aware的接口。 这些接口均继承于org.springframework.beans.factory.Aware标记接口，并提供一个将由 Bean 实现的set*方法,Spring通过基于setter的依赖注入方式使相应的对象可以被Bean使用。 介绍一些重要的Aware接口： ApplicationContextAware: 获得ApplicationContext对象,可以用来获取所有Bean definition的名字。 BeanFactoryAware:获得BeanFactory对象，可以用来检测Bean的作用域。 BeanNameAware:获得Bean在配置文件中定义的名字。 ResourceLoaderAware:获得ResourceLoader对象，可以获得classpath中某个文件。 ServletContextAware:在一个MVC应用中可以获取ServletContext对象，可以读取context中的参数。 ServletConfigAware： 在一个MVC应用中可以获取ServletConfig对象，可以读取config中的参数。 12345678910111213141516171819202122232425262728293031323334353637383940public class GiraffeService implements ApplicationContextAware, ApplicationEventPublisherAware, BeanClassLoaderAware, BeanFactoryAware, BeanNameAware, EnvironmentAware, ImportAware, ResourceLoaderAware&#123; @Override public void setBeanClassLoader(ClassLoader classLoader) &#123; System.out.println("执行setBeanClassLoader,ClassLoader Name = " + classLoader.getClass().getName()); &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; System.out.println("执行setBeanFactory,setBeanFactory:: giraffe bean singleton=" + beanFactory.isSingleton("giraffeService")); &#125; @Override public void setBeanName(String s) &#123; System.out.println("执行setBeanName:: Bean Name defined in context=" + s); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; System.out.println("执行setApplicationContext:: Bean Definition Names=" + Arrays.toString(applicationContext.getBeanDefinitionNames())); &#125; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; System.out.println("执行setApplicationEventPublisher"); &#125; @Override public void setEnvironment(Environment environment) &#123; System.out.println("执行setEnvironment"); &#125; @Override public void setResourceLoader(ResourceLoader resourceLoader) &#123; Resource resource = resourceLoader.getResource("classpath:spring-beans.xml"); System.out.println("执行setResourceLoader:: Resource File Name=" + resource.getFilename()); &#125; @Override public void setImportMetadata(AnnotationMetadata annotationMetadata) &#123; System.out.println("执行setImportMetadata"); &#125;&#125; BeanPostProcessor上面的*Aware接口是针对某个实现这些接口的Bean定制初始化的过程， Spring同样可以针对容器中的所有Bean，或者某些Bean定制初始化过程，只需提供一个实现BeanPostProcessor接口的类即可。 该接口中包含两个方法，postProcessBeforeInitialization和postProcessAfterInitialization。 postProcessBeforeInitialization方法会在容器中的Bean初始化之前执行， postProcessAfterInitialization方法在容器中的Bean初始化之后执行。 例子如下： 123456789101112public class CustomerBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=" + beanName); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=" + beanName); return bean; &#125;&#125; 要将BeanPostProcessor的Bean像其他Bean一样定义在配置文件中： 1&lt;bean class="com.giraffe.spring.service.CustomerBeanPostProcessor"/&gt; 单例管理的对象和非单例管理的对象单例管理的对象当scope=”singleton”，即默认情况下，会在启动容器时（即实例化容器时）时实例化。但我们可以指定Bean节点的lazy-init=”true”来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。如下配置： 1&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" lazy-init="true"/&gt; 如果想对所有的默认单例bean都应用延迟初始化，可以在根节点beans设置default-lazy-init属性为true，如下所示： 1&lt;beans default-lazy-init="true" …&gt; 默认情况下，Spring 在读取 xml 文件的时候，就会创建对象。在创建对象的时候先调用构造器，然后调用 init-method 属性值中所指定的方法。在对象在被销毁的时候，会调用 destroy-method 属性值中所指定的方法。 非单例管理的对象当scope=”prototype”时，容器也会延迟初始化 bean，Spring 读取xml 文件的时候，并不会立刻创建对象，而是在第一次请求该 bean 时才初始化（如调用getBean方法时）。在第一次请求每一个 prototype 的bean 时，Spring容器都会调用其构造器创建这个对象，然后调用init-method属性值中所指定的方法。在对象被销毁的时候，Spring 容器不会帮我们调用任何方法，因为是非单例，这个类型的对象有很多个，Spring容器一旦把这个对象交给你之后，就不再管理这个对象了。 总结可以发现，对于作用域为 prototype 的 bean ，其destroy方法并没有被调用。如果 bean 的 scope 设为prototype时，当容器关闭时，destroy 方法不会被调用。 对于 prototype 作用域的 bean，有一点非常重要，那就是 Spring不能对一个 prototype bean 的整个生命周期负责：容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法。但对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责（让Spring容器释放被prototype作用域bean占用资源的一种可行方式是，通过使用Bean的后置处理器BeanPostProcessor，该处理器持有要被清除的bean的引用）。 谈及prototype作用域的bean时，在某些方面你可以将Spring容器的角色看作是Java new操作的替代者，任何迟于该时间点的生命周期事宜都得交由客户端来处理。 Spring 容器可以管理 singleton 作用域下 bean 的生命周期，在此作用域下，Spring 能够精确地知道bean何时被创建，何时初始化完成，以及何时被销毁。而对于 prototype 作用域的bean，Spring只负责创建，当容器创建了 bean 的实例后，bean 的实例就交给了客户端的代码管理，Spring容器将不再跟踪其生命周期，并且不会管理那些被配置成prototype作用域的bean的生命周期。 参考： https://github.com/Snailclimb/JavaGuide/blob/master/%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/SpringBean.md https://www.jianshu.com/p/3944792a5fff]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC工作原理详解]]></title>
    <url>%2F2019%2F02%2F28%2FSpringMVC%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[先谈下什么是MVC模式MVC是一种设计模式，分为Model、View、Controller三层 Spring MVC的简单原理图如下： Spring MVC的简单介绍：SpringMVC 框架是以请求为驱动，围绕 Servlet 设计，将请求发给控制器，然后通过模型对象，分派器来展示请求结果视图。其中核心类是 DispatcherServlet，它是一个 Servlet，顶层是实现的Servlet接口。 Spring MVC的使用：在 web.xml 中配置 DispatcherServlet ，并且需要配置 Spring 监听器ContextLoaderListener： 12345678910111213141516171819&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;!-- 如果不设置init-param标签，则必须在/WEB-INF/下创建xxx-servlet.xml文件，其中xxx是servlet-name中配置的名称。 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/springmvc-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; Spring MVC的工作原理： 简单来说： 客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 DispatcherServlet根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 HandlerAdapter会根据 Handler 来调用真正的处理器来处理请求，并处理相应的业务逻辑。 处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） 如下图所示： 上图的一个笔误的小问题：Spring MVC 的入口函数也就是前端控制器 DispatcherServlet 的作用是接收请求，响应结果。 详细流程：(一次请求的过程)1、在整个 Spring MVC 框架中，DispatcherServlet 处于核心位置，它负责协调和组织不同组件完成请求处理并返回响应工作。在看 DispatcherServlet 类之前，我们先来看一下请求处理的大致流程：Tomcat 启动，对 DispatcherServlet 进行实例化，然后调用它的init() 方法进行初始化，在这个初始化过程中完成了：对 web.xml 中初始化参数的加载；建立WebApplicationContext (SpringMVC的IOC容器)；进行组件的初始化；2、客户端发出请求，由 Tomcat 接收到这个请求，如果匹配 DispatcherServlet 在 web.xml 中配置的映射路径，Tomcat 就将请求转交给 DispatcherServlet 处理；3、DispatcherServlet 从容器中取出所有 HandlerMapping 实例（每个实例对应一个 HandlerMapping 接口的实现类）并遍历，每个 HandlerMapping 会根据请求信息，通过自己实现类中的方式去找到处理该请求的 Handler (执行程序，如Controller中的方法)，并且将这个 Handler 与一堆 HandlerInterceptor (拦截器) 封装成一个 HandlerExecutionChain 对象，一旦有一个 HandlerMapping 可以找到 Handler 则退出循环；4、DispatcherServlet 取出 HandlerAdapter 组件，根据已经找到的 Handler，再从所有 HandlerAdapter 中找到可以处理该 Handler 的 HandlerAdapter 对象；5、执行 HandlerExecutionChain 中所有拦截器的 preHandler() 方法，然后再利用HandlerAdapter 执行 Handler ，执行完成得到 ModelAndView，再依次调用拦截器的 postHandler() 方法；6、利用 ViewResolver 将 ModelAndView 或是 Exception（可解析成 ModelAndView）解析成 View，然后 View 会调用 render() 方法再根据 ModelAndView 中的Model（数据）渲染出页面；7、最后再依次调用拦截器的 afterCompletion() 方法，这一次请求就结束了。 Spring MVC重要组件说明 1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供（重要） 作用：Spring MVC 的入口函数。接收请求，响应结果，相当于转发器，中央处理器。有了 DispatcherServlet 减少了其它组件之间的耦合度。用户请求到达前端控制器，它就相当于mvc模式中的c，DispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，DispatcherServlet的存在降低了组件之间的耦合性。 2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供 作用：根据请求的url查找Handler。HandlerMapping负责根据用户请求找到Handler即处理器（Controller），SpringMVC提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler。 通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 4、处理器Handler(需要工程师开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler。Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。 由于Handler涉及到具体的用户业务请求，所以一般情况需要工程师根据业务需求开发Handler。 5、视图解析器View resolver(不需要工程师开发),由框架提供 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view）。 View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。 一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。 6、视图View(需要工程师开发) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf…） 注意：处理器Handler（也就是我们平常说的Controller控制器）以及视图层view都是需要我们自己手动开发的。其他的一些组件比如：前端控制器DispatcherServlet、处理器映射器HandlerMapping、处理器适配器HandlerAdapter等等都是框架提供给我们的，不需要自己手动开发。 DispatcherServlet详细解析 首先看下源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package org.springframework.web.servlet; @SuppressWarnings("serial")public class DispatcherServlet extends FrameworkServlet &#123; public static final String MULTIPART_RESOLVER_BEAN_NAME = "multipartResolver"; public static final String LOCALE_RESOLVER_BEAN_NAME = "localeResolver"; public static final String THEME_RESOLVER_BEAN_NAME = "themeResolver"; public static final String HANDLER_MAPPING_BEAN_NAME = "handlerMapping"; public static final String HANDLER_ADAPTER_BEAN_NAME = "handlerAdapter"; public static final String HANDLER_EXCEPTION_RESOLVER_BEAN_NAME = "handlerExceptionResolver"; public static final String REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME = "viewNameTranslator"; public static final String VIEW_RESOLVER_BEAN_NAME = "viewResolver"; public static final String FLASH_MAP_MANAGER_BEAN_NAME = "flashMapManager"; public static final String WEB_APPLICATION_CONTEXT_ATTRIBUTE = DispatcherServlet.class.getName() + ".CONTEXT"; public static final String LOCALE_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".LOCALE_RESOLVER"; public static final String THEME_RESOLVER_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_RESOLVER"; public static final String THEME_SOURCE_ATTRIBUTE = DispatcherServlet.class.getName() + ".THEME_SOURCE"; public static final String INPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".INPUT_FLASH_MAP"; public static final String OUTPUT_FLASH_MAP_ATTRIBUTE = DispatcherServlet.class.getName() + ".OUTPUT_FLASH_MAP"; public static final String FLASH_MAP_MANAGER_ATTRIBUTE = DispatcherServlet.class.getName() + ".FLASH_MAP_MANAGER"; public static final String EXCEPTION_ATTRIBUTE = DispatcherServlet.class.getName() + ".EXCEPTION"; public static final String PAGE_NOT_FOUND_LOG_CATEGORY = "org.springframework.web.servlet.PageNotFound"; private static final String DEFAULT_STRATEGIES_PATH = "DispatcherServlet.properties"; protected static final Log pageNotFoundLogger = LogFactory.getLog(PAGE_NOT_FOUND_LOG_CATEGORY); private static final Properties defaultStrategies; static &#123; try &#123; ClassPathResource resource = new ClassPathResource(DEFAULT_STRATEGIES_PATH, DispatcherServlet.class); defaultStrategies = PropertiesLoaderUtils.loadProperties(resource); &#125; catch (IOException ex) &#123; throw new IllegalStateException("Could not load 'DispatcherServlet.properties': " + ex.getMessage()); &#125; &#125; /** Detect all HandlerMappings or just expect "handlerMapping" bean? */ private boolean detectAllHandlerMappings = true; /** Detect all HandlerAdapters or just expect "handlerAdapter" bean? */ private boolean detectAllHandlerAdapters = true; /** Detect all HandlerExceptionResolvers or just expect "handlerExceptionResolver" bean? */ private boolean detectAllHandlerExceptionResolvers = true; /** Detect all ViewResolvers or just expect "viewResolver" bean? */ private boolean detectAllViewResolvers = true; /** Throw a NoHandlerFoundException if no Handler was found to process this request? **/ private boolean throwExceptionIfNoHandlerFound = false; /** Perform cleanup of request attributes after include request? */ private boolean cleanupAfterInclude = true; /** MultipartResolver used by this servlet */ private MultipartResolver multipartResolver; /** LocaleResolver used by this servlet */ private LocaleResolver localeResolver; /** ThemeResolver used by this servlet */ private ThemeResolver themeResolver; /** List of HandlerMappings used by this servlet */ private List&lt;HandlerMapping&gt; handlerMappings; /** List of HandlerAdapters used by this servlet */ private List&lt;HandlerAdapter&gt; handlerAdapters; /** List of HandlerExceptionResolvers used by this servlet */ private List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers; /** RequestToViewNameTranslator used by this servlet */ private RequestToViewNameTranslator viewNameTranslator; private FlashMapManager flashMapManager; /** List of ViewResolvers used by this servlet */ private List&lt;ViewResolver&gt; viewResolvers; public DispatcherServlet() &#123; super(); &#125; public DispatcherServlet(WebApplicationContext webApplicationContext) &#123; super(webApplicationContext); &#125; @Override protected void onRefresh(ApplicationContext context) &#123; initStrategies(context); &#125; protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context); &#125;&#125; DispatcherServlet类中的属性beans： HandlerMapping：用于handlers映射请求和一系列的对于拦截器的前处理和后处理，大部分用@Controller注解。 HandlerAdapter：帮助DispatcherServlet处理映射请求处理程序的适配器，而不用考虑实际调用的是哪个处理程序。 ViewResolver：根据实际配置解析实际的View类型。 ThemeResolver：解决Web应用程序可以使用的主题，例如提供个性化布局。 MultipartResolver：解析多部分请求，以支持从HTML表单上传文件。- FlashMapManager：存储并检索可用于将一个请求属性传递到另一个请求的input和output的FlashMap，通常用于重定向。 在Web MVC框架中，每个DispatcherServlet都拥自己的WebApplicationContext，它继承了ApplicationContext。WebApplicationContext包含了其上下文和Servlet实例之间共享的所有的基础框架beans。 HandlerMapping： HandlerMapping接口处理请求的映射HandlerMapping接口的实现类： SimpleUrlHandlerMapping类通过配置文件把URL映射到Controller类。 DefaultAnnotationHandlerMapping类通过注解把URL映射到Controller类。 HandlerAdapter： HandlerAdapter接口-&gt;处理请求映射 AnnotationMethodHandlerAdapter：通过注解，把请求URL映射到Controller类的方法上。 HandlerExceptionResolver： HandlerExceptionResolver接口-异常处理接口 SimpleMappingExceptionResolver类通过配置文件进行异常处理。 AnnotationMethodHandlerExceptionResolver类通过注解进行异常处理。 ViewResolver： ViewResolver接口-&gt;解析View视图。 UrlBasedViewResolver类通过配置文件，把一个视图名交给到一个View来处理。 参考： https://github.com/Snailclimb/JavaGuide/blob/master/%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/SpringMVC%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3.md https://blog.csdn.net/jianyuerensheng/article/details/51258942]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql常见面试题]]></title>
    <url>%2F2019%2F02%2F24%2FMysql%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[数据库索引原理MyISAM和InnoDB索引的底层实现原理MyISAM 和 InnoDB 是MySQL的两代搜索引擎，区别在于，对于辅助索引的实现原理不一样，并且MyISAM是索引和文件分离的，而InnoDB不是。 一般以主键为索引的叫做主索引，而以其他键为索引的叫做辅助索引； MyISAM的实现原理，利用B+树实现：下面为主键索引： 由上图可以看出，col1是主键，而叶子结点存储的数据是一个地址，通过地址找到数据； 下面是辅助索引（和主索引不同的是辅助索引的key是可以重复的） : 注意：上图的叶子节点34下面的地址值应为0x07 InnoDB的实现原理，利用B+树实现:下面是主键索引： 这是主索引，即利用主键构造的B+树； 注意，和MyISAM不同的是叶子结点的数据域保存的是全部数据； 下面在看辅助索引： 仔细看辅助索引和主索引的区别，辅助索引的叶子结点保存的是主键。 这就是MyISAM和InnoDB最大的不同。 既然MyISAM和InnoDB是MySQL的两代引擎，肯定会有一个提升，而InnoDB是最新一代，那么它到底优在哪里？试想，MyISAM和InnoDB都是以B+树为基础实现的，相对于B树的不同其实前面已经讲过，即数据域和结点分离。而MyISAM更是索引和文件分离，B+树的叶子结点的数据域存放的是文件内容的地址，主索引和辅助索引的B+树都是如此。那么如果我改变了一个地址，是不是所有的索引树都得改变，正如前面我们讲的在磁盘上频繁的读写操作是效率很低的，而这块又不适用局部原理，因为逻辑上相邻的结点，物理上不一定相邻，那么这样就会造成效率上的降低； 于是乎，InnoDB就产生了，它让除了主索引以外的辅助索引的叶子结点的数据域都保存主键，先通过辅助索引找到主键，然后通过主键找到叶子结点的所有数据，听起来貌似很麻烦，遍历了两棵树，但是，这样如果有了修改的话，改变的只是主索引，其它辅助索引都不用动。而且，数据库中的树的每一个结点的key可不是咱们给的那么少。试想如果一个结点有1024个key，那么高度为2的B+树都有1024*1024个key，所以一般树的高度都很低，所以，遍历树的消耗几乎忽略不计！ 另外，我们也就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 为什么使用B+Tree作为数据库索引？红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。 这样我们对比上面的B+树和红黑树，比如查找节点21，红黑树要磁盘IO5次，而B+树只要2次，也就是说磁盘IO次数大致为树的高度，这样B+树就脱颖而出了，成为实现索引的不二选择。 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree的高度一般都在2~4层。MySQL的InnoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要1~3次磁盘I/O操作。 数据库中的B+Tree索引可以分为聚集索引（clustered index）和辅助索引（secondary index）。 上面的B+Tree示例图在数据库中的实现即为聚集索引，聚集索引的B+Tree中的叶子节点存放的是整张表的行记录数据。辅助索引与聚集索引的区别在于辅助索引的叶子节点并不包含行记录的全部数据，而是存储相应行数据的聚集索引键，即主键。当通过辅助索引来查询数据时，InnoDB存储引擎会遍历辅助索引找到主键，然后再通过主键在聚集索引中找到完整的行记录数据。（InnoDB的索引实现原理） 聚簇索引(聚集索引)：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB的聚簇索引其实就是在同一个结构中保存了B-Tree索引(技术上来说是B+Tree)和数据行。 数据库索引采用B+树而不是B树或红黑树的主要原因： 方便扫库：B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的，而B树只能中序遍历所有节点，效率太低。 文件与数据库都是需要较大的存储，也就是说，它们都不可能全部存储在内存中，故需要存储到磁盘上。而所谓索引，则为了数据的快速定位与查找，那么索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数，因此B+树相比B树更为合适。数据库系统巧妙利用了局部性原理与磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入，而红黑树这种结构，高度明显要深的多，并且由于逻辑上很近的节点(父子)物理上可能很远，无法利用局部性。 为什么数据库选B-tree或B+tree而不是二叉查找树作为索引结构？ 磁盘IO与预读 磁盘读取依靠的是机械运动，分为寻道时间、旋转延迟、传输时间三个部分，这三个部分耗时相加就是一次磁盘IO的时间，大概9ms左右。这个成本是访问内存的十万倍左右；正是由于磁盘IO是非常昂贵的操作，所以计算机操作系统对此做了优化：预读 预读：每一次IO时，不仅仅把当前磁盘地址的数据加载到内存，同时也把相邻数据也加载到内存缓冲区中。因为局部预读原理说明，当访问一个地址数据的时候，与其相邻的数据很快也会被访问到。每次磁盘IO读取的数据我们称之为一页（page）。一页的大小与操作系统有关，一般为4k或者8k。这也就意味着读取一页内数据的时候，实际上发生了一次磁盘IO。 B-Tree与二叉查找树的对比 我们知道二叉查找树查询的时间复杂度是O(logN)，查找速度最快和比较次数最少，既然性能已经如此优秀，但为什么实现索引是使用B-Tree而不是二叉查找树，关键因素是磁盘IO的次数。 数据库索引是存储在磁盘上，当表中的数据量比较大时，索引的大小也跟着增长，达到几个G甚至更多。当我们利用索引进行查询的时候，不可能把索引全部加载到内存中，只能逐一加载每个磁盘页，这里的磁盘页就对应索引树的节点。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。而磁盘IO的次数和树的高度有关，因此，减少磁盘IO的次数就必须要压缩树的高度，让瘦高的树尽量变成矮胖的树，所以B-Tree就在这样伟大的时代背景下诞生了。 索引是存在于索引文件中，是存在于磁盘中的。因为索引通常是很大的，因此无法一次将全部索引加载到内存当中，因此每次只能从磁盘中读取一个磁盘页的数据到内存中。而这个磁盘的读取的速度较内存中的读取速度而言是差了好几个级别。 注意，我们说的平衡二叉树结构，指的是逻辑结构上的平衡二叉树，其物理实现是数组。然后由于在逻辑结构上相近的节点在物理结构上可能会差很远。因此，每次读取的磁盘页的数据中有许多是用不上的。因此，查找过程中要进行许多次的磁盘读取操作。 而适合作为索引的结构应该是尽可能少的执行磁盘IO操作，因为执行磁盘IO操作非常的耗时。因此，平衡二叉树并不适合作为索引结构。 为什么说B+树比B树更适合数据库索引？ B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小。如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。 B+树的查询效率更加稳定：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。 PS：我在知乎上看到有人是这样说的,我感觉说的也挺有道理的： 他们认为数据库索引采用B+树的主要原因是：B树在提高了IO性能的同时并没有解决元素遍历的我效率低下的问题，正是为了解决这个问题，B+树应用而生。B+树只需要去遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作或者说效率太低。 参考：https://blog.csdn.net/qq_35571554/article/details/82759668 数据库索引类型首先要明白索引（index）是在存储引擎（storage engine）层面实现的，而不是server层面。不是所有的存储引擎都支持所有的索引类型。 即使多个存储引擎支持某一索引类型，它们的实现和行为也可能有所差别。 MySQL里的索引类型主要有以下几种： 1. B-Tree索引 最常见的索引类型，基于B-Tree数据结构。B-Tree的基本思想是，所有值（被索引的列）都是排过序的，每个叶节点到跟节点距离相等。所以B-Tree适合用来查找某一范围内的数据，而且可以直接支持数据排序（ORDER BY）。但是当索引多列时，列的顺序特别重要，需要格外注意。InnoDB和MyISAM都支持B-Tree索引。InnoDB用的是一个变种B+Tree，而MyISAM为了节省空间对索引进行了压缩，从而牺牲了性能。 2. Hash索引 基于hash表。所以这种索引只支持精确查找，不支持范围查找，不支持排序。这意味着范围查找或ORDER BY都要依赖server层的额外工作。目前只有Memory引擎支持显式的hash索引（但是它的hash是nonunique的，冲突太多时也会影响查找性能）。Memory引擎默认的索引类型即是Hash索引，虽然它也支持B-Tree索引。 例子： 12345CREATE TABLE testhash ( fname VARCHAR(50) NOT NULL, lname VARCHAR(50) NOT NULL, KEY USING HASH(fname)) ENGINE =MEMORY; 3. Spatial (R-Tree)（空间）索引 只有MyISAM引擎支持，并且支持的不好。可以忽略。 4. Full-text索引 主要用来查找文本中的关键字，而不是直接与索引中的值相比较。Full-text索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的WHERE语句的参数匹配。你可以对某列分别进行full-text索引和B-Tree索引，两者互不冲突。Full-text索引配合MATCH AGAINST操作使用，而不是一般的WHERE语句加LIKE。 从数据结构的角度 B+树索引(O(logn)) 哈希索引 仅仅能满足”=”,”IN”和”&lt;&gt;”查询，不能使用范围查询。 其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。 只有Memory存储引擎显示支持hash索引。 FULLTEXT索引（现在MyISAM和InnoDB引擎都支持了） R-Tree索引（用于对GIS数据类型创建SPATIAL索引） 从物理存储角度 聚集索引（clustered index） 非聚集索引（non-clustered index） MyISAM的是非聚簇索引，InnoDB使用的是聚簇索引。 从逻辑角度 主键索引：主键索引是一种特殊的唯一索引，不允许有空值。 普通索引或者单列索引。 多列索引（复合索引）：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。注意：使用复合索引时遵循最左前缀。 唯一索引或者非唯一索引。 空间索引：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。MYSQL使用SPATIAL关键字进行扩展，使得能够用于创建正规索引类型的语法创建空间索引。创建空间索引的列，必须将其声明为NOT NULL，空间索引只能在存储引擎为MYISAM的表中创建。 12CREATE TABLE table_name[col_name data type][unique|fulltext|spatial][index|key][index_name](col_name[length])[asc|desc] 注意： 1、unique|fulltext|spatial为可选参数，分别表示唯一索引、全文索引和空间索引； 2、index和key为同义词，两者作用相同，用来指定创建索引； 3、col_name为需要创建索引的字段列，该列必须从数据表中该定义的多个列中选择； 4、index_name指定索引的名称，为可选参数，如果不指定，MYSQL默认col_name为索引值； 5、length为可选参数，表示索引的长度，只有字符串类型的字段才能指定索引长度； 6、asc或desc指定升序或降序的索引值存储。 索引的使用场景 匹配全值（match the full value） 对索引中所有列都指定具体值，即是对索引中的所有列都有等值匹配的条件。例如，租赁表 rental 中通过指定出租日期 rental_date + 库存编号 inventory_id + 客户编号 customer_id 的组合条件进行查询，执行计划的 key和extra两字段的值看到优化器选择了复合索引 idx_rental_date: 123456789101112131415MySQL [sakila]&gt; explain select * from rental where rental_date=&apos;2005-05-25 17:22:10&apos; and inventory_id=373 and customer_id=343 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: rental partitions: NULL type: constpossible_keys: rental_date,idx_fk_inventory_id,idx_fk_customer_id key: rental_date key_len: 10 ref: const,const,const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) explain 输出结果中字段 type 的值为 const，表示是常量；字段 key 的值为 rental_date, 表示优化器选择索引 rental_date 进行扫描。 匹配值的范围查询（match a range of values） 对索引的值能够进行范围查找。例如，检索租赁表 rental 中客户编号 customer_id 在指定范围内的记录： 123456789101112131415MySQL [sakila]&gt; explain select * from rental where customer_id &gt;= 373 and customer_id &lt; 400 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: rental partitions: NULL type: rangepossible_keys: idx_fk_customer_id key: idx_fk_customer_id key_len: 2 ref: NULL rows: 718 filtered: 100.00 Extra: Using index condition 1 row in set, 1 warning (0.05 sec) 类型 type 为 range 说明优化器选择范围查询，索引 key 为 idx_fk_customer_id 说明优化器选择索引 idx_fk_customer_id 来加速访问，注意到这个列子中 extra 列为 using index codition ，表示 mysql 使用了 ICP（using index condition） 来进一步优化查询。 匹配最左前缀（match a leftmost prefix） 最左匹配原则可以算是 MySQL 中 B-Tree 索引使用的首要原则。 仅仅对索引进行查询（index only query）,也叫索引覆盖 当查询的列都在索引的字段中时，查询的效率更高。 所以应该尽量避免使用 select *，需要哪些字段，就只查哪些字段。 匹配列前缀（match a column prefix） 仅仅使用索引中的第一列，并且只包含索引第一列的开头一部分进行查找。例如，现在需要查询出标题 title 是以 AFRICAN 开头的电影信息，从执行计划能够清楚看到，idx_title_desc_part 索引被利用上了： 12345678910111213141516171819MySQL [sakila]&gt; create index idx_title_desc_part on film_text(title (10), description(20));Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0MySQL [sakila]&gt; explain select title from film_text where title like &apos;AFRICAN%&apos;\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: film_text partitions: NULL type: rangepossible_keys: idx_title_desc_part,idx_title_description key: idx_title_desc_part key_len: 32 ref: NULL rows: 1 filtered: 100.00 Extra: Using where 1 row in set, 1 warning (0.00 sec) extra 值为 using where 表示优化器需要通过索引回表查询数据。 能够实现索引匹配部分精确而其他部分进行范围匹配（match one part exactly and match a range on another part） 例如，需要查询出租日期 rental_date 为指定日期且客户编号 customer_id 为指定范围的库存： 123456789101112131415MySQL [sakila]&gt; MySQL [sakila]&gt; explain select inventory_id from rental where rental_date=&apos;2006-02-14 15:16:03&apos; and customer_id &gt;= 300 and customer_id &lt;=400\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: rental partitions: NULL type: refpossible_keys: rental_date,idx_fk_customer_id key: rental_date key_len: 5 ref: const rows: 182 filtered: 16.85 Extra: Using where; Using index 1 row in set, 1 warning (0.00 sec) 如果列名是索引，那么使用 column_name is null 就会使用索引 例如，查询支付表 payment 的租赁编号 rental_id 字段为空的记录就用到了索引： 123456789101112131415MySQL [sakila]&gt; explain select * from payment where rental_id is null \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: payment partitions: NULL type: refpossible_keys: fk_payment_rental key: fk_payment_rental key_len: 5 ref: const rows: 5 filtered: 100.00 Extra: Using index condition 1 row in set, 1 warning (0.00 sec) 索引失效的情况 用 or 分割开的条件，如果 or 前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。 复合索引的情况下，假如查询条件不包含索引列最左边部分，即不满足最左前缀原则 ，是不会使用复合索引的。 like查询是以%开头。 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 最左前缀匹配原则 定义：就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边 生效原则：组合索引的生效原则：从前往后依次使用生效，如果中间某个索引没有使用，那么断点前面的索引部分起作用，断点后面的索引没有起作用。比如： 1234567891011121314151617181920212223where a=3 and b=45 and c=5 .... 这种三个索引顺序使用中间没有断点，全部发挥作用；where a=3 and c=5... 这种情况下b就是断点，a发挥了效果，c没有效果where b=3 and c=4... 这种情况下a就是断点，在a后面的索引都没有发挥作用，这种写法联合索引没有发挥任何效果；where b=45 and a=3 and c=5 .... 这个跟第一个一样，全部发挥作用，abc只要用上了就行，跟写的顺序无关(0) select * from mytable where a=3 and b=5 and c=4;abc三个索引都在where条件里面用到了，而且都发挥了作用(1) select * from mytable where c=4 and b=6 and a=3;这条语句列出来只想说明 mysql没有那么笨，where里面的条件顺序在查询之前会被mysql自动优化，效果跟上一句一样(2) select * from mytable where a=3 and c=7;a用到索引，b没有用，所以c是没有用到索引效果的(3) select * from mytable where a=3 and b&gt;7 and c=3;a用到了，b也用到了，c没有用到，这个地方b是范围值，也算断点，只不过自身用到了索引(4) select * from mytable where b=3 and c=4;因为a索引没有使用，所以这里 bc都没有用上索引效果(5) select * from mytable where a&gt;4 and b=7 and c=9;a用到了 b没有使用，c没有使用(6) select * from mytable where a=3 order by b;a用到了索引，b在结果排序中也用到了索引的效果，前面说了，a下面任意一段的b是排好序的(7) select * from mytable where a=3 order by c;a用到了索引，但是这个地方c没有发挥排序效果，因为中间断点了，使用 explain 可以看到 filesort(8) select * from mytable where b=3 order by a;b没有用到索引，排序中a也没有发挥索引效果 MyISAM和InnoDB的区别 MyISAM简介MyISAM是MySQL的默认数据库引擎（5.5版之前），由早期的 ISAM （Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但MyISAM不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（另一种数据库引擎）。 下面这张图只是想表达的意思是现在大多数时候我们使用的都是InnoDB存储引擎，但是在某些情况下使用MyISAM更好，比如：MyISAM更适合读密集的表，而InnoDB更适合写密集的的表。 在数据库做主从分离的情况下，经常选择MyISAM作为主库的存储引擎。 MyISAM特点 不支持行锁(MyISAM只有表锁)，读取时对需要读到的所有表加锁，写入时则对表加排他锁； 不支持事务 不支持外键 不支持崩溃后的安全恢复 在表有读取查询的同时，支持往表中插入新纪录 支持BLOB和TEXT的前500个字符索引，支持全文索引 支持延迟更新索引，极大地提升了写入性能 对于不会进行修改的表，支持压缩表 ，极大地减少了磁盘空间的占用 InnoDB的简介InnoDB是MySQL的默认数据库引擎（5.5版之后），2006年五月时由甲骨文公司并购。与传统的ISAM与 MyISAM相比，InnoDB的最大特色就是支持了ACID兼容的事务（Transaction）功能。 InnoDB的特点 支持行锁，采用MVCC来支持高并发，有可能死锁 支持事务 支持外键 支持崩溃后的安全恢复 不支持全文索引 读写阻塞与事务隔离级别相关 具有非常高效的缓存特性，能缓存索引，也能缓存数据 支持分区、表空间，类似Oracle数据库 关于二者的对比与总结3.1 二者的常见对比1) count运算上的区别： 因为MyISAM缓存有表meta-data（行数等），因此在做COUNT(*)时对于一个结构很好的查询是不需要消耗多少资源的。而对于InnoDB来说，则没有这种缓存。 2) 是否支持事务和崩溃后的安全恢复： MyISAM强调的是性能，每次查询具有原子性，其执行数度比InnoDB类型更快，但是不提供事务支持。InnoDB提供事务支持事务，外部键等高级数据库功能，具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 3)是否支持外键： MyISAM不支持，而InnoDB支持。 3.2 总结MyISAM更适合读密集的表，而InnoDB更适合写密集的的表。 在数据库做主从分离的情况下，经常选择MyISAM作为主库的存储引擎。 一般来说，如果需要事务支持，并且有较高的并发读取频率(MyISAM的表锁的粒度太大，所以当该表写并发量较高时，要等待的查询就会很多了)，InnoDB是不错的选择。如果你的数据量很大（MyISAM支持压缩特性可以减少磁盘的空间占用），而且不需要支持事务时，MyISAM是最好的选择。 参考：https://juejin.im/post/5b1685bef265da6e5c3c1c34 字符集及校对规则字符集指的是一种从二进制编码到某类字符符号的映射。 校对规则则是指某种字符集下的排序规则。 Mysql中每一种字符集都会对应一系列的校对规则。 Mysql采用的是类似继承的方式指定字符集的默认值，每个数据库以及每张数据表都有自己的默认值，他们逐层继承。比如：某个库中所有表的默认字符集将是该数据库所指定的字符集（这些表在没有指定字符集的情况下，才会采用默认字符集） PS：整理自《Java工程师修炼之道》 查询缓存的使用my.cnf加入以下配置，重启Mysql开启查询缓存 12query_cache_type=1query_cache_size=600000 Mysql执行以下命令也可以开启查询缓存 12set global query_cache_type=1;set global query_cache_size=600000; 如上，开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果。这里的查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息。因此任何两个查询在任何字符上的不同都会导致缓存不命中。此外，如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、Mysql库中的系统表，其查询结果也不会被缓存。 缓存建立之后，Mysql的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。 缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。 因此，开启缓存查询要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十MB比较合适。此外，还可以通过sql_cache和sql_no_cache来控制某个查询语句是否需要缓存： 1select sql_no_cache count(*) from usr; 数据库并发控制Mysql的大多数事务型存储引擎实现都不是简单的行级锁，基于并发性能考虑，一般都实现了MVCC多版本并发控制。 Mysql的并发控制是为了实现事务的隔离性，实现隔离性就要解决脏读、不可重复读、幻读问题。 Mysql的并发控制主要有两种方式，一种是多版本的并发控制(MVCC)，一种是基于锁的并发控制。 最为常见的三种并发控制机制： 悲观并发控制，悲观并发控制其实是最常见的并发控制机制，也就是锁； 乐观并发控制，乐观并发控制其实也有另一个名字：乐观锁，乐观锁其实并不是一种真实存在的锁； 多版本并发控制，与前两者对立的命名不同，MVCC 可以与前两者中的任意一种机制结合使用，以提高数据库的读性能。 并发控制要解决的问题：未提交的修改数据 脏读：一个事务读到了另一个事务尚未提交的数据。 不可重复读：同一个事务中两次读取的数据发生改变，这种改变是由另一个事务修改了对应记录引起的。 幻读：同一事务多次查询进行的时候，由于其他插入或删除操作的事务提交，导致每次返回不同的结果集(查到的数据增多或者减少) 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表 MVCC多版本并发控制MVCC是通过保存数据在某个时间点的快照来实现的。不管事务执行多长时间，事务看到的数据都是一致的。 MVCC最大的优点就是：读不加锁，读写不冲突。 MVCC的实现原理为了实现MVCC，InnoDB对每一行都加上了两个隐藏的列，其中一列存储行被修改的时系统的修改版本号，另外一列存储行被删除时的系统的删除版本号。每当一个事务开始的时候，InnoDB都会给这个事务分配一个递增的版本号，事务开始时的系统版本号会作为事务的版本号。 实际上，这个描述是很不严格的，问题有以下几点： 每条记录含有的隐藏列不是两个而是三个 它们分别是： DB_TRX_ID, 6byte, 创建这条记录/最后一次更新这条记录的事务ID DB_ROLL_PTR, 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） DB_ROW_ID, 6byte，隐含的自增ID，如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引 另外，每条记录的头信息（record header）里都有一个专门的bit（deleted_flag）来表示当前记录是否已经被删除 记录的历史版本是放在专门的rollback segment里（undo log） UPDATE非主键语句的效果是: 老记录被复制到rollback segment中形成undo log，DB_TRX_ID和DB_ROLL_PTR不动 新记录的DB_TRX_ID = 当前事务ID，DB_ROLL_PTR指向老记录形成的undo log 这样就能通过DB_ROLL_PTR找到这条记录的历史版本。如果对同一行记录执行连续的update操作，新记录与undo log会组成一个链表，遍历这个链表可以看到这条记录的变迁） Mysql的一致性读，是通过一个叫做read view的结构来实现的，下面介绍相关概念和可见性比较算法： read_view中维护了系统中活跃事务集合的快照，这些活跃事务ID的最小值为up_limit_id，最大值为low_limit_id。 设要读取的行的最后提交事务id(即当前数据行的稳定事务id)为 trx_id_current，当前新开事务id为 new_id，当前新开事务创建的快照read view 中最早的事务id为up_limit_id, 最迟的事务id为low_limit_id(注意这个low_limit_id=未开启的事务id=当前最大事务id+1) 1.trx_id_current &lt; up_limit_id, 这种情况比较好理解, 表示新事务在读取该行记录时, 该行记录的稳定事务ID是小于系统当前所有活跃的事务, 所以当前行稳定数据对新事务可见, 跳到步骤5. 2.trx_id_current &gt;= trx_id_last, 这种情况也比较好理解, 表示该行记录的稳定事务ID是在本次新事务创建之后才开启的, 但是却在本次新事务执行第二个select前就commit了，所以该行记录的当前值不可见, 跳到步骤4。 3.trx_id_current &lt;= trx_id_current &lt;= trx_id_last, 表示该行记录所在事务在本次新事务创建的时候处于活动状态，从up_limit_id到low_limit_id进行遍历，如果trx_id_current等于他们之中的某个事务id的话，那么不可见，调到步骤4，否则表示可见，调到步骤5。 4.从该行记录的 DB_ROLL_PTR 指针所指向的回滚段中取出最新的undo-log的版本号, 将它赋值该 trx_id_current，然后跳到步骤1重新开始判断。 5.将该可见行的值返回。 辅助索引实现MVCC 刚才讲的内容是基于聚簇索引的，只有聚簇索引中含有DB_TRX_ID与DB_ROLL_PTR隐藏列，可以比较容易的实现MVCC，但是辅助中并不含有这几个隐藏列，只含有1个bit的deleted flag。 purge线程 从前面的分析可以看出，为了实现InnoDB的MVCC机制，更新或者删除操作都只是设置一下老记录的deleted_bit，并不真正将过时的记录删除。 为了节省磁盘空间，InnoDB有专门的purge线程来清理deleted_bit为true的记录。 为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view） 如果某个记录的deleted_bit为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。 补充概念 多版本控制: 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。在内部实现中，与Postgres在数据行上实现多版本不同，InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性(up_limit_id和low_limit_id)。 read view：主要是用来做可见性判断的, 比较普遍的解释便是”本事务不可见的当前其他活跃事务”。 对于read view快照的生成时机, 也非常关键, 正是因为生成时机的不同, 造成了RC,RR两种隔离级别的不同可见性; 在innodb中(默认repeatable read级别), 事务在begin/start transaction之后的第一条select读操作后, 会创建一个快照(read view), 将当前系统中活跃的其他事务记录记录起来; 在innodb中(默认repeatable committed级别), 事务中每条select语句都会创建一个快照(read view); undo_log Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，Undo记录默认被记录到系统表空间(ibdata)中，但从5.6开始，也可以使用独立的Undo 表空间。 Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作（例如bug#69812）。 大多数对数据的变更操作包括INSERT/DELETE/UPDATE，其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除（谁会对刚插入的数据有可见性需求呢！！），而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo 另外, 在回滚段中的undo logs分为: insert undo log 和 update undo log insert undo log : 事务对insert新记录时产生的undolog, 只在事务回滚时需要, 并且在事务提交后就可以立即丢弃。 update undo log : 事务对记录进行delete和update操作时产生的undo log, 不仅在事务回滚时需要, 一致性读也需要，所以不能随便删除，只有当数据库所使用的快照中不涉及该日志记录，对应的回滚日志才会被purge线程删除。 读操作分成两类：快照读和当前读。 快照读：简单的select操作属于快照读，不加锁。 1select * from table where ? 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。 1234select * from table where ? lock in share mode ;select * from table where ? for update ;update table set ? where ? ;delete from table where ? ; 乐观并发控制一般采用以下方式：使用版本号(version)机制来实现，版本号就是为数据添加一个版本标志，一般在表中添加一个version字段；当读取数据的时候把version也取出来，然后version+1，更新数据库的时候对比第一次取出来的version和数据库里面的version是否一致，如果一致则更新成功，否则失败进入重试，具体使用大致如下： 12345begin;select id,name,version from test_lock where id=1;....update test_lock set name='xxx',version=version+1 where id=1 and version=$&#123;version&#125;;commit; 先查询后更新，需要保证原子性，要么使用悲观锁的方式，对整个事务加锁；要么使用乐观锁的方式，如果在读多写少的系统中，乐观锁性能更好； 事务机制关系型数据库需要遵循ACID原则，具体内容如下： 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据库从一个一致性状态转换到另一个一致性状态。 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 为了达到上述事务特性，数据库定义了几种不同的事务隔离级别： READ_UNCOMMITTED（读未提交）: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 READ_COMMITTED（读已提交）: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 REPEATABLE_READ（可重复读）: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE（串行化）: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别。 锁机制与InnoDB锁算法MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking) InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁 按照锁的粒度把数据库锁分为表级锁和行级锁 表级锁： Mysql中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。 行级锁： Mysql中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。InnoDB支持的行级锁，包括如下几种： Record Lock: 对索引项加锁，锁定符合条件的行。其他事务不能修改和删除加锁项； Gap Lock: 对索引项之间的“间隙”加锁，锁定记录的范围（对第一条记录前的间隙或最后一条将记录后的间隙加锁），不包含索引项本身。其他事务不能在锁范围内插入数据，这样就防止了别的事务新增幻影行。 Next-key Lock： 锁定索引项本身和索引范围。即Record Lock和Gap Lock的结合，可解决幻读问题。 虽然使用行级索具有粒度小、并发度高等特点，但是表级锁有时候也是非常必要的： 事务更新大表中的大部分数据直接使用表级锁效率更高； 事务比较复杂，使用行级索很可能引起死锁导致回滚。 补充： 页级锁： MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。页级进行了折衷，一次锁定相邻的一组记录。BDB支持页级锁。开销和加锁时间界于表锁和行锁之间，会出现死锁。锁定粒度界于表锁和行锁之间，并发度一般。 按照是否可写进一步划分为共享锁(S锁)和排他锁(X锁)表级锁和行级锁可以进一步划分为共享锁（S）和排他锁（X） 共享锁（S） 共享锁（Share Locks，简记为S）又被称为读锁，其他用户可以并发读取数据，但任何事务都不能获取数据上的排他锁，直到已释放所有共享锁。 共享锁(S锁)又称为读锁，若事务T对数据对象A加上S锁，则事务T只能读A；其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这就保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。 排他锁（X）： 排它锁（(Exclusive lock,简记为X锁)）又称为写锁，若事务T对数据对象A加上X锁，则只允许T读取和修改A，其它任何事务都不能再对A加任何类型的锁，直到T释放A上的锁。它防止任何其它事务获取资源上的锁，直到在事务的末尾将资源上的原始锁释放为止。在更新操作(INSERT、UPDATE 或 DELETE)过程中始终应用排它锁。 两者之间的区别： 共享锁（S锁）：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，而不能加排他锁。获取共享锁的事务只能读数据，不能修改数据。 排他锁（X锁）：如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获取排他锁的事务既能读数据，又能修改数据。 共享锁下其它用户可以并发读取，查询数据，但不能修改，增加，删除数据，即资源共享。 另外两种表级锁：IS和IX当一个事务需要给自己需要的某个资源加锁的时候，如果遇到一个共享锁正锁定着自己需要的资源的时候，自己可以再加一个共享锁，不过不能加排他锁。但是，如果遇到自己需要锁定的资源已经被一个排他锁占有之后，则只能等待该锁定释放资源之后自己才能获取锁定资源并添加自己的锁定。而意向锁的作用就是当一个事务在需要获取的资源锁定的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。 InnoDB另外的两个表级锁： 意向共享锁（IS）： 表示事务准备给数据行记入共享锁，事务在一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）： 表示事务准备给数据行加入排他锁，事务在一个数据行加排他锁前必须先取得该表的IX锁。 注意： 这里的意向锁是表级锁，表示的是一种意向，仅仅表示事务正在读或写某一行记录，在真正加行锁时才会判断是否冲突。意向锁是InnoDB自动加的，不需要用户干预。 IX，IS是表级锁，不会和行级的X，S锁发生冲突，只会和表级的X，S发生冲突。 InnoDB的锁机制兼容情况如下： 当一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；反之如果请求不兼容，则该事物就等待锁释放。 死锁和避免死锁InnoDB的行级锁是基于索引实现的，如果查询语句为命中任何索引，那么InnoDB会使用表级锁。 此外，InnoDB的行级锁是针对索引加的锁，不针对数据记录，因此即使访问不同行的记录，如果使用了相同的索引键仍然会出现锁冲突。 另外，还需要注意，在通过以下情况使用锁的时候，如果表没有定义任何索引，那么InnoDB会创建一个隐藏的聚簇索引并使用这个索引来加记录锁。 12SELECT ...LOCK IN SHARE MODE;SELECT ...FOR UPDATE; 死锁产生：不同于MyISAM总是一次性获得所需的全部锁，InnoDB的锁是逐步获得的，当两个事务都需要获得对方持有的锁，导致双方都在等待，这就产生了死锁。 发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个则可以获取锁完成事务。 避免死锁： 通过表级锁来减少死锁产生的概率； 多个程序尽量约定以相同的顺序访问表（这也是解决并发理论中哲学家就餐问题的一种思路）； 同一个事务尽可能做到一次锁定所需要的所有资源。 大表优化当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 垂直分区： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起join操作，可以通过在应用层进行join来解决。此外，垂直分区会让事务变得更加复杂； 水平分区： 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。 水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。]]></content>
      <categories>
        <category>java面试准备</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络常见面试题]]></title>
    <url>%2F2019%2F02%2F23%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[常见状态码及原因短语 HTTP请求结构： 请求方式 + 请求URI + 协议及其版本 HTTP响应结构： 状态码 + 原因短语 + 协议及其版本 1×× : 请求处理中，请求已被接受，正在处理 2×× : 请求成功，请求被成功处理200 OK 3×× ：重定向，要完成请求必须进行进一步处理301 ：永久性转移302 ：暂时性转移304 ：所请求的资源未修改，服务器返回此状态码时，不会返回任何资源 4××：客户端错误，请求不合法400：Bad Request,请求有语法问题401：请求要求用户的身份认证403：服务器理解请求客户端的请求，但是拒绝执行此请求404：客户端所访问的页面不存在 5××：服务器端错误，服务器不能处理合法请求500：服务器内部错误502：充当网关或代理的服务器，从远端服务器接收到了一个无效的请求503：服务器暂时的无法处理客户端的请求 计算机网络中各层传递的数据形式 物理层：比特流 数据链路层：帧 网络层：数据报、分组 传输层：数据段 应用层：数据、消息 总结： 在每一层都有一个信息传输单元，叫信息分组（即上面提到的分组一般性的意思），每层具体的PDU叫法是由最初始的信息分组在各层经过封装后形成的。进入应用层加上应用层的报头后成为最初始的信息分组，叫信息，由传输层提供报文交付（下层为上层服务），应用层报文传到传输层。进入传输层，报文被分割固定长度，加上传输层报头，成为数据段。数据段向下传到网络层，加上网络层报头，包含路由信息，成为分组或数据报，网络层实现分组交付。继续向下传输，进入数据链路层，加上数据链路层头部和尾部，形成帧，将源自网络层来的数据可靠地传输到相邻节点的目标机网络层。进入物理层，将数据转换成物理链路中模拟量即bit进行传输。到达接收端是一个相反的过程。 TCP三次握手和四次挥手三次握手 客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有 ACK 标志的数据包–三次握手–服务端 四次挥手 断开一个 TCP 连接则需要“四次挥手”： 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送。 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号。 服务器-关闭与客户端的连接，发送一个FIN给客户端。 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1。 为什么TCP需要三次握手，四次挥手？为什么需要三次握手？“三次握手”的目的是“为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误”。 client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后 的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为 是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”， 那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server 的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样， server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不 会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。主要目的防止server端一直等待，浪费资源。 为什么需要四次挥手？因为TCP有个半关闭状态，假设A.B要释放连接，那么A发送一个释放连接报文给B，B收到后发送确认，这个时候A不发数据，但是B如果发数据A还是要接受，这叫半关闭。然后B还要发给A连接释放报文，然后A发确认，所以是4次。 为什么在TCP连接握手时为何ACK是和SYN一起发送，这里ACK却没有和FIN一起发送呢？因为tcp是全双工模式，接收到FIN时意味将没有数据再发来，但是还是可以继续发送数据。 为什么四次挥手最后需要2MSL的TIME-WAIT时间，才进入CLOSED状态？第一、为了保证发送方发送的最后一个ACK报文段能够到达接收方。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的接收方收不到对已发送的FIN+ACK报文段的确认。接收方会超时重传这个FIN+ACK报文段，而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。接着A重传一次确认，重新启动2MSL计时器。 第二、防止“已失效的连接请求报文段”出现在本连接中。A在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。 TCP和UDP有什么区别？UDP的主要特点 UDP是无连接的； UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP是面向报文的； UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）； UDP支持一对一、一对多、多对一和多对多的交互通信； UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 TCP的主要特点 TCP是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）； TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； TCP是面向字节流的。TCP中的“流”（stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 1). TCP对应的应用层协议 FTP：定义了文件传输协议，使用21端口。常说某某计算机开了FTP服务便是启动了文件传输服务。下载文件，上传主页，都要用到FTP服务。 Telnet：它是一种用于远程登陆的端口，用户可以以自己的身份远程连接到计算机上，通过这种端口可以提供一种基于DOS模式下的通信服务。如以前的BBS是-纯字符界面的，支持BBS的服务器将23端口打开，对外提供服务。 SMTP：定义了简单邮件传送协议，现在很多邮件服务器都用的是这个协议，用于发送邮件。如常见的免费邮件服务中用的就是这个邮件服务端口，所以在电子邮件设置-中常看到有这么SMTP端口设置这个栏，服务器开放的是25号端口。 POP3：它是和SMTP对应，POP3用于接收邮件。通常情况下，POP3协议所用的是110端口。也是说，只要你有相应的使用POP3协议的程序（例如Fo-xmail或Outlook），就可以不以Web方式登陆进邮箱界面，直接用邮件程序就可以收到邮件（如是163邮箱就没有必要先进入网易网站，再进入自己的邮-箱来收信）。 HTTP：从Web服务器传输超文本到本地浏览器的传送协议。 2). UDP对应的应用层协议 DNS：用于域名解析服务，将域名地址转换为IP地址。DNS用的是53号端口。 SNMP：简单网络管理协议，使用161号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。 TFTP(Trival File Transfer Protocal)：简单文件传输协议，该协议在熟知端口69上使用UDP服务。 交换机和路由器有什么区别？①工作所处的OSI层次不一样，交换机工作在OSI第二层数据链路层，路由器工作在OSI第三层网络层 ②寻址方式不同：交换机根据MAC地址寻址，路由器根据IP地址寻址 ③转发速不同：交换机的转发速度快，路由器转发速度相对较慢 交换机、路由器、网关、网桥的概念及各自用途？ 交换机用于局域网，利用主机的 MAC 地址进行数据传输，而不需要关心 IP 数据包中的 IP 地址，它工作于数据链路层。 路由器识别网络是通过 IP 数据包中 IP 地址的网络号进行的，所以为了保证数据包路由的正确性，每个网络都必须有一个唯一的网络号。路由器通过 IP 数据包的 IP 地址进行路由选择的（将数据包递交给哪个下一跳路由器），路由器工作于网络层。 网关就是连接两个网络的设备，能在不同协议间移动数据。作用于网络层以上。 网桥是一个局域网与另一个局域网之间建立连接的桥梁。属于数据链路层的一种设备。 ARP是地址解析协议，简单语言解释一下工作原理。ARP协议：通过目的的IP地址来获取MAC地址。 首先，每个主机都会在自己的ARP缓冲区中建立一个ARP列表，以表示IP地址和MAC地址之间的对应关系。 当源主机要发送数据时，首先检查ARP列表中是否有对应IP地址的目的主机的MAC地址，如果有，则直接发送数据，如果没有，就向本网段的所有主机发送ARP数据包，该数据包包括的内容有：源主机IP地址，源主机MAC地址，目的主机的IP地址。 当本网络的所有主机收到该ARP数据包时，首先检查数据包中的IP地址是否是自己的IP地址，如果不是，则忽略该数据包，如果是，则首先从数据包中取出源主机的IP和MAC地址写入到ARP列表中，如果已经存在，则覆盖，然后将自己的MAC地址写入ARP响应包中，告诉源主机自己是它想要找的MAC地址。 源主机收到ARP响应包后。将目的主机的IP和MAC地址写入ARP列表，并利用此信息发送数据。如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。 注意：广播发送ARP请求，单播发送ARP响应。 ICMP协议？ICMP是InternetControl Message Protocol，因特网控制报文协议。它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由器是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。ICMP报文有两种：差错报告报文和询问报文。 DHCP协议？动态主机配置协议，是一种让系统得以连接到网络上，并获取所需要的配置参数手段。通常被应用在大型的局域网络环境中，主要作用是集中的管理、分配IP地址，使网络环境中的主机动态的获得IP地址、Gateway地址、DNS服务器地址等信息，并能够提升地址的使用率。 HTTP的长连接和短连接?HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议。 短连接:浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。 长连接:当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭。如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。 TCP短连接: client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起close操作。短连接一般只会在 client/server间传递一次读写操作。 TCP长连接: client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 为什么TIME_WAIT状态还需要等2MSL（Max SegmentLifetime，最大分段生存期）秒之后才能返回到CLOSED状态呢？因为虽然双方都同意关闭连接了，而且握手的4个报文也都发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SENT状态到ESTABLISH状态那样），但是我们必须假想网络是不可靠的，你无法保证你最后发送的ACK报文一定会被对方收到，就是说对方处于LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文。所以这个TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。 IO中同步与异步，阻塞与非阻塞区别同步和异步关注的是消息通信机制 (synchronous communication/asynchronous communication)所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。 阻塞调用是指调用结果返回之前，当前线程会被挂起。函数只有在得到结果之后才会返回。 非阻塞：不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。 IP地址的分类IP地址根据网络号和主机号来分，分为A、B、C三类及特殊地址D、E。全0和全1的都保留不用。（全0表示网络地址，全1表示广播地址）A类：第一个字节为网络号，后三个字节为主机号。该类IP地址的最前面为“0”，所以地址的网络号取值于1~126之间。一般用于大型网络。B类：前两个字节为网络号，后两个字节为主机号。该类IP地址的最前面为“10”，所以地址的网络号取值于128~191之间。一般用于中等规模网络。C类：前三个字节为网络号，最后一个字节为主机号。该类IP地址的最前面为“110”，所以地址的网络号取值于192~223之间。一般用于小型网络。D类地址：以1110开头，第一个字节范围为224~239；多播地址E类地址：以1111开头，保留今后使用网络号（net-id）：标记主机（或路由器）所连接的网络；主机号（host-id）：标记主机（或路由器）。 IP地址与子网掩码相与得到网络号。 TCP/IP中，各层协议的简单介绍？ 数据链路层：PPP协议（点对点协议），SCMA/CD （带冲突检测的载波监听多路访问技术） 网络层：IP协议、ICMP协议（网际控制报文协议）、IGMP（网际组管理协议）、ARP协议、RARP协议，OSPF（开放路径最短优先协议）。ICMP协议： 因特网控制报文协议。它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。 传输层：UDP协议、TCP协议。 应用层：FTP（文件传送协议）、Telenet（远程登录协议）、DNS（域名解析协议）、SMTP（邮件传送协议），POP3协议（邮局协议），HTTP协议。TFTP协议： 是TCP/IP协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。HTTP协议： 超文本传输协议，是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。NAT协议：网络地址转换属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技术。 DHCP协议（动态主机配置协议）：一个局域网的网络协议，使用UDP协议工作。用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 在浏览器中输入www.baidu.com后执行的全部过程？ 浏览器查询DNS，获取域名对应的IP地址：具体过程包括浏览器搜索自身的DNS缓存、搜索操作系统的DNS缓存、读取本地的Host文件、检查路由器缓存和向本地DNS服务器进行查询(搜索本地DNS服务器缓存)等。对于向本地DNS服务器进行查询，如果要查询的域名由本地DNS服务器区域解析，则返回解析结果给客户机，完成域名解析(此解析具有权威性)；如果要查询的域名不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析（此解析不具有权威性）。如果本地域名服务器并未缓存该网址映射关系，那么将根据其设置发起递归查询或者迭代查询； 浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立链接，发起三次握手； TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求； 服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器； 浏览器解析并渲染视图，如果遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤，向服务器请求这些资源； 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面； 浏览器向服务器(目的主机)发起四次挥手，请求断开连接。 DNS的工作原理？ 应用进程将待解析的域名放在DNS请求报文中，以UDP数据报的形式发送给本地域名服务器，本地域名服务器查找到相应域名的IP地址后（主机向本地域名服务器的查询一般都是采用递归查询），就将该域名的IP地址信息放入应答报文中返回给客户进程。 如果主机所询问的本地域名服务器不知道被查询域名的IP地址，那么本地域名服务器就以DNS客户的身份，向其他域名服务器继续发送查询请求报文（本地域名服务器向根域名服务器的查询通常采用迭代查询）。 当根域名服务器收到本地域名服务器的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地域名服务器：“你下一步应当向哪一个域名服务器查询”。然后让本地域名服务器进行后续的查询。 Cookie和Session的区别概述 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session。典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。集群的时候也要考虑Session的转移，在大型的网站，一般会有专门的Session服务器集群，用来保存用户会话，这个时候 Session 信息都是放在内存的，使用一些缓存服务比如Memcached之类的来放 Session。 思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。 所以，总结一下：Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中。Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 总结 session 在服务器端，cookie 在客户端（浏览器） session 默认被存在在服务器的一个文件里（不是内存） session 的运行依赖 session id，而 session id 是存在 cookie 中的，也就是说，如果浏览器禁用了 cookie，同时 session 也会失效（但是可以通过其它方式实现，比如在 url 中传递 session_id） session 可以放在文件、数据库、或内存中都可以。 用户验证这种场合一般会用 session 论述一、cookie和session cookie和session都是用来跟踪浏览器用户身份的会话方式。 区别： 1、保持状态：cookie保存在浏览器端，session保存在服务器端 2、使用方式： （1）cookie机制：如果不在浏览器中设置过期时间，cookie被保存在内存中，生命周期随浏览器的关闭而结束，这种cookie简称会话cookie。如果在浏览器中设置了cookie的过期时间，cookie被保存在硬盘中，关闭浏览器后，cookie数据仍然存在，直到过期时间结束才消失。Cookie是服务器发给客户端的特殊信息，cookie是以文本的方式保存在客户端，由对服务器的请求来传递，每次都会携带在HTTP请求头中，如果使用cookie保存过多数据会带来性能问题。 （2）session机制：当服务器收到请求需要创建session对象时，首先会检查客户端请求中是否包含sessionid。如果有sessionid，服务器将根据该id返回对应session对象。如果客户端请求中没有sessionid，服务器会创建新的session对象，并把sessionid在本次响应中返回给客户端。通常使用cookie方式存储sessionid到客户端，在交互中浏览器按照规则将sessionid发送给服务器。如果用户禁用cookie，则要使用URL重写，可以通过response.encodeURL(url) 进行实现；API对encodeURL的结束为，当浏览器支持Cookie时，url不做任何处理；当浏览器不支持Cookie的时候，将会重写URL将SessionID拼接到访问地址后。 3、存储内容：cookie只能保存字符串类型，以文本的方式；session通过类似与Hashtable的数据结构来保存，能支持任何类型的对象(session中可含有多个对象) 4、存储的大小：cookie：单个cookie保存的数据不能超过4kb；session大小没有限制。 5、安全性：cookie：针对cookie所存在的攻击：Cookie欺骗，Cookie截获；session的安全性大于cookie。 原因如下： （1）sessionID存储在cookie中，若要攻破session首先要攻破cookie； （2）sessionID是要有人登录，或者启动session_start才会有，所以攻破cookie也不一定能得到sessionID； （3）第二次启动session_start后，前一次的sessionID就是失效了，session过期后，sessionID也随之失效。 （4）sessionID是加密的 （5）综上所述，攻击者必须在短时间内攻破加密的sessionID，这很难。 6、应用场景： cookie： （1）判断用户是否登陆过网站，以便下次登录时能够实现自动登录（或者记住密码）。如果我们删除cookie，则每次登录必须从新填写登录的相关信息。 （2）保存上次登录的时间等信息。 （3）保存上次查看的页面 （4）浏览计数 session：Session用于保存每个用户的专用信息，变量的值保存在服务器端，通过SessionID来区分不同的客户。 （1）网上商城中的购物车 （2）保存用户登录信息 （3）将某些数据放入session中，供同一用户的不同页面使用 （4）防止用户非法登录 7、缺点： cookie： 大小受限 用户可以操作（禁用）cookie，使功能受限 安全性较低 有些状态不可能保存在客户端。 每次访问都要传送cookie给服务器，浪费带宽。 cookie数据有路径（path）的概念，可以限制cookie只属于某个路径下。 session： Session保存的东西越多，就越占用服务器内存，对于用户在线人数较多的网站，服务器的内存压力会比较大。 依赖于cookie（sessionID保存在cookie），如果禁用cookie，则要使用URL重写，不安全。 创建Session变量有很大的随意性，可随时调用，不需要开发者做精确地处理，所以，过度使用session变量将会导致代码不可读而且不好维护。 下面是关于Cookie生命周期的介绍：Cookie如果不设置过期时间（使用setMaxAge()方法），则表示这个cookie生命周期为浏览器会话期间，只要关闭浏览器窗口，cookie就消失了。这种生命期为浏览会话期的cookie被称为会话cookie。会话cookie一般不保存在硬盘上而是保存在内存里。如果设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie依然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存的cookie，不同的浏览器还有不同的处理方式。 HTTPS的实现原理 废话一下：其实我也不知道自己理解的是不是对的，我觉得网上很多讲Https原理的文章其实本质上讲的都是SSL/TLS的原理。在我看来，Https指的是Http协议+ SSL/TLS协议,也就是说，Http协议本来定义了数据的包装方式（要包含请求头，请求方式等），现在在这个基础上，还要求行对数据进行加密，对通信双方身份进行验证（如何加密，如何验证由SSL决定）。 我也不知道我理解的对不对？求高人赐教。 下面上图： 补充： 非对称加密： 非对称加密（asymmetric cryptography），一种密码学算法类型，在这种密码学方法中，需要一对密钥，一个是私人密钥，另一个则是公开密钥。这两个密钥是数学相关，用某用户密钥加密后所得的信息，只能用该用户的解密密钥才能解密。 比如说： 我有一对密钥，一个是公钥(假设是’a’），一个是私钥（假设是’b’），现在我用’a’对一个内容加了密，那么，此时，，你只有用私钥’b’对它进行解密才能看到正确内容。同理，如果我用私钥’b’加了密，你只有用公钥’a’解密才能看到内容。 对称加密： 对称密钥加密（英语：Symmetric-key algorithm）又称为对称加密、私钥加密、共享密钥加密，是密码学中的一类加密算法。这类算法在加密和解密时使用相同的密钥，或是使用两个可以简单地相互推算的密钥。 对称加密其实就是说： 比如我用’a’作为密钥对内容加了密，你必须也得用’a’作为密钥对加密后的内容进行解密。 HTTP协议特点1.HTTP协议是无状态的 ​ 就是说每次HTTP请求都是独立的，任何两个请求之间没有什么必然的联系。但是在实际应用当中并不是完全这样的，引入了Cookie和Session机制来关联请求。 2.多次HTTP请求 在客户端请求网页时多数情况下并不是一次请求就能成功的，服务端首先是响应HTML页面，然后浏览器收到响应之后发现HTML页面还引用了其他的资源，例如，CSS，JS文件，图片等等，还会自动发送HTTP请求这些需要的资源。现在的HTTP版本支持管道机制，可以同时请求和响应多个请求，大大提高了效率。 3.基于TCP协议 HTTP协议目的是规定客户端和服务端数据传输的格式和数据交互行为，并不负责数据传输的细节。底层是基于TCP实现的。现在使用的版本当中是默认持久连接的，也就是多次HTTP请求使用一个TCP连接。 HTTP报文请求报文 1234567891011GET /wxisme HTTP/1.1 Host: www.cnblogs.com User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; zh-CN; rv:1.8.1) Gecko/20061010 Firefox/2.0 Accept: text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5 Accept-Language: en-us,zh-cn;q=0.7,zh;q=0.3 Accept-Encoding: gzip,deflate Accept-Charset: gb2312,utf-8;q=0.7,*;q=0.7 Keep-Alive: 300 Proxy-Connection: keep-alive Cookie: ASP.NET_SessionId=ey5drq45lsomio55hoydzc45Cache-Control: max-age=0 响应报文 12345678910HTTP/1.1 200 OKDate: Tue, 12 Jul 2016 21:36:12 GMTContent-Length: 563Content-Type: text/html&lt;html&gt; &lt;body&gt; Hello http! &lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>java面试准备</category>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类的加载机制和对象的创建]]></title>
    <url>%2F2019%2F02%2F18%2F%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%92%8C%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%9B%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[JVM类加载机制 定义：在代码编译后，就会生成JVM（Java虚拟机）能够识别的二进制字节流文件（*.class）。而JVM把Class文件中的类描述数据从文件加载到内存，并对数据进行校验、转换解析、初始化，使这些数据最终成为可以被JVM直接使用的Java类型，这个说来简单但实际复杂的过程叫做JVM的类加载机制。 Class文件中的“类”从加载到JVM内存中，到卸载出内存过程有七个生命周期阶段。类加载机制包括了前五个阶段。如下图所示： 过程： 类的加载：我们平常说的加载大多不是指的类加载机制，只是类加载机制中的第一步加载。在这个阶段，JVM主要完成三件事： 1、通过一个类的全限定名（包名与类名）来获取定义此类的二进制字节流（Class文件）。而获取的方式，可以通过jar包、war包、网络中获取、JSP文件生成等方式。 2、将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。这里只是转化了数据结构，并未合并数据。（方法区就是用来存放已被加载的类信息，常量，静态变量，编译后的代码的运行时内存区域） 3、在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。这个Class对象并没有规定是在Java堆内存中，它比较特殊，虽为对象，但存放在方法区中。 类的连接：类的加载过程后生成了类的java.lang.Class对象，接着会进入连接阶段，连接阶段负责将类的二进制数据合并入JRE（Java运行时环境）中。类的连接大致分三个阶段。 1、验证：验证被加载后的类是否有正确的结构，类数据是否会符合虚拟机的要求，确保不会危害虚拟机安全。 2、准备：为类的静态变量（static filed）在方法区分配内存，并赋默认初值（0值或null值）。对于非静态的变量，则不会为它们分配内存。 如static int a = 100，静态变量a就会在准备阶段被赋默认值0。 对于一般的成员变量是在类实例化时候，随对象一起分配在堆内存中。 另外，静态常量（static final filed）会在准备阶段赋程序设定的初值。 如static final int a = 666，静态常量a就会在准备阶段被直接赋值为666；对于静态变量，这个操作是在初始化阶段进行的。 3、解析：将类的二进制数据中的符号引用换为直接引用。 类的初始化：初始化阶段是根据用户程序中的初始化语句为类的静态变量赋予正确的初始值。这里初始化执行逻辑最终会体现在类构造器方法clinit方法中。该方法由编译器在编译阶段生成，它封装了两部分内容:静态变量的初始化语句和静态语句块。 类的初始化的主要工作是为静态变量赋程序设定的初值。 如static int a = 100;在准备阶段，a被赋默认值0，在初始化阶段就会被赋值为100。 初始化执行时机：jvm规范明确规定了初始化执行条件，只要满足以下四个条件之一，就会执行初始化工作(1) 通过new关键字实例化对象、读取或设置类的静态变量、调用类的静态方法(对应new,getstatic,putstatic,invokespecial这四条字节码指令）。 (2) 通过反射方式执行以上行为时。 (3) 初始化子类的时候，会触发父类的初始化。 (4) 作为程序入口直接运行时的主类。 初始化过程：初始化过程包括两步：(1) 如果类存在直接父类，并且父类没有被初始化则对直接父类进行初始化。 (2) 如果类当前存在clinit方法，则执行clinit方法。 需要注意的是接口(interface)的初始化并不要求先初始化它的父接口，只有当使用父接口的变量的时候才会进行初始化。(接口中不能有static块，但可以有变量初始化) clinit方法存在的条件：并不是每个类都有clinit方法,如下情况下不会有clinit方法：a. 类没有静态变量也没有静态语句块 b.类中虽然定义了静态变量，但是没有给出明确的初始化语句。 c.如果类中仅包含了final static的静态变量的初始化语句，而且初始化语句采用编译时常量表达时，也不会有clinit方法。 类的主动引用和被动引用的区别 类的主动引用（一定会发生类的初始化） —— new一个类的对象 —— 调用类的静态成员(除了final常量)和静态方法 —— 使用java.lang.reflect包的方法对类进行反射调用 —— 当虚拟机启动，先启动main方法所在的类 —— 当初始化一个类，如果父类没有被初始化，则先初始化它的父类 类的被动引用（不会发生类的初始化） —— 当访问一个静态域时，只有真正声明这个域的类才会被初始化 ​ 通过子类引用父类的静态变量，不会导致子类初始化 —— 通过数组定义类引用，不会触发此类的初始化 —— 引用final常量不会触发此类的初始化（常量在编译阶段就存入类的常量池中） 类加载器：类加载器的作用不仅仅是实现类的加载，它还与类的的“相等”判定有关，关系着Java“相等”判定方法的返回结果，只有在满足如下三个类“相等”判定条件，才能判定两个类相等： 1、两个类来自同一个Class文件 2、两个类是由同一个虚拟机加载 3、两个类是由同一个类加载器加载 JVM类加载器分类详解： 1、Bootstrap ClassLoader：启动类加载器，也叫根类加载器，它负责加载Java的核心类库，加载如(%JAVA_HOME%/lib)目录下的rt.jar（包含System、String这样的核心类）这样的核心类库。根类加载器非常特殊，它不是java.lang.ClassLoader的子类，它是JVM自身内部由C/C++实现的，并不是Java实现的。 2、Extension ClassLoader：扩展类加载器，它负责加载扩展目录(%JAVA_HOME%/jre/lib/ext)下的jar包，用户可以把自己开发的类打包成jar包放在这个目录下即可扩展核心类以外的新功能。 3、System ClassLoader\APP ClassLoader：系统类加载器或称为应用程序类加载器，是加载CLASSPATH环境变量所指定的jar包与类路径。一般来说，用户自定义的类就是由APP ClassLoader加载的。 各种类加载器间关系：参考ClassLoader源代码会发现，这些Class之间并不是采用继承的方式实现父子关系，而是采用组合方式。 类加载器的双亲委派加载机制：当一个类收到了类加载请求，他首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成，每一个层次类加载器都是如此，因此所有的加载请求都应该传送到启动类加载其中，只有当父类加载器反馈自己无法完成这个请求的时候（在它的加载路径下没有找到所需加载的Class），子类加载器才会尝试自己去加载。 双亲委派模型的源码实现：主要体现在ClassLoader的loadClass()方法中，思路很简单：先检查是否已经被加载过，若没有加载则调用父类加载器的loadClass()方法，若父类加载器为空则默认使用启动类加载器作为父类加载器。如果父类加载器加载失败，抛出ClassNotFoundException异常后，调用自己的findClass()方法进行加载。 123456789101112131415161718192021222324252627282930public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125;protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; // First, check if the class has already been loaded Class c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c;&#125; 对象的创建、内存布局和访问定位对象的创建 ①类加载检查： 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 ②分配内存： 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题：（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在Eden区分配一块儿内存，JVM在给线程中的对象分配内存时，首先在TLAB分配，当对象大于TLAB中的剩余内存或TLAB的内存已用尽时，再采用上述的CAS进行内存分配 ③初始化零值： 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 ④设置对象头： 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 ⑤执行 init 方法： 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的内存布局在 Hotspot 虚拟机中，对象在内存中的布局可以分为3块区域：对象头、实例数据和对齐填充。 对象头包括两部分信息，第一部分用于存储对象自身的自身运行时数据（哈希码、GC分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为Hotspot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说就是对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象的访问定位建立对象就是为了使用对象，我们的Java程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式有虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么Java堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference 中存储的直接就是对象的地址。 两种访问方式的比较：这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。]]></content>
      <categories>
        <category>java面试准备</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm内存结构和GC机制]]></title>
    <url>%2F2019%2F02%2F15%2Fjvm%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%92%8CGC%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[java内存区域，也称运行时数据区 线程私有： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 JVM会分配一个运行时内存空间。包括5大部分：程序计数器(Program Counter Register)、虚拟机栈(VM Stack)、本地方法栈(Native Method Stack)、方法区(Method Area)、堆(Heap)。线程私有的空间，会随着线程而生，随着线程而亡。这3个区域内存分配和回收都是确定了的，无需考虑内存回收的问题。但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。 程序计数器：程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现OutOfMemoryError的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 虚拟机栈：与程序计数器一样，Java虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若Java虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的Java虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 本地方法栈：和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。 堆：Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 下图是JDK1.7版本时的堆空间的内存分配示意图： 方法区：方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 HotSpot 虚拟机中方法区也常被称为 “永久代”，本质上两者并不等价。仅仅是因为 HotSpot 虚拟机设计团队用永久代来实现方法区而已，这样 HotSpot 虚拟机的垃圾收集器就可以像管理 Java 堆一样管理这部分内存了。但是这并不是一个好主意，因为这样更容易遇到内存溢出问题。 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 常量池：Java中有三种常量池，字符串常量池、class常量池、运行时常量池 字符串常量池： 在jdk1.6以及之前的版本，字符串常量池是放到方法区中的，在1.7及之后就被移到了堆中；在HotSpot VM里实现string pool功能的是一个StringTable类，它是一个Hash表，默认长度是1009；StringTable在每个HotSpot VM的实例只有一份，被所有的类共享。字符串常量由一个个字符组成，放在了StringTable上。 字符串常量池里放的是什么？ 在JDK6.0及之前的版本中，String Pool里放的都是字符串常量；在JDK7.0中，String Pool中可以存放放于堆内的字符串对象的引用。 class常量池： 每一个Java类都会被编译成class文件；class文件中除了包含类的版本、字段、方法、接口等描述信息外，还有一项信息就是常量池，用于存放编译器生成的各种字面量和符号引用，每个class文件都有一个class常量池。 什么是字面量和符号引用： 字面量包括：1.文本字符串 2.八种基本类型的值 3.被声明为final的常量等; 符号引用包括：1.类和方法的全限定名 2.字段的名称和描述符 3.方法的名称和描述符。 运行时常量池： 运行时常量池存在于方法区中（JDK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。），也就是class常量池被加载到内存之后的版本。 不同之处是：它的字面量可以动态的添加，符号引用可以被解析为直接引用。当类加载到内存中后，jvm就会将class常量池中的内容存放到运行时常量池中，由此可知，运行时常量池也是每个类都有一个。在解析阶段，会把符号引用替换为直接引用，解析的过程会去查询字符串常量池，以保证运行时常量池所引用的字符串与字符串常量池中是一致的。 既然运行时常量池时方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 GC机制Java GC（Garbage Collection）垃圾回收机制，Java VM中，存在自动内存管理和垃圾清理机制。GC机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。Java中不能显式分配和注销内存。有些开发者把对象设置为null或者调用System.gc()显式清理内存。设置为null至少没什么坏处，但是调用System.gc()会一定程度上影响系统性能。Java开发人员通常无须直接在程序代码中清理内存，而是由垃圾回收器自动寻找不必要的垃圾对象，并且清理掉它们。 Java GC主要做三件事：（a）哪些内存需要GC？（b）何时需要执行GC？（c）以何策略执行GC？ Java中什么哪些内存需要GC回收？JVM会分配一个运行时内存空间。包括5大部分：程序计数器(Program Counter Register)、虚拟机栈(VM Stack)、本地方法栈(Native Method Stack)、方法区(Method Area)、堆(Heap)。其中程序计数器、虚拟机栈、本地方法栈是每个线程私有内存空间，随线程而生，随线程而亡。这3个区域内存分配和回收都是确定的，无需考虑内存回收的问题。但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。GC主要进行回收的内存是JVM中的方法区和堆，涉及到多线程(指堆)、多个对该对象不同类型的引用(指方法区)，才会涉及GC的回收。小结：Java GC针对的是JVM中堆和方法区。 Java GC机制启动之前，需要确定堆内存中哪些对象是存活的，一般有两种方法：引用计数法和可达性分析法。引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。引用计数法实现简单，判定高效，但不能解决对象之间相互引用的问题。可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。通过称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，搜索路径称为 “引用链(Reference Chain)”，以下对象可作为GC Roots： (a)、虚拟机栈（栈帧中的本地变量表）中引用的对象； (b)、方法区中的类静态变量引用的对象； (c)、方法区中常量引用的对象 (d)、本地方法栈中JNI（即一般说的Native方法）中引用的对象 小结：当一个对象到 GC Roots 没有任何引用链时，意味着该对象可以被回收。 小结：Java GC垃圾回收机制，回收的是已死的Java对象（引用无法可达）。 Java GC垃圾回收算法： 标记-清除：当堆中的有效内存空间被耗尽的时候，就会停止整个程序，然后进行标记和清除。 标记：标记的过程其实就是，遍历所有的GC Roots，然后将所有的GC Roots可达对象标记为存活的对象。 清除：清除的过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。 之所以说它是最基础的内存回收算法，是因为后续的算法都是基于这种思路、并对其缺点进行改进而得到的。 主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高(需要递归标记和全堆对象遍历清理)；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存时候，不得不提前触发另一次垃圾收集动作。 标记-压缩：该算法与标记-清除算法类似，都是先对存活的对象进行标记，但是在清除后会把活的对象向左端空闲空间移动，然后再更新其引用对象的指针。 复制：它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。 Java GC的分代垃圾回收机制 GC分代的假设：绝大部分对象的生命周期都非常短暂，存活时间短。 “分代回收”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。 新生代：在新生代中，使用“停止-复制”算法进行内存清理。绝大多数最新被创建的对象会被分配到这里，由于大部分对象在创建后会很快变得不可到达，所以很多对象被创建在新生代，然后消失。对象从这个区域消失的过程称为“Minor GC” 。 老年代：对象没有变得不可达，并且从新生代中存活下来，会被拷贝到这里。其所占用的空间要比新生代多。也正由于其相对较大的空间，发生在老年代上的GC要比新生代少得多。对象如果在年轻代存活了足够长的时间而没有被清理掉（即在几次Young GC后存活了下来），则会被复制到年老代，年老代的空间一般比年轻代大，能存放更多的对象，在年老代上发生的GC次数也比年轻代少。当年老代内存不足时，将执行Major GC，这种情况下也叫Full GC。老年代存储的对象比年轻代多得多，而且不乏大对象。 对老年代进行内存清理时，如果使用停止-复制算法，则相当低效。一般，老年代用的算法是标记-整理算法，即：标记出仍然存活的对象（存在引用的），将所有存活的对象向一端移动，以保证内存的连续。 JVM内存分配策略 对象优先分配在Eden区 大对象直接进入老年代 长期存活的对象将进入老年代 空间担保分配（老年代剩余空间要多于幸存区的一般，否则要进行Full GC） 小结：Java内存分配和回收机制是：分代分配，分代回收。新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。 新生代垃圾回收过程(复制)基于大多数新生对象都会在GC中被收回的假设。新生代的GC 使用复制算法。在GC前To 幸存区(survivor)保持清空,对象保存在 Eden 和 From 幸存区(survivor)中。GC运行时,Eden中的幸存对象被复制到 To 幸存区(survivor)，针对 From 幸存区(survivor)中的幸存对象，会考虑对象年龄,如果年龄没达到阀值(tenuring threshold)，对象会被复制到To 幸存区(survivor)，如果达到阀值对象被复制到老年代。复制阶段完成后，Eden 和From 幸存区中只保存死对象，可以视为清空。如果在复制过程中To 幸存区被填满了，剩余的对象会被复制到老年代中。最后 From 幸存区和 To幸存区会调换下名字，在下次GC时，To 幸存区会成为From 幸存区。 上图演示GC过程，黄色表示死对象，绿色表示剩余空间，红色表示幸存对象 总结一下，对象一般出生在Eden区，年轻代GC过程中，对象在2个幸存区之间移动，如果对象存活到适当的年龄，会被移动到老年代。当对象在老年代死亡时，就需要更高级别的GC，更重量级的GC算法(复制算法不适用于老年代，因为没有多余的空间用于复制) 现在应该能理解为什么新生代大小非常重要了(译者,有另外一种说法：新生代大小并不重要，影响GC的因素主要是幸存对象的数量)，如果新生代过小，会导致新生对象很快就晋升到老年代中，在老年代中对象很难被回收。如果新生代过大，会发生过多的复制过程。我们需要找到一个合适大小，不幸的是，要想获得一个合适的大小，只能通过不断的测试调优。 参考博客： http://ifeve.com/useful-jvm-flags-part-5-young-generation-garbage-collection/ https://blog.csdn.net/anjoyandroid/article/details/78609971 https://blog.csdn.net/zhangphil/article/details/78260863 https://github.com/Snailclimb/JavaGuide/blob/master/Java%E7%9B%B8%E5%85%B3/%E5%8F%AF%E8%83%BD%E6%98%AF%E6%8A%8AJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E8%AE%B2%E7%9A%84%E6%9C%80%E6%B8%85%E6%A5%9A%E7%9A%84%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0.md]]></content>
      <categories>
        <category>java面试准备</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F02%2F09%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[常用符号字符类 类型 解释 [abc] a、b或c [^abc] 任何字符，除了a、b或c [a-zA-Z] a到z或A到Z，两头的字母包括在内 [a-d[m-p]] a到d或m到p(也可以写成[a-dm-p]) [a-z&amp;&amp;[^bc]] a到z除去b和c(也可以写成[ad-z]) [a-z&amp;&amp;[def]] d、e或f [a-z&amp;&amp;[^m-p]] a到z除去m到p(也可以写成[a-lq-z]) 预定义字符类 类型 解释 . 任何字符(与行结束符可能匹配也可能不匹配) \d 数字([0-9]) \D 非数字([^0-9]) \s 空白字符([\t\n\x0B\f\r]) \S 非空白字符([^\s]) \w 单词字符([a-zA-Z_0-9]) \W 非单词字符([^\w]) 边界匹配器 类型 解释 ^ 行的开头 $ 行的结尾 \b 单词的边界 \B 非单词的边界 Greedy数量词 类型 解释 X? X，一次或一次也没有 X* X，零次或多次 X+ X，一次或多次 X{n} X，恰好 n 次 X{n,} X，至少 n 次 X{n,m} X，至少 n 次，但是不超过 m 次 元字符 元字符 举例 . 例如正则表达式r.t匹配这些字符串：rat、rut、r t，但是不匹配root。 $ 例如正则表达式weasel$，能够匹配字符串”He’s a weasel”的末尾，但是不能匹配字符串”They are a bunch of weasels.” ^ 匹配一行的开始。例如正则表达式^When in能够匹配字符串”When in the”的开始，但是不能匹配”What and When in the” * 匹配0或多个正好在它之前的那个字符。例如正则表达式(.*)意味着能够匹配任意数量的任何字符。 \ 这个是用来转义用的。例如正则表达式\$被用来匹配美元符号，而不是行尾，类似的，正则表达式.用来匹配点字符，而不是任何字符的通配符。 将两个匹配条件进行逻辑“或”（or）运算 + 匹配1或多个正好在它之前的那个字符。例如正则表达式9+匹配9、99、999等。 ? 匹配0或1个正好在它之前的那个字符。 {i},{i,j} 例如正则表达式A[0-9]{3} 能够匹配字符”A”后面跟着正好3个数字字符的串，例如A123、A348等，但是不匹配A1234。而正则表达式[0-9]{4,6} 匹配连续的任意4个、5个或者6个数字字符。 正则表达式的() [] {}的区别12345678910111213() 是为了提取匹配的字符串。表达式中有几个()就有几个相应的匹配字符串。圆括号中的字符视为一个整体。[]是定义匹配的字符范围。比如 [a-zA-Z0-9] 表示相应位置的字符要匹配英文字符和数字。&#123;&#125;一般用来表示匹配的长度，比如 \s&#123;3&#125; 表示匹配三个空格，\s[1,3]表示匹配一到三个空格。(0-9) 匹配 '0-9′ 本身。[0-9]* 匹配数字（注意后面有*，可以为空）[0-9]+ 匹配数字（注意后面有+，不可以为空），例如&#123;1-9&#125; 写法错误。[0-9]&#123;0,9&#125; 表示长度为 0 到 9 的数字字符串。]]></content>
      <categories>
        <category>java进阶</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[version]]></title>
    <url>%2F2019%2F01%2F16%2Fversion%2F</url>
    <content type="text"><![CDATA[Version Control初始化一个Git仓库1$ git init 添加文件到Git仓库12$ git add &lt;file&gt;$ git commit -m "description" git add可以反复多次使用，添加多个文件，git commit可以一次提交很多文件，-m后面输入的是本次提交说明。 添加全部修改到暂存区1git add -A . git add -A表示添加所有内容 git add . 表示添加新文件和编辑过的文件不包括删除的文件 git add -u 表示添加编辑或者删除的文件，不包括新添加的文件。 查看工作区状态1$ git status 查看修改内容 git diff 查看工作区(work dict)和暂存区(stage)的区别 git diff --cached 查看暂存区(stage)和分支(master)的区别 git diff HEAD -- &lt;file&gt; 查看工作区和版本库里面最新版本的区别 版本回退1$ git reset --hard HEAD^ 以上命令是返回上一个版本，在Git中，用HEAD表示当前版本，上一个版本就是HEAD^，上上一个版本是HEAD^^，往上100个版本写成HEAD~100。 回退指定版本号1$ git reset --hard commit_id commit_id是版本号，是一个用SHA1计算出的序列 放弃暂存区修改 退回工作区 1$ git reset HEAD &lt;file&gt; 撤销工作区的修改 1$ git checkout -- &lt;file&gt; Tip： 当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- &lt;file&gt;。 当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD &lt;file&gt;，就回到了第一步，第二步按第一步操作。 已经提交了不合适的修改到版本库时，想要撤销本次提交，进行版本回退，前提是没有推送到远程库。 很重要的git checkout注意点git checkout -- file命令中的--很重要！很重要！很重要！，没有--，就变成了切换到另一个分支的命令]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tag]]></title>
    <url>%2F2019%2F01%2F16%2Ftag%2F</url>
    <content type="text"><![CDATA[标签tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 新建一个标签1$ git tag &lt;tagname&gt; 命令git tag &lt;tagname&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id。 指定标签信息you can use like this: git tag -a &lt;tagname&gt; -m &quot;say something...&quot; 1$ git tag -a &lt;tagname&gt; -m &lt;description&gt; &lt;branchname&gt; or commit_id PGP签名标签you can use like this: git tag -s &lt;tagname&gt; -m &quot;say something...&quot; 1$ git tag -s &lt;tagname&gt; -m &lt;description&gt; &lt;branchname&gt; or commit_id 查看所有标签1$ git tag 推送一个本地标签1$ git push origin &lt;tagname&gt; 推送全部未推送过的本地标签1$ git push origin --tags 删除一个本地标签1$ git tag -d &lt;tagname&gt; 删除一个远程标签1$ git push origin :refs/tags/&lt;tagname&gt;]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[other]]></title>
    <url>%2F2019%2F01%2F16%2Fother%2F</url>
    <content type="text"><![CDATA[常用忽略文件https://github.com/seeways/MyIgnore 配置别名1git config --global alias.&lt;name&gt; &lt;git-name&gt; 建议熟悉git命令后使用 12345# 设置别名git config --global alias.unstage 'reset HEAD'# 使用别名撤掉修改,实际执行git reset HEAD test.pygit unstage test.py 对长命令的别名尤其好用git config –global alias.lg “log –color –graph –pretty=format:’%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset’ –abbrev-commit” 使用效果git lg 显示颜色如果没有默认显示变更文件的颜色，推荐此选项 1git config --global color.ui true git config 配置Git的时候，加上–global是针对当前用户起作用的，如果不加，那只针对当前的仓库起作用。 仓库的Git配置文件都在仓库.git/config文件中 全局的Git配置文件放在用户主目录下的.gitconfig中(隐藏文件) 如果想修改配置或者别名，直接修改或删掉即可]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[origin]]></title>
    <url>%2F2019%2F01%2F16%2Forigin%2F</url>
    <content type="text"><![CDATA[远程仓库创建SSH Key12345678910111213141516171819202122$ ssh-keygen -t rsa -C "youremail@example.com"Generating public/private rsa key pair. # 生成密钥对 Enter file in which to save the key (/root/.ssh/id_rsa): # 保存路径Enter passphrase (empty for no passphrase): # 密码，默认空Enter same passphrase again: # 重复密码Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:92:41:73:6d:ba:03:bf:36:f8:ab:a2:90:0c:9c:a1:85 youremail@example.comThe key's randomart image is:+--[ RSA 2048]----+| o .. || . . o o ||E.. . o ||o.o .o. ||oo ooS. ||o. .+ ||o. . o || . . . + || .. ..+oo |+-----------------+ 关联远程仓库1git remote add origin git@github.com:git_username/repository_name.git 添加后，远程库的名字就是origin 取消关联远程库1git remote remove origin 查看远程库1git remote 查看远程库详细信息如果没有相关权限，则看不到相关地址信息 例如没有推送权限，则看不到push地址 1git remote -v 推送到远程仓库1$ git push origin &lt;branch-name&gt; -u 表示第一次推送master分支的所有内容,不过建议先clone在push，尽量避免此方法 1$ git push -u origin &lt;branch-name&gt; 日常提交只需要使用git push即可 从远程克隆1$ git clone https://github.com/usern/repositoryname.git 日常获取只需要使用git pull即可]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log]]></title>
    <url>%2F2019%2F01%2F16%2Flog%2F</url>
    <content type="text"><![CDATA[查看1$ git log 简化版日志1$ git log --pretty=oneline 查看前N条1git log -n 变更日志-n 同上,不加则显示全部 1git log --stat -n 查看提交修改查看某次commit做了哪些修改 1git show &lt;commit-hash-id&gt; 退出log状态有时候日志过长，可以按下英文q退出状态 查看提交历史和说明1$ git reflog]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[branch]]></title>
    <url>%2F2019%2F01%2F16%2Fbranch%2F</url>
    <content type="text"><![CDATA[分支常用分支命令1234567891011查看分支：git branch创建分支：git branch &lt;name&gt;切换分支：git checkout &lt;name&gt;创建+切换分支：git checkout -b &lt;name&gt;合并某分支到当前分支：git merge &lt;name&gt;删除分支：git branch -d &lt;name&gt; 分支策略在实际开发中，我们应该按照几个基本原则进行分支管理： master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活 在dev分支上进行开发，也就是说，dev分支是不稳定的，到版本发布时，再把dev分支合并到master上，在master分支发布版本 你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。 如果有中间版本，比如测试版，预发布版，按照优先级和流程，从dev递归合并到master上。 合并分支时，加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 创建分支1$ git branch &lt;branchname&gt; 查看分支all branch 1$ git branch 切换分支1$ git checkout &lt;branchname&gt; 创建+切换分支1$ git checkout -b &lt;branchname&gt; 合并某分支到当前分支1$ git merge &lt;branchname&gt; 删除分支1$ git branch -d &lt;branchname&gt; 强制删除1$ git branch -D &lt;branchname&gt; 查看分支合并图当Git无法自动合并分支时，就必须首先解决冲突。 解决冲突后，才可以继续操作。 1$ git log --graph 查看分支树(简版)为什么要敲这么长的命令呢？因为树很长，而简版一眼可以看穿架构 1git log --graph --pretty=oneline --abbrev-commit 普通模式合并分支:禁用no-ff因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。 合并分支时，加上--no-ff参数就可以用普通模式合并，能看出来曾经做过合并，包含作者和时间戳等信息，而fast forward合并就看不出来曾经做过合并。 1$ git merge --no-ff -m "description" &lt;branchname&gt; 保存工作现场1$ git stash 查看工作现场列表1$ git stash list 恢复工作现场但是恢复后，stash内容并不删除，你需要手动删除 1git stash apply 删除工作现场1git stash drop 恢复后自动删除工作现场1git stash pop 恢复指定的工作现场1git stash apply &lt;stash version&gt; 丢弃一个没有合并过的分支1$ git branch -D &lt;branchname&gt; 查看远程库信息1$ git remote -v 在本地创建和远程分支对应的分支建议本地和远程分支的名称保持一致 1$ git checkout -b branch-name origin/branch-name， 建立本地分支和远程分支的关联1$ git branch --set-upstream branch-name origin/branch-name； 从本地推送分支如果推送失败，先用git pull抓取远程的新提交； 1$ git push origin branch-name 从远程抓取分支如果有冲突，要先处理冲突。 1$ git pull]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitBase]]></title>
    <url>%2F2019%2F01%2F14%2FGitBase%2F</url>
    <content type="text"><![CDATA[Git配置12$ git config --global user.name "Your Name"$ git config --global user.email "email@example.com" git config命令的--global参数，表明这台机器上的所有Git仓库都会使用这个配置，也可以对某个仓库指定不同的用户名和邮箱地址。 工作区、暂存区和版本库d的区别 工作区：在电脑里能看到的目录； 版本库：在工作区有一个隐藏目录.git，是Git的版本库。 Git的版本库中存了很多东西，其中最重要的就是称为stage（或者称为index）的暂存区，还有Git自动创建的master，以及指向master的指针HEAD。 进一步解释一些命令： git add实际上是把文件添加到暂存区 git commit实际上是把暂存区的所有内容提交到当前分支撤销修改丢弃工作区的修改1$ git checkout -- &lt;file&gt; 该命令是指将文件在工作区的修改全部撤销，这里有两种情况： 一种是file自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是file已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 克隆仓库1$ git clone https://github.com/usern/repositoryname.git 删除文件1$ git rm &lt;file&gt; git rm &lt;file&gt;相当于执行 1234$ rm &lt;file&gt;$ git add &lt;file&gt;$ git commit -m "say something..."$ git push(optional) 删除文件解释Q：比如执行了rm text.txt 误删了怎么恢复？A：执行git checkout -- text.txt 把版本库的东西重新写回工作区就行了Q：如果执行了git rm text.txt我们会发现工作区的text.txt也删除了，怎么恢复？A：先撤销暂存区修改，重新放回工作区，然后再从版本库写回到工作区12$ git reset head text.txt$ git checkout -- text.txt Q：如果真的想从版本库里面删除文件怎么做？A：执行git commit -m &quot;delete text.txt&quot;，提交后最新的版本库将不包含这个文件]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
</search>
